<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Haes&#39; Blog">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;index.html">
<meta property="og:site_name" content="Haes&#39; Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Little Haes">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>Haes' Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Haes' Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/smallhaes" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/07/2019%E5%B9%B4%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/07/2019%E5%B9%B4%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" class="post-title-link" itemprop="url">2019年的第一篇博客</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-07 17:08:41" itemprop="dateCreated datePublished" datetime="2019-10-07T17:08:41+08:00">2019-10-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="碎碎念"><a href="#碎碎念" class="headerlink" title="碎碎念"></a>碎碎念</h3><p>​        很长时间没有在littlehaes.com上更新博客了, 最主要的原因是不方便在多台电脑上维护博客, 不过现在github能创建私人仓库了, 可以把文章, 配置什么的上传到私人仓库, 这样维护起来就方便多了.<br>​        今天是国庆假期的最后一天, 2019年还有三个月, 时间过得真是太快了, 明年将是不平常的一年, 要么准备秋招, 要么申请出国, 我也不清楚自己会做出什么样的选择, 先把当下的事情做好吧.</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2019/10/07/2019%E5%B9%B4%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/09/ubuntu16-04%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEvsftpd%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/09/ubuntu16-04%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEvsftpd%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/" class="post-title-link" itemprop="url">ubuntu16.04安装配置vsftpd踩坑记录</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-11-09 09:44:44" itemprop="dateCreated datePublished" datetime="2018-11-09T09:44:44+08:00">2018-11-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>最近学习linux, 在安装配置vsftpd时遇到问题,记录一下<br>一. 安装配置可以参考<a href="https://www.linuxidc.com/Linux/2017-06/144807.htm" target="_blank" rel="noopener">Ubuntu 16.04下vsftpd 安装配置实例</a><br>二. 没看上面的教程之前,我以为是vsfpd.conf配置错了,所以想重装vsftpd, 但是无法sudo apt remove vsftpd, 总提示Do you want to continue? [Y/n] Abort, 自动帮我选择了n, 可能是vsftpd的服务还开着,于是用了sudo service vsftpd stop,但还是不能删除,最后就重启了,可以删掉了. 但是发现vsftpd.conf还存在,所以手动删掉了它</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2018/11/09/ubuntu16-04%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEvsftpd%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/08/Xshell%E8%BF%9E%E6%8E%A5%E8%99%9A%E6%8B%9F%E6%9C%BALinux%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/08/Xshell%E8%BF%9E%E6%8E%A5%E8%99%9A%E6%8B%9F%E6%9C%BALinux%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/" class="post-title-link" itemprop="url">Xshell连接虚拟机Linux踩坑记录</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-11-08 09:43:39" itemprop="dateCreated datePublished" datetime="2018-11-08T09:43:39+08:00">2018-11-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="https://upload-images.jianshu.io/upload_images/9608551-5b6323ed8d237d11.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="xshell.png"><br>我使用的是桥接模式,没有做其他修改网卡配置的设置,在windows下可以ping通虚拟机上的Linux,但是Xshell依旧连不上Linux<br>在终端输入:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -e | grep ssh</span><br></pre></td></tr></table></figure>
<p>发现没有sshd,也就是说没有ssh 服务器</p>
<h4 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install openssh-server</span><br><span class="line">此时再输入ps -e | grep ssh便发现sshd启动了</span><br><span class="line">7087 ?        00:00:00 sshd</span><br></pre></td></tr></table></figure>
<p>现在可以用Xshell连接虚拟机上的linux了.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/06/python%E8%A3%85%E9%A5%B0%E5%99%A8%E5%B0%8F%E8%AE%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/06/python%E8%A3%85%E9%A5%B0%E5%99%A8%E5%B0%8F%E8%AE%A1/" class="post-title-link" itemprop="url">python装饰器小计</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-11-06 09:41:06" itemprop="dateCreated datePublished" datetime="2018-11-06T09:41:06+08:00">2018-11-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>研读代码时遇到了python装饰器,所以要搞懂它,这一过程加深了自己对于函数定义与函数调用的体会<br>一. 装饰器本质就是个函数,用来增加被装饰函数的功能.<br>二. 使用装饰器的巨大优势:不改变被装饰函数的调用方式; 不修改被装饰函数的源代码<br>三. 无参数装饰器使用两层嵌套; 带参数装饰器使用三层嵌套<br>下面举两个简单的例子,为了笔记的方便,定义装饰器函数时我用了三种命名:decorator_para; decorator; wrapper,这样有助于记忆</p>
<h4 id="无参数装饰器"><a href="#无参数装饰器" class="headerlink" title="无参数装饰器"></a>无参数装饰器</h4><p>为函数加上计时功能<br><strong>写两层嵌套装饰器函数时牢记两点:向decorator传入被装饰函数名; decorator返回wrapper</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decorator</span><span class="params">(func)</span>:</span> <span class="comment"># 向decorator传入函数名</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">		start = time.time()</span><br><span class="line">		func(*args, **kwargs)</span><br><span class="line">		total = time.time() - start</span><br><span class="line">		print(<span class="string">'函数%s的执行时间为:%s秒'</span> % (func.__name__,total))</span><br><span class="line">	<span class="keyword">return</span> wrapper <span class="comment"># decorator返回wrapper</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@decorator	#相当于 tesla = decorator(tesla)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tesla</span><span class="params">(price=<span class="string">'95W'</span>)</span>:</span></span><br><span class="line">	time.sleep(<span class="number">2</span>)</span><br><span class="line">	print(<span class="string">'鸥翼门真帅气,不过价格挺刺激:%s'</span> % price)</span><br><span class="line"></span><br><span class="line">tesla()</span><br></pre></td></tr></table></figure>
<h4 id="带参数装饰器"><a href="#带参数装饰器" class="headerlink" title="带参数装饰器"></a>带参数装饰器</h4><p>为函数加上计时功能, 可以选择显示方式:default方式是不保留小数;normal方式保留两位小数<br><strong>写三层嵌套装饰器函数时牢记三点:向decorator_para中传入装饰器要用的参数;向decorator传入被装饰函数名; decorator返回wrapper; decorator_para返回decorator</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decorator_para</span><span class="params">(count_type)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">decorator</span><span class="params">(func)</span>:</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">			start = time.time()</span><br><span class="line">			func(*args, **kwargs)</span><br><span class="line">			total = time.time() - start</span><br><span class="line">			<span class="keyword">if</span> count_type == <span class="string">'default'</span>:</span><br><span class="line">				print(<span class="string">'函数%s的执行时间为:%s秒'</span> % (func.__name__,total))</span><br><span class="line">			<span class="keyword">elif</span> count_type == <span class="string">'normal'</span>:</span><br><span class="line">				print(<span class="string">'函数%s的执行时间为:%.2f秒'</span> % (func.__name__,total))</span><br><span class="line">		<span class="keyword">return</span> wrapper</span><br><span class="line">	<span class="keyword">return</span> decorator</span><br><span class="line">					</span><br><span class="line"><span class="meta">@decorator_para('normal')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tesla</span><span class="params">(price=<span class="string">'95W'</span>)</span>:</span></span><br><span class="line">	time.sleep(<span class="number">2</span>)</span><br><span class="line">	print(<span class="string">'鸥翼门真帅气,不过价格挺刺激:%s'</span> % price)</span><br><span class="line"></span><br><span class="line">tesla()</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/01/%E7%AE%B1%E7%BA%BF%E5%9B%BE%E6%80%8E%E4%B9%88%E7%9C%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/01/%E7%AE%B1%E7%BA%BF%E5%9B%BE%E6%80%8E%E4%B9%88%E7%9C%8B/" class="post-title-link" itemprop="url">箱线图怎么看</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-11-01 09:39:58" itemprop="dateCreated datePublished" datetime="2018-11-01T09:39:58+08:00">2018-11-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="https://upload-images.jianshu.io/upload_images/9608551-e622d7dcda429aa0.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="boxplot.jpg"></p>
<p>看图说话，注意以下几个点：</p>
<p>一.箱子的中间一条线，是数据的中位数，代表了样本数据的平均水平。</p>
<p>二.箱子的上下限，分别是数据的上四分位数和下四分位数。这意味着箱子包含了50%的数据。因此，箱子的宽度在一定程度上反映了数据的波动程度。</p>
<p>三.在箱子的上方和下方，又各有一条线。有时候代表着最大最小值，有时候会有一些点“冒出去”。请千万不要纠结，不要纠结，不要纠结（重要的事情说三遍），如果有点冒出去，理解成“异常值”就好。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>一.箱线图是针对连续型变量的，解读时候重点关注平均水平、波动程度和异常值。</p>
<p>二.当箱子被压得很扁，或者有很多异常的时候，试着做对数变换。</p>
<p>三.当只有一个连续型变量时，并不适合画箱线图，直方图是更常见的选择。</p>
<p>四.箱线图最有效的使用途径是作比较，配合一个或者多个定性数据，画分组箱线图。</p>
<p>转载于<a href="https://www.sohu.com/a/134414348_455817" target="_blank" rel="noopener">箱线图应该怎么用</a>,里面是更详细的介绍</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/30/3%E5%88%86%E9%92%9F%E7%90%86%E8%A7%A3np-meshgrid/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/30/3%E5%88%86%E9%92%9F%E7%90%86%E8%A7%A3np-meshgrid/" class="post-title-link" itemprop="url">3分钟理解np.meshgrid()</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-10-30 09:39:13" itemprop="dateCreated datePublished" datetime="2018-10-30T09:39:13+08:00">2018-10-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>官方解释:<br>np.meshgrid(<em>xi, *</em>kwargs)<br>Return coordinate matrices from coordinate vectors. 从坐标向量中返回坐标矩阵<br>不够直观</p>
<h4 id="直观的例子"><a href="#直观的例子" class="headerlink" title="直观的例子"></a>直观的例子</h4><p>二维坐标系中,X轴可以取三个值1,2,3,  Y轴可以取三个值7,8, 请问可以获得多少个点的坐标?<br>显而易见是6个:<br>(1,7)(2,7)(3,7)<br>(1,8)(2,8)(3,8)</p>
<p><strong>np.meshgrid()就是干这个的!</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 坐标向量</span></span><br><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="comment"># 坐标向量</span></span><br><span class="line">b = np.array([<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line"><span class="comment"># 从坐标向量中返回坐标矩阵</span></span><br><span class="line"><span class="comment"># 返回list,有两个元素,第一个元素是X轴的取值,第二个元素是Y轴的取值</span></span><br><span class="line">res = np.meshgrid(a,b)</span><br><span class="line"><span class="comment">#返回结果: [array([ [1,2,3] [1,2,3] ]), array([ [7,7,7] [8,8,8] ])]</span></span><br></pre></td></tr></table></figure>
<p>同理还可以生成更高维度的坐标矩阵</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/30/%E5%BF%AB%E9%80%9F%E7%90%86%E8%A7%A3NMS%E5%9C%A8%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/30/%E5%BF%AB%E9%80%9F%E7%90%86%E8%A7%A3NMS%E5%9C%A8%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">快速理解NMS在物体检测中的应用原理</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-10-30 09:38:16" itemprop="dateCreated datePublished" datetime="2018-10-30T09:38:16+08:00">2018-10-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在物体检测的过程中,模型会生成大量的候选框,通过NMS(Non-Maximum Suppression,非极大值抑制)可以筛选出最优的候选框,<strong>原理非常直观,简单来说就是选出所有的局部最大值.</strong><br>最大值容易找,主要就是<strong>如何定义局部,通过IoU就OK啦~</strong></p>
<h4 id="NMS执行流程"><a href="#NMS执行流程" class="headerlink" title="NMS执行流程"></a>NMS执行流程</h4><p>假定最终选取的候选框集合为res,开始时res是空集; 假定模型输出的大量候选框集合为A,A中的各个候选框有对应的得分</p>
<p>首先从A中选出分数最高的候选框a,将a放入集合res,同时从A中剔除a,接着计算a与A中剩下的各个候选框x的IoU,如果a和某个x的IoU&gt;0.7 (这里取0.7只是用来举例),<strong>说明a和x重叠的范围太多,所以框出的是同一个物体,不过x的分数不如a高,所以就要抑制x,也就是从A中剔除x</strong>.  <strong>这样筛选多次后,A中剩下的候选框与a的重叠范围很小,说明框出的是其他物体了</strong></p>
<p>再从A中选出得分最高的候选框b,将b放入集合res,同时从A中剔除b,接着计算b与A中剩下的各个候选框的IoU……其实你已经发现这都是重复上面的操作了<br>经过多次循环后,就为图片中每个可能的物体选取了最合适的候选框,存储在res中</p>
<h4 id="NMS前"><a href="#NMS前" class="headerlink" title="NMS前"></a>NMS前</h4><p><img src="https://upload-images.jianshu.io/upload_images/9608551-fb9c4d019def0f11.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="nms1.png"></p>
<h4 id="NMS后"><a href="#NMS后" class="headerlink" title="NMS后"></a>NMS后</h4><p><img src="https://upload-images.jianshu.io/upload_images/9608551-248e5c16bda8d0ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="nms2.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/28/3%E5%88%86%E9%92%9F%E7%90%86%E8%A7%A3ROI-Pooling%E5%B1%82/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/28/3%E5%88%86%E9%92%9F%E7%90%86%E8%A7%A3ROI-Pooling%E5%B1%82/" class="post-title-link" itemprop="url">3分钟理解ROI Pooling层</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-10-28 09:36:29" itemprop="dateCreated datePublished" datetime="2018-10-28T09:36:29+08:00">2018-10-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="https://upload-images.jianshu.io/upload_images/9608551-2ae41fb173eb7e88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3分钟理解ROI"><br>直接使用SPPNet论文的原图,下面的变量均用上图中的表示,注意对应关系<br>不同size的图片(input image)经过’convolutional layers’后得到的feature map of conv5的size不同,ROI Pooling层的功能便是将不同size的feature map of conv5处理后得到相同长度的特征表示.(对应图中的大黑框部分)</p>
<h1 id="具体流程"><a href="#具体流程" class="headerlink" title="具体流程:"></a>具体流程:</h1><p>假设输入ROI Pooling层的feature map of conv5的shape是(h,w,c)<br>一. 首先ROI Pooling层把feature map of conv5划分成4*4的小方块(对应图中蓝色矩形),每个小方块的宽高分别为w/4,h/4,通道数为c,不能整除时需要取整.针对feature map的每个通道,分别在这16个小方块进行最大池化(MaxPooling),也就是取出小方块里的最大值.每一个通道都能取出16个最大值,所以所有通道共有16c个值<br>二. 然后ROI Pooling层把feature map of conv5划分成2*2的小方块(对应图中绿色矩形),使用同样的方法得到4c个值<br>三. 接着ROI Pooling层把feature map of conv5划分成1*1的小方块(对应图中灰色矩形),得到c个值<br>四. 最后将上面三个值串联起来得到长度为16c+4c+c=21c的特征表示</p>
<p>显而易见,通过ROI Pooling层得到的特征表示是固定长度(21c)的, 与输入的h,w无关!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/28/numpy%E7%9A%84%E5%B9%BF%E6%92%AD%E5%8E%9F%E7%90%86%E5%8F%8A%E7%A4%BA%E4%BE%8B%E6%BC%94%E7%A4%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/28/numpy%E7%9A%84%E5%B9%BF%E6%92%AD%E5%8E%9F%E7%90%86%E5%8F%8A%E7%A4%BA%E4%BE%8B%E6%BC%94%E7%A4%BA/" class="post-title-link" itemprop="url">numpy的广播原理及示例演示</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-10-28 09:35:46" itemprop="dateCreated datePublished" datetime="2018-10-28T09:35:46+08:00">2018-10-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>研读faster rcnn的源码时发现numpy的广播非常重要,故总结一波<br>以数组A和数组B的相加为例,先牢记两点原则:<br><strong>核心:广播时先使A.shape=B.shape,然后对应位置进行相加</strong><br><strong>返回结果的shape是:A.shape和B.shape对应位置的最大值</strong>,比如:A.shape=(1,9,4),B.shape=(15,1,4),那么A+B的shape是(14,9,4)<br>有两种情况能够进行广播</p>
<ol>
<li>A.ndim 大于 B.ndim, 并且A.shape最后几个元素包含B.shape, 比如:A.shape=(2,3,4,5),B.shape=(3,4,5)或者B.shape=(4,5)或者B.shape=(5).   </li>
<li>A.ndim 等于 B.ndim, 并且<strong>A.shape和B.shape对应位置的元素要么相同要么其中一个是1</strong>,比如:A.shape=(1,9,4),B.shape=(15,1,4), 再比如A.shape=(1,9,4),B.shape=(15,1,1)</li>
</ol>
<p>下面分别进行举例</p>
<h4 id="A-ndim-大于-B-ndim"><a href="#A-ndim-大于-B-ndim" class="headerlink" title="A.ndim 大于 B.ndim"></a>A.ndim 大于 B.ndim</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a.shape=(2,2,3,4)</span></span><br><span class="line">a = np.arange(<span class="number">1</span>,<span class="number">49</span>).reshape((<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment"># b.shape=(3,4)</span></span><br><span class="line">b = np.arange(<span class="number">1</span>,<span class="number">13</span>).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment"># numpy会将b.shape调整至(2,2,3,4), 这一步相当于numpy自动实现np.tile(b,[2,2,1,1])</span></span><br><span class="line">res = a + b</span><br><span class="line">print(<span class="string">'==================================='</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(a.shape)</span><br><span class="line">print(<span class="string">'==================================='</span>)</span><br><span class="line">print(b)</span><br><span class="line">print(b.shape)</span><br><span class="line">print(<span class="string">'==================================='</span>)</span><br><span class="line">print(res)</span><br><span class="line">print(res.shape)</span><br><span class="line">print(<span class="string">'==================================='</span>)</span><br><span class="line">print(a+b == a + np.tile(b,[<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>]) )</span><br></pre></td></tr></table></figure>

<h4 id="A-ndim-等于-B-ndim"><a href="#A-ndim-等于-B-ndim" class="headerlink" title="A.ndim 等于 B.ndim"></a>A.ndim 等于 B.ndim</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#示例1</span></span><br><span class="line"><span class="comment"># a.shape=(4,3)</span></span><br><span class="line">a = np.arange(<span class="number">12</span>).reshape(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># b.shape=(4,1)</span></span><br><span class="line">b = np.arange(<span class="number">4</span>).reshape(<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># numpy会将b.shape调整至(4,3), 这一步相当于numpy自动实现np.tile(b,[1,3])</span></span><br><span class="line">res = a + b</span><br><span class="line">print(<span class="string">'==================================='</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(a.shape)</span><br><span class="line">print(<span class="string">'==================================='</span>)</span><br><span class="line">print(b)</span><br><span class="line">print(b.shape)</span><br><span class="line">print(<span class="string">'==================================='</span>)</span><br><span class="line">print(res)</span><br><span class="line">print(res.shape)</span><br><span class="line">print(<span class="string">'==================================='</span>)</span><br><span class="line">print((a+b == a + np.tile(b,[<span class="number">1</span>,<span class="number">3</span>])) )  <span class="comment"># 打印结果都是True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#示例2</span></span><br><span class="line"><span class="comment"># a.shape=(1,9,4)</span></span><br><span class="line">a = np.arange(<span class="number">1</span>,<span class="number">37</span>).reshape((<span class="number">1</span>,<span class="number">9</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment"># b.shape=(15,1,4)</span></span><br><span class="line">b = np.arange(<span class="number">1</span>,<span class="number">61</span>).reshape((<span class="number">15</span>,<span class="number">1</span>,<span class="number">4</span>))</span><br><span class="line">res = a + b</span><br><span class="line">print(<span class="string">'==================================='</span>)</span><br><span class="line"><span class="comment"># print(a)</span></span><br><span class="line">print(a.shape)</span><br><span class="line">print(<span class="string">'==================================='</span>)</span><br><span class="line"><span class="comment"># print(b)</span></span><br><span class="line">print(b.shape)</span><br><span class="line">print(<span class="string">'==================================='</span>)</span><br><span class="line"><span class="comment"># print(res)</span></span><br><span class="line">print(res.shape)</span><br><span class="line">print(<span class="string">'==================================='</span>)</span><br><span class="line">q = np.tile(a,[<span class="number">15</span>,<span class="number">1</span>,<span class="number">1</span>]) + np.tile(b,[<span class="number">1</span>,<span class="number">9</span>,<span class="number">1</span>])</span><br><span class="line">print(q == res)	<span class="comment"># 打印结果都是True</span></span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/28/np-tile-%E5%92%8Cnp-repeat/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/28/np-tile-%E5%92%8Cnp-repeat/" class="post-title-link" itemprop="url">np.tile()和np.repeat()</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-10-28 09:34:20" itemprop="dateCreated datePublished" datetime="2018-10-28T09:34:20+08:00">2018-10-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>np.tile()和np.repeat()都可以对array进行重复操作,但np.tile()是以axis为最小单位(axis-wise)进行重复的,而np.repeat()是以element为最小单位(element-wise)进行重复的</p>
<h4 id="np-tile-A-reps"><a href="#np-tile-A-reps" class="headerlink" title="np.tile(A,reps)"></a>np.tile(A,reps)</h4><p>输入: A是数组,reps是个list,<strong>reps的元素表示对A的各个axis进行重复的次数</strong><br>返回: 一个数组,维度的数量等于max(A.ndim,len(reps)),<strong>注意不要混淆A.ndim和A.shape</strong><br>有两种特殊情况:</p>
<ol>
<li>A.ndim &lt; len(reps), 此时需要调整A的维度使得A.ndim = len(reps),即添加长度为1的维度,注意:新的维度在原维度的前面,比如原来的A.shape是(3,5),调整后是(1,3,5)</li>
<li>A.ndim &gt; len(reps), 此时需要增加list的长度,使得A.ndim = len(reps),即在reps的最前面增加元素1,比如原来的list是[2,2],增加长度后是[1,2,2]</li>
</ol>
<p>官方示例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例1,正常情况</span></span><br><span class="line">a = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 将axis=0重复2次</span></span><br><span class="line">np.tile(A=a, reps=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># array([0, 1, 2, 0, 1, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例2,特殊情况1:A.ndim &lt; len(reps)</span></span><br><span class="line">a = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 将a.shape调整至(1,3),然后将axis=0重复2次,将axis=1重复2次</span></span><br><span class="line">np.tile(A=a, reps=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment">#array([[0, 1, 2, 0, 1, 2], [0, 1, 2, 0, 1, 2]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例3,特殊情况1:A.ndim &lt; len(reps)</span></span><br><span class="line">a = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 将a.shape调整至(1,1,3),然后将axis=0重复2次,将axis=1重复1次,将axis=2重复2次</span></span><br><span class="line">np.tile(A=a, reps=(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment">#array([[[0, 1, 2, 0, 1, 2]], [[0, 1, 2, 0, 1, 2]]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例4,特殊情况2:A.ndim &gt; len(reps)</span></span><br><span class="line">b = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="comment"># 将reps=[2]调整至[1,2],然后将axis=0重复1次,将axis=1重复2次</span></span><br><span class="line">np.tile(A=b, reps=<span class="number">2</span>)</span><br><span class="line"><span class="comment">#array([[1, 2, 1, 2], [3, 4, 3, 4]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例5,正常情况</span></span><br><span class="line">b = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="comment"># 将axis=0重复两次,将axis=1重复1次</span></span><br><span class="line">np.tile(A=b, reps=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment">#array([[1, 2], [3, 4], [1, 2], [3, 4]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例6,特殊情况1:A.ndim &lt; len(reps)</span></span><br><span class="line">c = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="comment"># 将c.shape调整至(4,1),然后将axis=0重复4次,将axis=1重复1次</span></span><br><span class="line">np.tile(A=c, reps=(<span class="number">4</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># array([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]])</span></span><br></pre></td></tr></table></figure>

<h4 id="np-repeat-a-repeats-axis-None"><a href="#np-repeat-a-repeats-axis-None" class="headerlink" title="np.repeat(a, repeats, axis=None)"></a>np.repeat(a, repeats, axis=None)</h4><p>输入: a是数组,repeats是各个元素重复的次数(<strong>repeats一般是个标量,稍复杂点是个list</strong>),在axis的方向上进行重复<br>返回: 如果不指定axis,则将重复后的结果展平(维度为1)后返回;如果指定axis,则不展平<br>官方示例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例1,展平</span></span><br><span class="line"><span class="comment"># 将3重复4次</span></span><br><span class="line">np.repeat(a=<span class="number">3</span>, repeats=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># array([3, 3, 3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例2,展平</span></span><br><span class="line"><span class="comment"># 每个元素都重复2次,并展平后输出</span></span><br><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">np.repeat(a=x, repeats=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># array([1, 1, 2, 2, 3, 3, 4, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#示例3,不展平</span></span><br><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="comment"># 沿着axis=1方向重复,将axis=1方向上的每个元素重复3次</span></span><br><span class="line">np.repeat(a=x, repeats=<span class="number">3</span>, axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#array([[1, 1, 1, 2, 2, 2], [3, 3, 3, 4, 4, 4]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#示例3,不展平</span></span><br><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="comment"># 沿着axis=0方向重复,将axis=0方向上的第0个元素重复1次,第1个元素重复2次</span></span><br><span class="line">np.repeat(a=x, repeats=[<span class="number">1</span>, <span class="number">2</span>], axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># array([[1, 2], [3, 4], [3, 4]])</span></span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/26/tf-softmax-cross-entropy-with-logits-%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E5%8F%8A%E4%BB%A3%E7%A0%81%E6%BC%94%E7%A4%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/26/tf-softmax-cross-entropy-with-logits-%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E5%8F%8A%E4%BB%A3%E7%A0%81%E6%BC%94%E7%A4%BA/" class="post-title-link" itemprop="url">tf.softmax_cross_entropy_with_logits()的计算过程及代码演示</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-10-26 09:32:41" itemprop="dateCreated datePublished" datetime="2018-10-26T09:32:41+08:00">2018-10-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>tf.softmax_cross_entropy_with_logits()的计算过程一共分为两步:1.将logits转换成概率；2.计算交叉熵损失</p>
<h3 id="1-将logits转换成概率"><a href="#1-将logits转换成概率" class="headerlink" title="1.将logits转换成概率"></a>1.将logits转换成概率</h3><p>比如某个logits =  [2, 7, 5],使用softmax将logits转换成概率,就是按照公式:$\frac {e^x}{\sum e^x}$计算logits中每个元素的值:[$\frac {e^2}{e^2 + e^7 +e5}$ , $\frac {e^7}{e^2 + e^7 +e5}$ , $\frac {e^5}{e^2 + e^7 +e5}$],计算结果为[0.00589975 0.8756006  0.11849965],这三个元素每个都不小于0并且和为1,所以构成了概率分布</p>
<h3 id="2-计算交叉熵损失"><a href="#2-计算交叉熵损失" class="headerlink" title="2.计算交叉熵损失"></a>2.计算交叉熵损失</h3><p>假设对应logits的标签labels是[0,1,0]<br>根据交叉熵公式 $-\sum y’\times log(y)$计算概率化之后的logits和标签之间的交叉熵损失,其中$y’=[0,1,0],y=[0.00589975 0.8756006  0.11849965]$ ,也就是$-0\times log(0.00589975)-1\times log(0.8756006)-0\times log(0.11849965) = 0.6355716$</p>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>上面举的例子只对应一个样本的logits=[2, 7, 5],一般训练时batch size不会为设为1, 所以要使用tf.reduce_mean()来对tf.softmax_cross_entropy_with_logits()的结果取平均,得到关于样本的平均交叉熵损失.比如batch size = 2<br>logits=[[2,7,5],[6,3,4]]  labels=[[0,1,0],[1,0,0]]<br>使用tf.softmax_cross_entropy_with_logits()计算后得到[2,7,5],[6,3,4]这两个样本的交叉熵损失,再使用tf.reduce_mean()取平均,具体见下面的代码演示</p>
<h3 id="代码演示"><a href="#代码演示" class="headerlink" title="代码演示"></a>代码演示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">###############################</span></span><br><span class="line"><span class="comment"># 一个样本</span></span><br><span class="line"><span class="comment">###############################</span></span><br><span class="line">logits = tf.constant([<span class="number">2</span>,<span class="number">7</span>,<span class="number">5</span>],dtype=tf.float32)</span><br><span class="line">labels = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line"><span class="comment">#对logits使用softmax,[0.00589975 0.8756006  0.11849965]</span></span><br><span class="line">res1 = tf.nn.softmax(logits) </span><br><span class="line"><span class="comment"># 交叉熵损失中的各个对数部分,[-5.1328454  -0.13284525 -2.1328452 ]</span></span><br><span class="line">res2 = tf.log(res1) </span><br><span class="line"><span class="comment"># 交叉熵损失,0.13284527</span></span><br><span class="line">res3 = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	res1,res2,res3 = sess.run([res1,res2,res3])</span><br><span class="line">	print(res1)</span><br><span class="line">	print(res2)</span><br><span class="line">	print(res3)</span><br><span class="line">print(<span class="string">'===================================================='</span>)</span><br><span class="line"><span class="comment">###############################</span></span><br><span class="line"><span class="comment"># 多个样本</span></span><br><span class="line"><span class="comment">###############################</span></span><br><span class="line">logits = tf.constant([[<span class="number">2</span>,<span class="number">7</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">3</span>,<span class="number">4</span>]],dtype=tf.float32)</span><br><span class="line">labels = [[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]]</span><br><span class="line"><span class="comment">#对logits使用softmax,[[0.00589975 0.8756006  0.11849965] [0.8437947  0.04201007 0.11419519]]</span></span><br><span class="line">res1 = tf.nn.softmax(logits)</span><br><span class="line"><span class="comment"># 交叉熵损失中的各个对数部分,[[-5.1328454  -0.13284525 -2.1328452 ] [-0.16984606 -3.169846   -2.169846  ]]</span></span><br><span class="line">res2 = tf.log(res1)</span><br><span class="line"><span class="comment"># 交叉熵损失,[0.13284527 0.16984604]</span></span><br><span class="line">res3 = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels)</span><br><span class="line"><span class="comment"># 求出交叉熵损失后再对各个样本的交叉熵损失取平均,0.15134566</span></span><br><span class="line">res4 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	res1,res2,res3,res4 = sess.run([res1,res2,res3,res4])</span><br><span class="line">	print(res1)</span><br><span class="line">	print(res2)</span><br><span class="line">	print(res3)</span><br><span class="line">	print(res4)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/26/softmax-cross-entropy-with-logits%E4%B8%8Esparse-softmax-cross-entropy-with-logits%E7%9A%84%E5%8C%BA%E5%88%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/26/softmax-cross-entropy-with-logits%E4%B8%8Esparse-softmax-cross-entropy-with-logits%E7%9A%84%E5%8C%BA%E5%88%AB/" class="post-title-link" itemprop="url">softmax_cross_entropy_with_logits与sparse_softmax_cross_entropy_with_logits的区别</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-10-26 09:31:19" itemprop="dateCreated datePublished" datetime="2018-10-26T09:31:19+08:00">2018-10-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>读源码时发现softmax交叉熵损失函数的logits是(N,2),但是labels却是(N,1)的,仔细一看原来用的是sparse_softmax_cross_entropy_with_logits,看了一下文档,补充了基础知识</p>
<p>举例来说:<br>某个关于图像的单类别分类任务中,类别共有5种,<br>构造标签时往往会用[0,1,0,0,0],[0,0,0,1,0]这种one hot的形式,这种情况下使用tf.softmax_cross_entropy_with_logits()<br>如果构造标签时使用索引来表示类别,如第2类用[1],第4类用[3],这时就得用tf.sparse_softmax_cross_entropy_with_logits()了. 五种类别,索引的取值范围是:{0,1,2,3,4}</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>二者的不同在于标签形式的不同,<br>tf.softmax_cross_entropy_with_logits()使用[0,1,0,0,0],[0,0,0,1,0]形式的标签；<br>同样的标签在tf.sparse_softmax_cross_entropy_with_logits()表示成[1],[3]</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/26/Tenor-%E5%92%8Cnumpy-array-%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/26/Tenor-%E5%92%8Cnumpy-array-%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2/" class="post-title-link" itemprop="url">Tenor 和numpy array 相互转换</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-10-26 09:29:51" itemprop="dateCreated datePublished" datetime="2018-10-26T09:29:51+08:00">2018-10-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>a = np.array([1,2,3])<br>b = tf.constant([1,2,3])</p>
<h3 id="numpy-array-转-Tensor"><a href="#numpy-array-转-Tensor" class="headerlink" title="numpy array 转 Tensor"></a>numpy array 转 Tensor</h3><p>res = tf.convert_to_tensor(a)</p>
<h3 id="Tensor-转-numpy-array"><a href="#Tensor-转-numpy-array" class="headerlink" title="Tensor 转 numpy array"></a>Tensor 转 numpy array</h3><p>res = b.eval(session=sess)</p>
<p>二者的转换实际上就是<strong>静态图阶段的数据</strong>和<strong>运行时的数据</strong>之间的转换<br>其实sess.run(tensor)和tensor.eval(session=sess)效果一样,但是tensor.eval()写起来更高效快捷</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/24/FasterRCNN%E4%B9%8B%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6%E8%AF%A6%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/24/FasterRCNN%E4%B9%8B%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6%E8%AF%A6%E8%A7%A3/" class="post-title-link" itemprop="url">FasterRCNN之整体框架详解</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-10-24 09:41:41" itemprop="dateCreated datePublished" datetime="2018-10-24T09:41:41+08:00">2018-10-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>刚接触物体检测领域,学习了Faster RCNN的论文及Tensorflow版本的源码,不得不说,读源码真的过瘾..不过确实能够帮助理解框架,下面按照Faster RCNN的预测过程介绍其整体流程</p>
<h2 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h2><p>Faster RCNN整体框架包括4部分:<br>一. 使用VGG16或者其他成熟的图片分类模型提取<strong>图片特征(feature map)</strong><br>二. 将<strong>图片特征</strong>喂入RPN(Region Proposal Network)网络得到<strong>proposals</strong> (包含第一次回归)<br>三. 将上两步的结果:<strong>图片特征</strong>和 <strong>proposals</strong> 喂入RoI Pooling层得到综合的<strong>proposals特征</strong><br>四. 根据<strong>poposals特征</strong>预测<strong>物体的bounding box</strong>和<strong>物体的类别</strong> (包含第二次回归)</p>
<p>对应下图: <strong>图中conv(3,3,512,1,1)对应conv(filter_height,filter_width,output_channels,stride_height,stride_width)</strong><br><img src="https://upload-images.jianshu.io/upload_images/9608551-32b0af61b605c33e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Faster RCNN预测.jpg"></p>
<p>接下来分别介绍这四部分对应的网络结构,以预测过程为例(训练过程涉及ground truth的构建,会在之后的博客中具体介绍)</p>
<h2 id="一-获取图片特征"><a href="#一-获取图片特征" class="headerlink" title="一.获取图片特征"></a>一.获取图片特征</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-4cf624aa8f12a005.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="vgg.png"><br>使用预训练好的VGG16提取图片特征,如下图所示,喂入的图片(也就是’data’)并没有经过VGG16所有的流程,而是在得到’conv5_3’这个结果后就停下了,’conv5_3’就是Faster RCNN需要的图片特征(feature map)<br>注意:’conv5_3’的h,w是输入图片’data’的1/16<br><img src="https://upload-images.jianshu.io/upload_images/9608551-9f4cb555209f38f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"></p>
<h2 id="二-RPN网络输出proposals"><a href="#二-RPN网络输出proposals" class="headerlink" title="二.RPN网络输出proposals"></a>二.RPN网络输出proposals</h2><p>这是Faster RCNN中最复杂的一部分. 将通过VGG16得到的<strong>图片特征</strong>喂入RPN(Region Proposal Network)网络得到<strong>proposals</strong><br>RPN网络结构如下图所示<br><img src="https://upload-images.jianshu.io/upload_images/9608551-17f54c255c42525f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="rpn.png"><br>(1) 首先将图片特征’conv5_3’喂给卷积层conv(3,3,512,1,1),输出的图片特征 <strong>‘rpn_conv/3x3’</strong> 尺寸和通道数均保持不变.<br>根据卷积的运算公式,猜测这一层的作用为:该层 <strong>每个filter</strong> 分别将’conv5_3’的所有通道联系起来,也就是说经过卷积后, <strong>‘rpn_conv/3x3’的每个通道</strong> 都整合了’conv5_3’所有通道的信息.(这地方说起来比较绕,如果明白卷积的计算过程就容易理解了)<br>(2.1) 将 <strong>‘rpn_conv/3x3’</strong> 喂给conv(1,1,36,1,1)得到 <strong>‘rpn_box_pred’</strong> ,36个通道表示:每个点有9组proposals的回归值,每组proposals的回归值对应论文中的tx,ty,tw,th.(这里是第一次回归)<br><strong>这里要重点注意一下,’rpn_conv/3x3’的尺寸是h,w,也就是’rpn_conv/3x3’有h*w个像素(不要考虑通道数,举个例子:平时我们看到的彩色图是3通道的,假设分辨率是1920*1080,这表示有1920*1080个像素点,而不是3*1920*1080个).针对这h*w个像素,以每个像素为中心为每个像素生成9组proposals的回归值,每组回归值包含4个信息.所以每个像素点需要36个维度去存储9*4个信息,这就是输出通道36的来源!</strong><br> (2.2.1) 将 <strong>‘rpn_conv/3x3’</strong> 喂给conv(1,1,18,1,1)得到 <strong>‘rpn_cls_score’</strong> ,刚才说了,要为 <strong>‘rpn_conv/3x3’</strong> 的每个像素生成9个proposals,而每个proposal是有类别的,要么是foreground proposal(框出物体)，要么是backgroud proposal(框出背景).所以每个proposal需要2个维度来存储类别得分,因此 <strong>‘rpn_conv/3x3’</strong> 的每个像素需要9*2=18个维度,这就是输出通道18的来源.<br>(2.2.2)接着将 <strong>‘rpn_cls_score’</strong> 依次喂入reshape_layer,softmax,reshape_layer,最终得到 <strong>‘rpn_cls_prob_reshape’</strong> ,这一过程将每个类别得分转换为概率.<br>为什么要经过两个reshape_layer呢? 这里引用<a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">白裳大神的解释:</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">那么为何要在softmax前后都接一个reshape layer？其实只是为了便于softmax分类，至于具体原因这就要从caffe的实现形式说起了。</span><br><span class="line">在caffe基本数据结构blob中以如下形式保存数据：blob=[batch_size, channel，height，width]</span><br><span class="line">对应至上面的保存bg/fg anchors的矩阵，其在caffe blob中的存储形式为[<span class="number">1</span>, <span class="number">2</span>x9, H, W]。</span><br><span class="line">而在softmax分类时需要进行fg/bg二分类，所以reshape layer会将其变为[<span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>xH, W]大小，</span><br><span class="line">即单独“腾空”出来一个维度以便softmax分类，之后再reshape回复原状。</span><br><span class="line">贴一段caffe softmax_loss_layer.cpp的reshape函数的解释，非常精辟：</span><br><span class="line"><span class="string">"Number of labels must match number of predictions; "</span></span><br><span class="line"><span class="string">"e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), "</span></span><br><span class="line"><span class="string">"label count (number of labels) must be N*H*W, "</span></span><br><span class="line"><span class="string">"with integer values in &#123;0, 1, ..., C-1&#125;."</span>;</span><br><span class="line">综上所述，RPN网络中利用anchors和softmax初步提取出foreground anchors作为候选区域。</span><br></pre></td></tr></table></figure>
<p>(3) 将 <strong>‘rpn_box_pred’</strong> 和 <strong>‘rpn_cls_prob_reshape’</strong> 以及 <strong>im_info</strong> 喂给proposal_layer 得到 <strong>‘rois’</strong> ,也就是RPN网络最终输出的proposals. 这些proposals都是从foreground anchors里面挑选的,  用到了nms. 接下来还需要对RPN输出的proposals进行第二次回归</p>
<h2 id="三-通过RoI-Pooling层得到综合的proposals特征"><a href="#三-通过RoI-Pooling层得到综合的proposals特征" class="headerlink" title="三. 通过RoI Pooling层得到综合的proposals特征"></a>三. 通过RoI Pooling层得到综合的proposals特征</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-a838d33a0c8c5e20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="roi.png"><br>(1) 将 <strong>‘rois’</strong> 和 <strong>‘conv5_3’</strong> 喂给roi_pool 得到 <strong>‘pool_5’</strong> , 关于roi_pool的输入,尺寸是任意大小的,但输出的特征都是长度固定的. 具体可参考<a href="https://blog.csdn.net/littlehaes/article/details/83473863" target="_blank" rel="noopener">3分钟理解ROI Pooling层</a></p>
<h2 id="四-根据poposals特征进行框回归和物体分类"><a href="#四-根据poposals特征进行框回归和物体分类" class="headerlink" title="四.根据poposals特征进行框回归和物体分类"></a>四.根据poposals特征进行框回归和物体分类</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-6faac99ad1a14cdb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="final.png"><br>(1) 将 <strong>‘pool_5’</strong> 连续经过两个全连接层得到 <strong>‘fc_7’</strong><br>(2.1)将 <strong>‘fc_7’</strong> 经过全连接层得到 <strong>‘bbox_pred’</strong> ,这里是二次回归,表示tx,ty,tw,th.用来和 <strong>‘rois’</strong> 相加,从而得到Faster RCNN最终的proposals!<br>(2.2.1) 将 <strong>‘fc’</strong> 经过全连接层得到 <strong>‘cls_score’</strong><br>(2.2.2) 将 <strong>‘cls_score’</strong> 经过softmax层得到 <strong>‘cls_prob’</strong> ,也就是proposal属于各个物体的概率</p>
<p>以上便是Faster RCNN的4部分,关于模块的实现细节,我过几天再写</p>
<p>训练时迭代了50000次,pascal2007测试结果MAP=0.65,其中,chair的ap最低0.46,horse的ap最高0.81<br>运行环境:CUDA8,cuDNN7,1070Ti,TensorFlow1.4.0,python2.7<br>训练集:voc_2007_trainval    训练时间148分钟<br>测试集:voc_2007_testt    测试时间10分钟(5000图)</p>
<p>参考:<br><a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">白裳</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/22/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E4%B8%8E%E7%AD%89%E9%AB%98%E7%BA%BF%E5%9E%82%E7%9B%B4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/22/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E4%B8%8E%E7%AD%89%E9%AB%98%E7%BA%BF%E5%9E%82%E7%9B%B4/" class="post-title-link" itemprop="url">为什么梯度方向与等高线垂直</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-10-22 08:42:33" itemprop="dateCreated datePublished" datetime="2018-10-22T08:42:33+08:00">2018-10-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="https://upload-images.jianshu.io/upload_images/9608551-23c4863ce96bf109.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="1.jpg"></p>
<p>有些结论用起来习以为常,却不知道背后的原理,比如为什么梯度方向与等高线垂直,弄明白后心里才舒畅<br>要解决这个问题首先得有等高线的数学表达式</p>
<h3 id="等高线的法线"><a href="#等高线的法线" class="headerlink" title="等高线的法线"></a>等高线的法线</h3><p>以三维空间为例, 设某曲面的表达式为$z=f(x,y)$,对于任意高度且平行于xoy的平面$z=c$来说,等高线为$\begin{cases}z=f(x,y)\z=c\end{cases}$,也即$f(x,y)=c$,等高线上任意一点处的斜率是:$\frac {dy}{dx}$,通过$z=f(x,y)$来表示等高线上的斜率有$\frac {dy}{dx}={\frac {\partial f}{\partial x} / {\frac {\partial f}{\partial y} }}$,该点对应的法线方向为斜率的负倒数,即$- \frac {1}{\frac {dy}{dx}}=\frac {dx}{dy}=\frac {\partial f}{\partial y}/\frac{\partial f}{\partial x}=tan\theta$,其中$\theta$是法线和x轴的夹角.如果梯度的方向和等高线的法线方向一致,就证明了垂直关系</p>
<h3 id="梯度向量"><a href="#梯度向量" class="headerlink" title="梯度向量"></a>梯度向量</h3><p>$z=f(x,y)$的梯度向量为$(\frac {\partial f}{\partial x}, \frac {\partial f}{\partial y})$,设该向量和x轴的夹角大小为$\gamma$,则夹角的正切为$tan\gamma=\frac {\partial f}{\partial y}/\frac {\partial f}{\partial x}=tan\theta$</p>
<p><strong>所以梯度向量的方向和等高线的法线方向是一样的!也就是说,梯度方向和等高线垂直</strong></p>
<p>参考:<br><a href="https://zhuanlan.zhihu.com/p/27731819" target="_blank" rel="noopener">知乎忆臻大神</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/19/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%9C%80%E5%A4%A7%E5%8C%96%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/19/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%9C%80%E5%A4%A7%E5%8C%96%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87/" class="post-title-link" itemprop="url">为什么要最大化后验概率</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-10-19 08:41:01" itemprop="dateCreated datePublished" datetime="2018-10-19T08:41:01+08:00">2018-10-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>训练模型时，我们经常先为后验概率建模，也就是写出后验概率的数学表达式，然后求后验概率的最大值，使得后验概率最大的那些参数就是训练结果了。<br>为什么最大化后验概率是有意义的呢？本质上和我们日常生活中的判断方式是一致的。<br>举个例子，我们对一类物体进行分类，类别有c1,c2,c3…等等<br>我们拿到某个物体x时，怎么对x进行分类？其实就是判断p(x,c1),p(x,c2),p(x,c3)…中哪个值最大！比如p(x,c2)最大，那我们就认为当前这个物体的类别是c2。注意到这一判断过程就是我们常规的思维方式。<br>本质上我们是根据联合概率p(x,ci)做出判断的，如何跟后验概率联系起来？用贝叶斯公式展开！<br>p(x,ci) = p(ci|x)p(x), 回到上面那个例子，也就是说我们判断的是p(c1|x)p(x)，p(c2|x)p(x)，p(c3|x)p(x)…中哪个最大，注意到p(x)相当于常数，所以也就是找出最大的p(ci|x)，也就是最大化后验概率了。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/23/tf-nn-embedding-lookup%E7%94%A8%E6%B3%95%E8%A7%A3%E9%87%8A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/09/23/tf-nn-embedding-lookup%E7%94%A8%E6%B3%95%E8%A7%A3%E9%87%8A/" class="post-title-link" itemprop="url">tf.nn.embedding_lookup用法解释</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-23 08:33:45" itemprop="dateCreated datePublished" datetime="2018-09-23T08:33:45+08:00">2018-09-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>tf.nn.embedding_lookup( params, ids, …),主要使用params, ids两个参数，函数的功能是从params中挑出索引为ids的元素，并返回一个张量，<br>假设params的shape是batch * hidden, ids的shape是batch * n<br>那么函数返回张量的shape是batch *n * hidden</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">w = tf.constant([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">res = tf.nn.embedding_lookup(w, [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">res2 = tf.nn.embedding_lookup(w, [[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">0</span>]])</span><br><span class="line">res4 = tf.nn.embedding_lookup(w, [[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	res,res2,res4 = sess.run([res,res2,res4])</span><br><span class="line"><span class="keyword">print</span> res,res.shape</span><br><span class="line"><span class="keyword">print</span> res2,res2.shape</span><br><span class="line"><span class="keyword">print</span> res4,res4.shape</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">打印结果</span></span><br><span class="line"><span class="string">res: (3, 2)</span></span><br><span class="line"><span class="string">[[1 2]</span></span><br><span class="line"><span class="string"> [3 4]</span></span><br><span class="line"><span class="string"> [1 2]] </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">res2:(3, 1, 2)</span></span><br><span class="line"><span class="string">[[[1 2]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[3 4]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[1 2]]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">res4: (3, 4, 2)</span></span><br><span class="line"><span class="string">[[[1 2]</span></span><br><span class="line"><span class="string">  [1 2]</span></span><br><span class="line"><span class="string">  [1 2]</span></span><br><span class="line"><span class="string">  [1 2]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[3 4]</span></span><br><span class="line"><span class="string">  [3 4]</span></span><br><span class="line"><span class="string">  [3 4]</span></span><br><span class="line"><span class="string">  [3 4]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[1 2]</span></span><br><span class="line"><span class="string">  [1 2]</span></span><br><span class="line"><span class="string">  [1 2]</span></span><br><span class="line"><span class="string">  [1 2]]] (3, 4, 2)</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/22/tf-multinomial-%E7%94%A8%E6%B3%95%E8%A7%A3%E9%87%8A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/09/22/tf-multinomial-%E7%94%A8%E6%B3%95%E8%A7%A3%E9%87%8A/" class="post-title-link" itemprop="url">tf.multinomial()用法解释</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-22 08:19:10" itemprop="dateCreated datePublished" datetime="2018-09-22T08:19:10+08:00">2018-09-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>tf.multinomial(<strong>logits</strong>, <strong>num_samples</strong>, seed=None, name=None)<br>看一个使用LSTM的代码，使用了这个函数，故学习下。<br>从multinomial分布中采样，样本个数是<strong>num_samples</strong>，每个样本被采样的概率由<strong>logits</strong>给出</p>
<p>参数：<br><strong>logits</strong>: 2-D Tensor with shape [batch_size, num_classes]. Each slice [i, :] represents the unnormalized log probabilities for all classes.2维量，shape是 [batch_size, num_classes]，每一行都是关于种类的未归一化的对数概率<br><strong>num_samples</strong>: 0-D. Number of independent samples to draw for each row slice.标量，表示采样的个数，<strong>更重要的是，它限制了返回张量中元素的范围:{0，1，2，…，num_samples-1 }</strong></p>
<p>返回值：<br>The drawn samples of shape [batch_size, num_samples]，注意元素的取值范围取决于num_samples</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">samples = tf.multinomial(tf.log([[<span class="number">10.</span>, <span class="number">10.</span>, <span class="number">10.</span>]]), <span class="number">5</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	sess.run(samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行结果：array([[2, 1, 2, 2, 0]])</span></span><br></pre></td></tr></table></figure>
<p>存在一个疑问：使用这个函数会直接跑满GPU，并且在交互环境下运行完，依旧占用GPU，退出交互环境后才释放显存</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/15/FP-FN-TP-TN%E4%B8%8E%E7%B2%BE%E7%A1%AE%E7%8E%87-Precision-%E5%8F%AC%E5%9B%9E%E7%8E%87-Recall-%E5%87%86%E7%A1%AE%E7%8E%87-Accuracy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/09/15/FP-FN-TP-TN%E4%B8%8E%E7%B2%BE%E7%A1%AE%E7%8E%87-Precision-%E5%8F%AC%E5%9B%9E%E7%8E%87-Recall-%E5%87%86%E7%A1%AE%E7%8E%87-Accuracy/" class="post-title-link" itemprop="url">FP,FN,TP,TN与精确率(Precision),召回率(Recall),准确率(Accuracy)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-15 09:28:00" itemprop="dateCreated datePublished" datetime="2018-09-15T09:28:00+08:00">2018-09-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="一-FP-FN-TP-TN"><a href="#一-FP-FN-TP-TN" class="headerlink" title="一: FP,FN,TP,TN"></a>一: FP,FN,TP,TN</h3><p>刚接触这些评价指标时,感觉很难记忆FP,FN,TP,TN,主要还是要理解,理解后就容易记住了<br>P(Positive)和N(Negative) 代表<strong>模型的判断结果</strong><br>T(True)和F(False) 评价<strong>模型的判断结果是否正确</strong><br>比如FP:模型的判断是正例(P),实际上这是错误的(F),连起来就是假正例<br>以此类推:<br>FP:假正例<br>FN:假负例<br>TP:真正例<br>TN:真负例</p>
<h3 id="二-精确率-Precision-召回率-Recall-准确率-Accuracy"><a href="#二-精确率-Precision-召回率-Recall-准确率-Accuracy" class="headerlink" title="二:精确率(Precision),召回率(Recall),准确率(Accuracy)"></a>二:精确率(Precision),召回率(Recall),准确率(Accuracy)</h3><p><strong>准确率(Accuracy)</strong>:这三个指标里最直观的就是准确率: 模型判断正确的数据(TP+TN)占总数据的比例</p>
<p>$Acc = \frac {TP+TN}{TP+TN+FP+FN}$ </p>
<p><strong>召回率(Recall)</strong>: <strong>针对数据集中的所有正例(TP+FN)而言</strong>,模型正确判断出的正例(TP)占数据集中所有正例的比例.FN表示被模型误认为是负例但实际是正例的数据.召回率也叫查全率,以物体检测为例,我们往往把图片中的物体作为正例,此时召回率高代表着模型可以找出图片中更多的物体!</p>
<p>$Recall = \frac {TP}{TP+FN}$</p>
<p><strong>精确率(Precision)</strong>:<strong>针对模型判断出的所有正例(TP+FP)而言</strong>,其中真正例(TP)占的比例.精确率也叫查准率,还是以物体检测为例,精确率高表示模型检测出的物体中大部分确实是物体,只有少量不是物体的对象被当成物体</p>
<p>$Precision = \frac {TP}{TP+FP}$</p>
<p>区分好召回率和精确率的关键在于:针对的数据不同,召回率针对的是数据集中的所有正例,精确率针对的是模型判断出的所有正例</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/10/%E5%9C%A8hexo%E4%B8%AD%E4%BD%BF%E7%94%A8mathjax/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/09/10/%E5%9C%A8hexo%E4%B8%AD%E4%BD%BF%E7%94%A8mathjax/" class="post-title-link" itemprop="url">在hexo中使用mathjax</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-10 09:25:09" itemprop="dateCreated datePublished" datetime="2018-09-10T09:25:09+08:00">2018-09-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>之前总是使用mathtype写公式然后再上传图片, 速度太慢了, 而且公式出错后不容易修改, 所以开始用mathjax, 但是原生hexo并不能直接渲染mathjax, 查阅资料总结了一波<br>只需五步:<br>可以先换成淘宝的下载源,增加npm下载速度: npm config set registry <a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a></p>
<h3 id="一-使用Kramed-代替-Marked"><a href="#一-使用Kramed-代替-Marked" class="headerlink" title="一:使用Kramed 代替 Marked"></a>一:使用Kramed 代替 Marked</h3><p>渲染引擎kramed支持mathjax</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<p>打开:博客根目录/node_modules/hexo-renderer-kramed/lib/renderer.js<br>将下面这几句进行更改</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Change inline math rule</span><br><span class="line">function formatText(text) &#123;</span><br><span class="line">  &#x2F;&#x2F; Fit kramed&#39;s rule: $$ + \1 + $$</span><br><span class="line">  return text.replace(&#x2F;&#96;\$(.*?)\$&#96;&#x2F;g, &#39;$$$$$1$$$$&#39;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>更改为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Change inline math rule</span><br><span class="line">function formatText(text) &#123;</span><br><span class="line">  &#x2F;&#x2F; Fit kramed&#39;s rule: $$ + \1 + $$</span><br><span class="line">    return text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="二-使用hexo-renderer-mathjax-代替-hexo-math"><a href="#二-使用hexo-renderer-mathjax-代替-hexo-math" class="headerlink" title="二:使用hexo-renderer-mathjax 代替 hexo-math"></a>二:使用hexo-renderer-mathjax 代替 hexo-math</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-math --save</span><br><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure>
<h3 id="三-更新-Mathjax-的-CDN-链接"><a href="#三-更新-Mathjax-的-CDN-链接" class="headerlink" title="三:更新 Mathjax 的 CDN 链接"></a>三:更新 Mathjax 的 CDN 链接</h3><p>打开:博客根目录/node_modules/hexo-renderer-mathjax/mathjax.html<br>将最下面&lt;script src=后的url改为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;cdnjs.cloudflare.com&#x2F;ajax&#x2F;libs&#x2F;mathjax&#x2F;2.7.1&#x2F;MathJax.js?config&#x3D;TeX-MML-AM_CHTML</span><br></pre></td></tr></table></figure>
<h3 id="四-更改默认转义规则"><a href="#四-更改默认转义规则" class="headerlink" title="四:更改默认转义规则"></a>四:更改默认转义规则</h3><p>打开:博客根目录/node_modules/kramed/lib/rules/inline.js</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;escape: &#x2F;^\\([\\&#96;*&#123;&#125;\[\]()#$+\-.!_&gt;])&#x2F;,      第11行，将其修改为</span><br><span class="line">escape: &#x2F;^\\([&#96;*\[\]()#$+\-.!_&gt;])&#x2F;,</span><br><span class="line">&#x2F;&#x2F;em: &#x2F;^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,    第20行，将其修改为</span><br><span class="line">em: &#x2F;^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,</span><br></pre></td></tr></table></figure>
<h3 id="五-开启mathjax"><a href="#五-开启mathjax" class="headerlink" title="五:开启mathjax"></a>五:开启mathjax</h3><p>在主题的配置文件中,我用的是next主题,那么在其_config.yml中找到mathjax并设置为true</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># MathJax Support</span><br><span class="line">mathjax:</span><br><span class="line">  enable: true</span><br><span class="line">  per_page: true</span><br><span class="line">  cdn: &#x2F;&#x2F;cdn.bootcss.com&#x2F;mathjax&#x2F;2.7.1&#x2F;latest.js?config&#x3D;TeX-AMS-MML_HTMLorMML</span><br></pre></td></tr></table></figure>
<p>之后再写文章时,加上mathjax: true</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">date: 2017&#x2F;8&#x2F;3 18:20:00</span><br><span class="line">tags: hexo</span><br><span class="line">mathjax: true</span><br><span class="line">title: hexo博客MathJax公式渲染</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<p>参考:<br><a href="https://blog.csdn.net/u014630987/article/details/78670258" target="_blank" rel="noopener">如何在 hexo 中支持 Mathjax？</a><br><a href="https://www.cnblogs.com/Ai-heng/p/7282110.html" target="_blank" rel="noopener">hexo博客MathJax公式渲染问题</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/05/%E4%BB%80%E4%B9%88%E6%98%AF%E4%B8%8E%E5%B0%BA%E5%BA%A6%E6%97%A0%E5%85%B3%E7%9A%84%E5%8F%98%E9%87%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/09/05/%E4%BB%80%E4%B9%88%E6%98%AF%E4%B8%8E%E5%B0%BA%E5%BA%A6%E6%97%A0%E5%85%B3%E7%9A%84%E5%8F%98%E9%87%8F/" class="post-title-link" itemprop="url">什么是与尺度无关的变量</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-05 08:37:02" itemprop="dateCreated datePublished" datetime="2018-09-05T08:37:02+08:00">2018-09-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>今天碰到了与尺度无关的平移量和与尺度无关的缩放量，什么是与尺度无关呢？尺度是什么？<br>以图片为例,假设有各种各样尺寸的图片,不同的尺寸就是不同的尺度,<br>与尺度无关就是与不同的尺寸无关<br>要想获得与尺度无关的变量,对变量进行归一化即可,比如图片的宽是w,在宽这个维度上有两个坐标x1,x2,那么(x1-x2)表示图片宽度的某一部分,(x1-x2)/w表示(x1-x2)占图片宽度的比例,这种表示的好处是,对于不同尺寸的图片来说,把变量都放缩到[0,1]之间,从而具有了可比性</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/04/tf-strided-slice-%E5%AE%98%E6%96%B9%E6%A1%88%E4%BE%8B%E8%A7%A3%E9%87%8A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/09/04/tf-strided-slice-%E5%AE%98%E6%96%B9%E6%A1%88%E4%BE%8B%E8%A7%A3%E9%87%8A/" class="post-title-link" itemprop="url">tf.strided_slice()官方示例解释</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-04 20:18:51" itemprop="dateCreated datePublished" datetime="2018-09-04T20:18:51+08:00">2018-09-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p> <strong>tf.strided_slice( input_, begin, end )</strong> 提取张量的一部分</p>
<ol>
<li>一个维度一个维度地看:begin 加 stride,直到二者的和大于等于end</li>
<li><strong>[begin,end)</strong>,左闭右开</li>
<li>清楚各个维度指的是哪部分</li>
<li>返回的张量中,元素的个数:end与begin对应元素做差再相乘,结果取绝对值<br>下面以官方的三个示例为例进行解释,t是一个3*2*3的张量<br><img src="https://upload-images.jianshu.io/upload_images/9608551-3bb29f9b24491f5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"><h2 id="示例1"><a href="#示例1" class="headerlink" title="示例1"></a>示例1</h2>对于tf.strided_slice(t, [1, 0, 0], [2, 1, 3], [1, 1, 1]) ,一个维度一个维度地看.<br>begin的第0维是1,end的第0维是2,begin+stride=1+1=2,2大于等于end的第0维,所以不用继续加stride了,取值区间为[1,2),第0维返回索引为1的元素,即[[3, 3, 3], [4, 4, 4]]<br>再看第1维,取值区间为[0,1),<strong>在第0维结果的基础上</strong>,第1维返回索引为0的元素,即[3, 3, 3]<br>最后看第2维,取值区间为[0,3),<strong>在第1维结果的基础上</strong>,第2维返回索引为0,1,2的元素,即[3, 3, 3]<br>最终结果为[3, 3, 3]<h2 id="示例2"><a href="#示例2" class="headerlink" title="示例2"></a>示例2</h2>tf.strided_slice(t, [1, 0, 0], [2, 2, 3], [1, 1, 1])<br>第0维取值区间为[1,2),返回第0维索引为1的元素,即[[3, 3, 3], [4, 4, 4]]<br>第1维取值区间为[0,2),<strong>在第0维结果的基础上</strong>,返回第1维索引为0,1的元素,即[3, 3, 3], [4, 4, 4]<br>第2维取值区间为[0,3],<strong>在第1维结果的基础上</strong>,返回第2维索引为0,1,2的元素,即[3, 3, 3], [4, 4, 4],这里注意,因为第1维结果是两个list,0,1,2这三个索引分别作用于这两个list<br>最终结果[[3, 3, 3], [4, 4, 4]]<h2 id="示例3"><a href="#示例3" class="headerlink" title="示例3"></a>示例3</h2>tf.strided_slice(t, [1, -1, 0], [2, -3, 3], [1, -1, 1])<br>第0维取值区间[1,2),第0维返回索引为1的元素,即[[3, 3, 3], [4, 4, 4]]<br>第1维取值区间[-1,-3),<strong>在第0维结果的基础上</strong>,返回第1维索引为-1,-2的元素,即[4, 4, 4],[3, 3, 3]<br>第2维取值区间为[0,3),<strong>在第1维结果的基础上</strong>,返回第2维索引为0,1,2的元素,即[4, 4, 4],[3, 3, 3]<br>最终结果[[4, 4, 4],[3, 3, 3]]</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/02/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0-Learning-to-Generate-Time-Lapse-Videos-Using-Multi-StageDynamic-Generative-Adversarial-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/09/02/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0-Learning-to-Generate-Time-Lapse-Videos-Using-Multi-StageDynamic-Generative-Adversarial-Networks/" class="post-title-link" itemprop="url">论文学习:Learning to Generate Time-Lapse Videos Using Multi-StageDynamic Generative Adversarial Networks</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-02 17:27:56" itemprop="dateCreated datePublished" datetime="2018-09-02T17:27:56+08:00">2018-09-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这篇论文收录于KDD2018,有关视频生成的,论文有个<a href="https://sites.google.com/site/whluoimperial/mdgan" target="_blank" rel="noopener">项目主页</a>,题目翻译过来大致是:使用多阶段动态生成对抗网络学习生成time-lapse(延时)视频.<br>多阶段具体来说是两阶段,</p>
<ol>
<li>第一阶段(Base-Net): 注重每一帧内容的真实性   </li>
<li>第二阶段(Refine-Net): 注重帧与帧之间物体的运动</li>
</ol>
<p>下图是模型整体框架,叫做<strong>MD-GAN</strong><br>MD-GAN由base-net和refine-net构成<br><img src="https://upload-images.jianshu.io/upload_images/9608551-8561a8290430e22a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"></p>
<h2 id="Base-Net"><a href="#Base-Net" class="headerlink" title="Base-Net"></a>Base-Net</h2><h3 id="Generator-G1"><a href="#Generator-G1" class="headerlink" title="Generator,G1"></a>Generator,G1</h3><p>G1采用的是encoder-decoder这种结构,具体是采用了多个3D卷积层-反卷积层对;同时还采用了skip connection(构成了U型架构)的策略,3D卷积和skip connection 对视频内容进行了很好的建模.<br>skip connections用于连接与encoder对应的decoder的feature maps,从而使decoder再次利用encoder的信息,减少了信息损失.<br>skip connection是通过identity mapping(恒等映射)实现的</p>
<h3 id="Discriminator-D1"><a href="#Discriminator-D1" class="headerlink" title="Discriminator,D1"></a>Discriminator,D1</h3><p>D1采用的是G1中encoder的网络,除了最后一层用的是sigmoid激活函数而不是ReLU了</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>Lcon是像素级别的L1距离,L1可以使画面更加sharpness<br><img src="https://upload-images.jianshu.io/upload_images/9608551-278cb667c136a574.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.png"></p>
<h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><p>base-net保证内容的sharpness</p>
<h2 id="Refine-Net"><a href="#Refine-Net" class="headerlink" title="Refine-Net"></a>Refine-Net</h2><h3 id="Generator-G2"><a href="#Generator-G2" class="headerlink" title="Generator,G2"></a>Generator,G2</h3><p>G2的网络和G1很像,只不过G2中移除了部分skip connections,具体的是溢出了 “conv1” and “deconv6”, “conv2” and “deconv5”之间的skip connection<br>因为使用G1中那么多的skip connection对视频的动态性不能很好的建模</p>
<h3 id="Discriminator-D2"><a href="#Discriminator-D2" class="headerlink" title="Discriminator,D2"></a>Discriminator,D2</h3><p>D2和D1的结构一样,只不过有3个D2,分别是Y1,Y2,Y对应的D2<br>D2的重点是Gram matrix和ranking loss</p>
<h4 id="Gram-matrix-建模帧与帧之间物体运动的动态性"><a href="#Gram-matrix-建模帧与帧之间物体运动的动态性" class="headerlink" title="Gram matrix 建模帧与帧之间物体运动的动态性"></a>Gram matrix 建模帧与帧之间物体运动的动态性</h4><p>在refine-net中,作者引入Gram matrix作为运动特征的表示(motion feature representation),用来辅助G2学到帧与帧之间的动态特性<br>具体怎么用?</p>
<ol>
<li>首先从D2中提取特征,具体的就是以D2的某一层的输出作为特征,论文中说的是:features of the first and third convolutional layers (after the ReLU layer) of discriminator D2,也就是第一个和第三个卷积层+ReLU激活后的输出作为特征.</li>
<li>接下来用这些特征去计算Gram矩阵,这样做的好处是加入了丰富的时间信息,而动作是随着时间进行的,所以也就是加入了训练集视频片段的动作信息.<br>Gram矩阵的计算如论文所示,这其实是求提取自D2中的特征之间的协方差矩阵,计算得到的Gram矩阵将会用于ranking loss.<br><img src="https://upload-images.jianshu.io/upload_images/9608551-8ba0151b6b77597e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"></li>
</ol>
<h4 id="ranking-loss"><a href="#ranking-loss" class="headerlink" title="ranking loss"></a>ranking loss</h4><p>ranking loss是论文的一大特点,计算得到的Gram矩阵将会用于ranking loss.<br>D2的结构和D1一样,针对D2中某一层的特征,ranking loss计算公式如下所示,这个公式引用自<strong>ContrastingLoss-Generative Semantic Manipulation with Contrasting</strong>这篇论文,原论文用的是l2范式,这里用的是l1范式,也许是因为l1范式对于视频生成任务更合适,确切地说是因为l1范式会提升生成视频的sharpness(清晰度).进一步观察公式,<strong>如果ranking loss小,说明:g(Y2;l)接近g(Y;l),同时g(Y2;l)远离g(Y1;l),换句话说就是生成的视频Y2更接近ground truth Y,同时比起base-net的输出Y1,refine-net的Y2有了进一步的提升(动作上的提升)</strong><br><img src="https://upload-images.jianshu.io/upload_images/9608551-e8a5a271df83348e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.png"></p>
<h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>最终的ranking loss是把所有层的结果加起来(论文中是用了D2的第一层和第三层卷积层输出)<br><img src="https://upload-images.jianshu.io/upload_images/9608551-50ad6a1dff60d8fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4.png"><br>注意到三个D2(Y,Y1,Y2)组成的ranking loss充当Discriminator的一部分</p>
<p>refine-net的损失函数,Lcon是像素级别的L1距离,L1可以是画面更加sharpness<br><img src="https://upload-images.jianshu.io/upload_images/9608551-0d5cd184ce595628.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="6.png"></p>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>最大化D,最小化G<br><img src="https://upload-images.jianshu.io/upload_images/9608551-26e5a27b4460995c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.png"></p>
<h3 id="作用-1"><a href="#作用-1" class="headerlink" title="作用"></a>作用</h3><p>在保证视频内容的清晰度(sharpness)的基础上,提升画面的动态性</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>论文提出了MD-GAN,MD-GAN有两个网络:base-net和refine-net</li>
<li>使用base-net保证内容的sharpness</li>
<li>使用refine-net提升画面的动态性</li>
<li>Generator中的3D卷积与skip connection(通过恒等映射实现)算是亮点</li>
<li>D2中的Gram matrix和ranking loss算是重点</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/02/tensorflow%E8%8E%B7%E5%8F%96%E5%8F%AF%E7%94%A8%E8%BF%90%E7%AE%97%E8%AE%BE%E5%A4%87-CPU-GPU/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/09/02/tensorflow%E8%8E%B7%E5%8F%96%E5%8F%AF%E7%94%A8%E8%BF%90%E7%AE%97%E8%AE%BE%E5%A4%87-CPU-GPU/" class="post-title-link" itemprop="url">tensorflow获取可用运算设备(CPU,GPU)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-02 17:24:04" itemprop="dateCreated datePublished" datetime="2018-09-02T17:24:04+08:00">2018-09-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>正常来说,运行下面两句会打印tensorflow能用的CPU和GPU</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line">print(device_lib.list_local_devices())</span><br></pre></td></tr></table></figure>
<p>结果发现只有CPU可用,查阅资料后发现可能跟tensorflow版本有关,在终端运行pip3 list查看安装的包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 list</span><br></pre></td></tr></table></figure>
<p>发现有两个tensorflow,一个是tensorflow,还有一个是tensorflow-gpu.卸载无gpu版本的tensorflow</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 uninstall tensorflow</span><br></pre></td></tr></table></figure>
<p>重新运行最开始两句代码,结果报错:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: No module named &#39;tensorflow.python&#39;</span><br></pre></td></tr></table></figure>
<p>卸载tensorflow-gpu后重新安装</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip3 uninstall tensorflow-gpu</span><br><span class="line">pip3 install tensorflow-gpu&#x3D;&#x3D;1.4.0</span><br></pre></td></tr></table></figure>
<p>重新运行最开始两句代码,成功:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[name: &quot;&#x2F;device:CPU:0&quot;</span><br><span class="line">device_type: &quot;CPU&quot;</span><br><span class="line">memory_limit: 268435456</span><br><span class="line">locality &#123;</span><br><span class="line">&#125;</span><br><span class="line">incarnation: 13177083330855175469</span><br><span class="line">, name: &quot;&#x2F;device:GPU:0&quot;</span><br><span class="line">device_type: &quot;GPU&quot;</span><br><span class="line">memory_limit: 10968950375</span><br><span class="line">locality &#123;</span><br><span class="line">  bus_id: 1</span><br><span class="line">&#125;</span><br><span class="line">incarnation: 6161624703599064583</span><br><span class="line">physical_device_desc: &quot;device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:00:08.0, compute capability: 6.1&quot;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最开始存在两个tensorflow包:无gpu版本的tensorflow和gpu版本的tensorflow-gpu,可能是默认使用了无gpu版的tensorflow,所以可用设备中没有GPU</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/08/TensorFlow%E8%8E%B7%E5%8F%96Tensor%E7%BB%B4%E5%BA%A6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/08/08/TensorFlow%E8%8E%B7%E5%8F%96Tensor%E7%BB%B4%E5%BA%A6/" class="post-title-link" itemprop="url">TensorFlow获取Tensor维度</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-08 10:25:45" itemprop="dateCreated datePublished" datetime="2018-08-08T10:25:45+08:00">2018-08-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="获取Tensor维度"><a href="#获取Tensor维度" class="headerlink" title="获取Tensor维度"></a>获取Tensor维度</h3><p>比如一个Tensor为<code>a = tf.constant([[1,2,],[3,4]],name=&#39;a&#39;)</code>,有三种方式可以获取a的维度<br>    1. a.shape<br>    2. a.get_shape()<br>    3. tf.shape(a)<br>前两种返回类型是TensorShape,代表静态shape,a.shape.as_list()返回list类型的shape<br>第三种返回类型是Tensor,代表动态shape</p>
<h3 id="动态shape与静态shape"><a href="#动态shape与静态shape" class="headerlink" title="动态shape与静态shape"></a>动态shape与静态shape</h3><p>当需要reshape Tensor时,就能体现出差异了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">b = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>,<span class="number">32</span>])</span><br><span class="line">b_static = b.shape.as_list()</span><br><span class="line">b_dynamic = tf.unstack(tf.shape(b))</span><br><span class="line">dim = [s[<span class="number">1</span>] <span class="keyword">if</span> s[<span class="number">0</span>] <span class="keyword">if</span> <span class="literal">None</span> <span class="keyword">else</span> s[<span class="number">0</span>] <span class="keyword">for</span> s <span class="keyword">in</span> zip(b_static,b_dynamic)]</span><br><span class="line">tf.reshape(b,[dim[<span class="number">0</span>],dim[<span class="number">1</span>]*dim[<span class="number">2</span>]])</span><br><span class="line"><span class="comment">#此时b的维度变成[None,320]</span></span><br><span class="line"><span class="comment">#如果直接用静态shape对b进行reshape会报错</span></span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/08/%E4%BA%A4%E5%8F%89%E7%86%B5%E4%B8%8EKL%E6%95%A3%E5%BA%A6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/08/08/%E4%BA%A4%E5%8F%89%E7%86%B5%E4%B8%8EKL%E6%95%A3%E5%BA%A6/" class="post-title-link" itemprop="url">交叉熵与KL散度</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-08 00:41:05" itemprop="dateCreated datePublished" datetime="2018-08-08T00:41:05+08:00">2018-08-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>老遇到交叉熵作为损失函数的情况,于是总结一下</p>
<h2 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h2><p>交叉熵从KL散度(相对熵)中引出,KL散度(Kullback-Leibler Divergence)公式为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-dd9a7e8d34f5e8a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"><br>KL散度是衡量两个分布之间的差异大小的,KL散度大于等于0,并且越接近0说明p与q这两个分布越像,当且仅当p与q相等时KL散度取0.</p>
<h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><p>在机器学习的分类问题中,常以交叉熵作为损失函数,此时同样可以衡量两个分布的差异.<br>在分类问题中,<strong>某一个样本x</strong>可能是K种类别中的一种,y(x)代表样本x对应<strong>类别的分布</strong>,y^<del>(x)代表x属于各个<strong>类别的预测值的分布</strong>,<strong>这句话描述的是关于类别的分布,而不是样本的分布,不要弄混.</strong><br>训练时,针对某一个标签信息y(x)是已知的,所以讲KL(y(x)||y^</del>(x))中的H(y(x))是个常数,此时KL散度等价于交叉熵,所以交叉熵可以衡量p(x)与q(x)的差异,我们希望q(x)尽可能地接近p(x),等价于最小化交叉熵<br><strong>对于某一个样本x</strong>,其交叉熵为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-0a777dcf5c169aaa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"><br><strong>对于一个数据集x</strong>,其交叉熵为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-5ac60bb2e96072b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.png"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>因为训练集中每个样本的标签是已知的,此时标签和预测的标签之间的KL散度等价于交叉熵.<br>要认识到,<strong>标签可以看成分布</strong>,举例来说,某个分类任务共有4类,其中一个样本的真实标签分布为(0,0,1,0),预测的标签分布为(0.2,0.1,0.5,0.2),使用交叉熵的目的便是使预测的标签分布尽可能接近(0,0,1,0)</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/07/Ubuntu16-04%E8%A7%A3%E5%86%B3%E6%97%A0%E5%A3%B0%E9%9F%B3%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/08/07/Ubuntu16-04%E8%A7%A3%E5%86%B3%E6%97%A0%E5%A3%B0%E9%9F%B3%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">Ubuntu16.04解决无声音问题</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-07 08:50:39" itemprop="dateCreated datePublished" datetime="2018-08-07T08:50:39+08:00">2018-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>接了音响后没有声音,解决方法很简单,只需三步</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">第一步</span><br><span class="line">sudo apt install pavucontrol</span><br><span class="line">第二步</span><br><span class="line">pavucontrol</span><br><span class="line">第三步</span><br><span class="line">在弹出的界面中,切换到Configuration</span><br><span class="line">Built-in Audio选择Analog Stereo Output</span><br></pre></td></tr></table></figure>
<p>测试的时候,我打开了音乐,所以设置正确后立即出声音,Built-in Audio选择其它的大部分选项也可以.下面是各个选项卡下的配置<br><img src="https://upload-images.jianshu.io/upload_images/9608551-c399efa6ec16b872.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/9608551-5ab99ecb6c674d00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/9608551-570d3cf2197ed33b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/06/hexo%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%88%B0%E5%8F%A6%E4%B8%80%E5%8F%B0%E7%94%B5%E8%84%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/08/06/hexo%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%88%B0%E5%8F%A6%E4%B8%80%E5%8F%B0%E7%94%B5%E8%84%91/" class="post-title-link" itemprop="url">hexo博客迁移到另一台电脑</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-08-06 09:00:49" itemprop="dateCreated datePublished" datetime="2018-08-06T09:00:49+08:00">2018-08-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>之前的电脑不怎么用了,准备在新电脑上写博客,所以需要迁移一下,查询了各种资料,也算是重新了解了下hexo</p>
<h2 id="搭建hexo博客环境"><a href="#搭建hexo博客环境" class="headerlink" title="搭建hexo博客环境"></a>搭建hexo博客环境</h2><p>按照<a href="https://www.cnblogs.com/visugar/p/6821777.html" target="_blank" rel="noopener">hexo博客搭建</a>操作即可,通过在gitbash中查看版本检查是否安装成功  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git version</span><br><span class="line">node -v</span><br><span class="line">hexo -v</span><br></pre></td></tr></table></figure>
<h2 id="复制原博客文件夹中的文件"><a href="#复制原博客文件夹中的文件" class="headerlink" title="复制原博客文件夹中的文件"></a>复制原博客文件夹中的文件</h2><p>我主要是按照<a href="https://blog.csdn.net/eternity1118_/article/details/71194395?ref=myread" target="_blank" rel="noopener">hexo迁移</a>这篇文章操作的,实践中有不同的地方  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">将下面的文件或文件夹复制到新的博客目录下进行替换</span><br><span class="line"> _config.yml</span><br><span class="line"> package.json (这个没有用到)</span><br><span class="line"> scaffolds&#x2F; (这个没有用到)</span><br><span class="line"> source&#x2F;</span><br><span class="line"> themes&#x2F;</span><br></pre></td></tr></table></figure>

<h2 id="安装所需模块"><a href="#安装所需模块" class="headerlink" title="安装所需模块"></a>安装所需模块</h2><p>避免下载缓慢可以先换源</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npm.taobao.org </span><br><span class="line">npm info underscore (输出正常反馈信息则说明换源成功)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">本地测试的时候需要用hexo server</span><br><span class="line">npm i hexo-server</span><br><span class="line">将文章部署到github上的模块</span><br><span class="line">npm install hexo-deployer-git --save</span><br><span class="line">安装RSS插件</span><br><span class="line">npm install hexo-generator-feed --save</span><br><span class="line">添加Sitemap,加速网页收录速度</span><br><span class="line">npm install hexo-generator-sitemap --save</span><br></pre></td></tr></table></figure>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>推荐一篇非常详细的文章:<a href="https://www.jianshu.com/p/35e197cb1273" target="_blank" rel="noopener">Hexo搭建Github-Pages博客填坑教程</a><br>.deploy：执行hexo deploy命令部署到GitHub上的内容目录<br>public：执行hexo generate命令，输出的静态网页内容目录<br>scaffolds：layout模板文件目录，其中的md文件可以添加编辑<br>scripts：扩展脚本目录，这里可以自定义一些javascript脚本<br>source：文章源码目录，该目录下的markdown和html文件均会被hexo处理。该页面对应repo的根目录，404文件、favicon.ico文件，CNAME文件等都应该放这里，该目录下可新建页面目录。<br>_drafts：草稿文章<br>_posts：发布文章<br>themes：主题文件目录<br>_config.yml：全局配置文件，大多数的设置都在这里<br>package.json：应用程序数据，指明hexo的版本等信息，类似于一般软件中的关于按钮</p>
<p>Hexo原理就是hexo在执行hexo generate时会在本地先把博客生成的一套静态站点放到public文件夹中，在执行hexo deploy时将其复制到.deploy文件夹中。Github的版本库通常建议同时附上README.md说明文件，但是hexo默认情况下会把所有md文件解析成html文件，所以即使在线生成了README .md，它也会在你下一次部署时被删去。怎么解决呢？<br>在执行hexo deploy前把在本地写好的README.md文件复制到.deploy文件夹中，再去执行hexo deploy。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/31/Ubuntu16-04%E5%AE%89%E8%A3%85Matlab2018a/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/31/Ubuntu16-04%E5%AE%89%E8%A3%85Matlab2018a/" class="post-title-link" itemprop="url">Ubuntu16.04安装Matlab2018a</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-31 22:00:50" itemprop="dateCreated datePublished" datetime="2018-07-31T22:00:50+08:00">2018-07-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="1-文件准备"><a href="#1-文件准备" class="headerlink" title="1.文件准备"></a>1.文件准备</h2><p>我把Matlab2018a安装镜像及破解文件放在了/home/haes/Downloads/matlab下<br><img src="https://upload-images.jianshu.io/upload_images/9608551-6cfb7c077c46201f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"></p>
<h2 id="2-挂载dvd1"><a href="#2-挂载dvd1" class="headerlink" title="2.挂载dvd1"></a>2.挂载dvd1</h2><p>在/home/haes/Downloads/matlab/下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir dvd</span><br><span class="line">sudo mount -t auto -o loop &#x2F;home&#x2F;haes&#x2F;Downloads&#x2F;matlab&#x2F;R2018a_glnxa64_dvd1.iso dvd&#x2F;</span><br><span class="line">sudo &#x2F;home&#x2F;haes&#x2F;Downloads&#x2F;matlab&#x2F;dvd&#x2F;install</span><br></pre></td></tr></table></figure>
<h2 id="3-挂载dvd2"><a href="#3-挂载dvd2" class="headerlink" title="3.挂载dvd2"></a>3.挂载dvd2</h2><p>再打开一个终端:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t auto -o loop &#x2F;home&#x2F;haes&#x2F;Downloads&#x2F;matlab&#x2F;R2018a_glnxa64_dvd2.iso dvd&#x2F;</span><br></pre></td></tr></table></figure>
<p>点击继续安装</p>
<h2 id="4-输入秘钥"><a href="#4-输入秘钥" class="headerlink" title="4.输入秘钥"></a>4.输入秘钥</h2><p>安装过程中选择Use a File Installation Key<br>Key在Matlab 2018a Linux64 Crack文件夹下的readme.txt中<br>选择红框中的key<br><img src="https://upload-images.jianshu.io/upload_images/9608551-cc12a6b70446f5f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"></p>
<h2 id="5-激活Matlab"><a href="#5-激活Matlab" class="headerlink" title="5.激活Matlab"></a>5.激活Matlab</h2><p>断网<br>Matlab默认安装在/usr/local/MATLAB下<br>打开Matlab</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo &#x2F;usr&#x2F;local&#x2F;MATLAB&#x2F;R2018a&#x2F;bin&#x2F;matlab</span><br></pre></td></tr></table></figure>
<p>第一次启动比较慢,等一等<br>出现激活框后,选择:Activate manually without the Internet<br>然后选择破解文件,具体路径是/home/haes/Downloads/matlab/Matlab 2018a Linux64 Crack/license_server.lic<br>还需要再弄一个破解软件,将/home/haes/Downloads/matlab/Matlab 2018a Linux64 Crack/R2018a/bin/glnxa64/matlab_startup_plugins下的libmwlmgrimpl.so复制到/usr/local/MATLAB/R2018a/bin/glnxa64/matlab_startup_plugins/lmgrimpl下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cp -f &#x2F;home&#x2F;haes&#x2F;Downloads&#x2F;matlab&#x2F;Matlab 2018a Linux64 Crack&#x2F;R2018a&#x2F;bin&#x2F;glnxa64&#x2F;matlab_startup_plugins&#x2F;libmwlmgrimpl.so &#x2F;usr&#x2F;local&#x2F;MATLAB&#x2F;R2018a&#x2F;bin&#x2F;glnxa64&#x2F;matlab_startup_plugins&#x2F;lmgrimpl</span><br></pre></td></tr></table></figure>
<p>顺便取消挂载(相当于弹出光盘)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo umount &#x2F;home&#x2F;haes&#x2F;Downloads&#x2F;matlab&#x2F;dvd&#x2F;</span><br></pre></td></tr></table></figure>
<h2 id="6-创建桌面快捷方式"><a href="#6-创建桌面快捷方式" class="headerlink" title="6.创建桌面快捷方式"></a>6.创建桌面快捷方式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit &#x2F;usr&#x2F;share&#x2F;applications&#x2F;Matlab2018a.desktop</span><br></pre></td></tr></table></figure>
<p>在弹出的文本框中输入,保存退出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[Desktop Entry] </span><br><span class="line">Encoding&#x3D;UTF-8 </span><br><span class="line">Name&#x3D;Matlab 2018a</span><br><span class="line">Comment&#x3D;MATLAB</span><br><span class="line">Exec&#x3D;&#x2F;usr&#x2F;local&#x2F;MATLAB&#x2F;R2018a&#x2F;bin&#x2F;matlab</span><br><span class="line">Icon&#x3D;&#x2F;usr&#x2F;local&#x2F;MATLAB&#x2F;R2018a&#x2F;toolbox&#x2F;shared&#x2F;dastudio&#x2F;resources&#x2F;MatlabIcon.png</span><br><span class="line">Terminal&#x3D;true  </span><br><span class="line">Type&#x3D;Application  </span><br><span class="line">Categories&#x3D;Application;</span><br></pre></td></tr></table></figure>
<p>重启使用matlab<br><img src="https://upload-images.jianshu.io/upload_images/9608551-a4f7e84c1e98713f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/27/%E5%8E%9F%E5%A7%8BGAN%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E5%8F%8ATensorFlow%E5%AE%9E%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/27/%E5%8E%9F%E5%A7%8BGAN%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E5%8F%8ATensorFlow%E5%AE%9E%E7%8E%B0/" class="post-title-link" itemprop="url">原始GAN论文笔记及TensorFlow实现</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-27 22:12:35" itemprop="dateCreated datePublished" datetime="2018-07-27T22:12:35+08:00">2018-07-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><ul>
<li>在GAN诞生之前,比起生成模型而言,判别模型更受关注,比如Alex Net,VGG,Google Net,因为典型的生成模型往往具有原理复杂,推导复杂,实现复杂的特点</li>
<li>对于生成模型而言,通常有两种建模方式<ul>
<li>最常见的是对目标对象的概率分布建模,将其表达成具体的某种参数形式,再通过最大似然一类的方法训练模型,如深度玻尔兹曼机DBM,这样做的缺点:通常得到的似然函数无法直接求解,需要借助近似算法或者采样算法</li>
<li>采用非参数的方式建模,如GSN,方法核心:假设一条马尔科夫链的稳态分布是数据的真实分布,然后将马尔科夫链的求解操作替换为可以用梯度反向传播来执行的操作</li>
</ul>
</li>
<li>GAN作为一种训练框架,由两个网络Generator和Discriminator构成,D采用判别式准则辅助训练生成模型G,结构如下,X是真实数据,Z是随机噪声,Z经过Generator后成为X’;X’和X作为Discriminator的输入,Discriminator根据X判断X’是不是真实的数据,并将结果反馈给Generator.GAN目的就是希望X’尽可能地接近X,也就是P_g = P_data<br><img src="https://upload-images.jianshu.io/upload_images/9608551-bc8b683b31cb5c8e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"></li>
</ul>
<h2 id="GAN的两个网络"><a href="#GAN的两个网络" class="headerlink" title="GAN的两个网络"></a>GAN的两个网络</h2><h3 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h3><p>G本来就是做生成的,比如Auto Encoder就是一种生成模型,GAN为什么要增加D呢?因为只用G有缺陷,以AE为例,AE侧重于生成与原图片尽可能相似的图片,但这样会牺牲掉图片中各个component之间的联系,如下图所示<br><img src="https://upload-images.jianshu.io/upload_images/9608551-186e6fe65e05c733.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"><br>对于AE来说,output1更像原图,但是我们写数字时,笔画的中间往往不会有空缺,也就是说,虽然output2最后的笔画拉长了,但比起output1来说更自然,因为output2更care各个component之间的联系.<br>当然了,AE可以通过增加神经网络的层数使得网络可以考虑这种联系,但是生成相同质量图片的情况下,GAN的结构更加简单.</p>
<h3 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h3><p>D虽然是判别模型,但也可以做生成,需要解下面这个式子<br><img src="https://upload-images.jianshu.io/upload_images/9608551-203c29ddba9771eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/200" alt="3.png"><br>也就是对于给定的输入x,遍历所有可能的数据,挑出分数最高的图片作为生成结果.但是首先需要假设D(x)的形式,如果假设D(x)是线性的,那么模型的能力太弱;如果假设是非线性的,又不好解argmax.<br>在GAN中,D的输入是G的输出,G的输出是一张完整的图片,D对一张完整的图片进行判别可以很好地catch到各个component之间的联系,然后将这个信息反馈给G,从而使G生成具有大局观的图片</p>
<h2 id="GAN的数学推导"><a href="#GAN的数学推导" class="headerlink" title="GAN的数学推导"></a>GAN的数学推导</h2><p>IanGoodfellow的论文Generative Adversarial Nets是这样引出GAN的目标函数的<br>对于Discriminator来说,它用来判断输入的数据是真还是假,具体做法是:对真实的数据赋予高分,对虚假的数据赋予低分;也就是希望赋予D(X)高分,赋予D(X’)低分,可以写成如下的形式<br><img src="https://upload-images.jianshu.io/upload_images/9608551-3b3c256460eea30d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4.png"></p>
<ul>
<li>取1-D(X’)是为了满足对数的定义域要求</li>
<li>取对数,个人认为是为了凑alog(x)+blog(1-x)的形式,之后会提到</li>
<li>取期望是把分布P_data和P_g考虑进来<br>对于Generator来说,希望自己生成的数据X’更接近真实数据X,也就是希望D(X’)越大越好,这便体现了G与D的博弈思想,结合G与D的初衷可得目标函数为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-b1590a5a5c2601e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.png"><h3 id="目标函数的有效性"><a href="#目标函数的有效性" class="headerlink" title="目标函数的有效性"></a>目标函数的有效性</h3></li>
</ul>
<p><strong>优化V(D,G)后,等价于实现了P_g = P_data</strong>,下面说明原因:</p>
<h4 id="固定G-优化D"><a href="#固定G-优化D" class="headerlink" title="固定G,优化D"></a>固定G,优化D</h4><p>首先直接使用一个概率论中的定理:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-d336a1928f8e33b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="6.png"><br>将V(D,G)展开<br><img src="https://upload-images.jianshu.io/upload_images/9608551-2f2b4a408beee262.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.png"><br>最后一步合并了两个积分,从而扩大了积分限,两个被积函数在无定义处取0即可<br>刚才提到为什么目标函数采用对数形式,原因如下<br><img src="https://upload-images.jianshu.io/upload_images/9608551-842f7f8be9167459.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="8.png"><br>目标函数正好符合上述定理形式,所以固定G,优化D时D的最优值为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-0ae0910c17f3f527.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="9.png"></p>
<h4 id="固定D-优化G"><a href="#固定D-优化G" class="headerlink" title="固定D,优化G"></a>固定D,优化G</h4><p><img src="https://upload-images.jianshu.io/upload_images/9608551-c55c9e76d3849372.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="10.png"><br>当P_g = P_data时,上面的不等式取等号,C(G)取得最小值,<strong>说明按照上面的方式优化目标函数,效果相当于P_g = P_data,说明了GAN的可行性</strong></p>
<h3 id="优化流程"><a href="#优化流程" class="headerlink" title="优化流程"></a>优化流程</h3><p><img src="https://upload-images.jianshu.io/upload_images/9608551-a9de1900a7675e53.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="11.png"><br>优化D的时候优化了k次,不过论文中实验的时候取k=1<br><strong>在优化G的初期,由于G生成的数据X’很假,所以log(1-D(G(z)))的梯度接近1,有点小,不利于迭代,所以会使用max_G log(D(G(z)))优化G</strong></p>
<h2 id="TensorFlow实现"><a href="#TensorFlow实现" class="headerlink" title="TensorFlow实现"></a>TensorFlow实现</h2><p>完整代码可以参考<a href="http://study.163.com/course/courseMain.htm?courseId=1005703030" target="_blank" rel="noopener">深度学习-GAN专题代码复现</a>中的”GAN的诞生”.<br>如果对logistic regression和交叉熵有一定的认识会对理解代码实现有很大帮助</p>
<ol>
<li>关于交叉熵,可以参考<a href="http://littlehaes.com/2018/08/08/%E4%BA%A4%E5%8F%89%E7%86%B5%E4%B8%8EKL%E6%95%A3%E5%BA%A6/" target="_blank" rel="noopener">交叉熵与KL散度</a>  </li>
<li>关于logistic regression,可以参考<a href="http://littlehaes.com/2018/04/01/Logistic-Regression%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">Logistic Regression逻辑斯蒂回归</a></li>
<li>TF文档中关于logistic loss的解释<br><img src="https://upload-images.jianshu.io/upload_images/9608551-6a80a35ea3e09636.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="12.png"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入噪声从正态分布中采样得到</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xavier_init</span><span class="params">(size)</span>:</span></span><br><span class="line">    in_dim = size[<span class="number">0</span>]</span><br><span class="line">    xavier_stddev = <span class="number">1.</span> / tf.sqrt(in_dim / <span class="number">2.</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.random_normal(shape=size, stddev=xavier_stddev)</span><br><span class="line"><span class="comment"># Generator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(z)</span>:</span></span><br><span class="line">    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)</span><br><span class="line">    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2</span><br><span class="line">    G_prob = tf.nn.sigmoid(G_log_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> G_prob</span><br><span class="line"><span class="comment"># Discriminator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator</span><span class="params">(x)</span>:</span></span><br><span class="line">    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)</span><br><span class="line">    D_logit = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class="line">    D_prob = tf.nn.sigmoid(D_logit)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> D_prob, D_logit</span><br></pre></td></tr></table></figure>
<h3 id="个人总结"><a href="#个人总结" class="headerlink" title="个人总结"></a>个人总结</h3></li>
<li>GAN是一种<strong>框架</strong>,核心思想是<strong>对抗训练</strong>:针对D,希望赋予D(X)高分,赋予D(X’)低分;针对G,希望赋予D(G(z))高分.  </li>
<li>这种对抗训练思想的<strong>有效性</strong>是通过求解下面的目标函数实现的,求解结果是P_G=P_data<br><img src="https://upload-images.jianshu.io/upload_images/9608551-b1590a5a5c2601e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.png"></li>
<li><strong>代码实现时,只要能够体现GAN的核心思想即可</strong>,使用TensorFlow实现原始GAN模型时,由于TF有simoid_cross_entropy_with_logits这个函数,所以可以使用logistic regression对X和X’进行二分类.此时<strong>最大化样本构成的似然函数,相当于最小化样本标签和D输出之间的交叉熵</strong>.</li>
</ol>
<p>最后推荐一下杨双老师的课程,<a href="http://study.163.com/course/courseMain.htm?courseId=1005673004&share=1&shareId=5176782" target="_blank" rel="noopener">深度学习-GAN专题论文研读</a>,老师讲得非常棒</p>
<p>参考:<br><a href="http://study.163.com/course/introduction/1005673004.htm" target="_blank" rel="noopener">杨双:深度学习-GAN专题论文研读</a><br><a href="https://www.bilibili.com/video/av24011528?from=search&seid=8264303730334566834" target="_blank" rel="noopener">李宏毅对抗生成网络</a><br>统计学习方法</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/06/ubuntu16-04%E5%AE%89%E8%A3%85Nvidia%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8-CUDA8-0-cuDNN6-TensorFlow-gpu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/07/06/ubuntu16-04%E5%AE%89%E8%A3%85Nvidia%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8-CUDA8-0-cuDNN6-TensorFlow-gpu/" class="post-title-link" itemprop="url">ubuntu16.04安装Nvidia显卡驱动,CUDA8.0,cuDNN6,TensorFlow-gpu</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-06 20:47:07" itemprop="dateCreated datePublished" datetime="2018-07-06T20:47:07+08:00">2018-07-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a href="http://littlehaes.com/" target="_blank" rel="noopener">Welcome To My Blog</a><br>实验室学姐让跑一个深度学习模型的程序,需要配置环境,查阅各种资料后安装成功,记录一下安装过程,确实挺刺激,也算是开启了自己的深度学习之旅了.  </p>
<h2 id="安装Nvidia显卡驱动"><a href="#安装Nvidia显卡驱动" class="headerlink" title="安装Nvidia显卡驱动"></a>安装Nvidia显卡驱动</h2><h3 id="1-下载驱动"><a href="#1-下载驱动" class="headerlink" title="1. 下载驱动"></a>1. 下载驱动</h3><p>根据自己的系统和显卡型号直接在<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn" target="_blank" rel="noopener">NVIDIA官网</a>,我是64位ubuntu16.04,显卡是1070Ti,安装的驱动型号是390.67</p>
<h3 id="2-禁用nouveau第三方驱动"><a href="#2-禁用nouveau第三方驱动" class="headerlink" title="2. 禁用nouveau第三方驱动"></a>2. 禁用nouveau第三方驱动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在终端输入: sudo gedit &#x2F;etc&#x2F;modprobe.d&#x2F;blacklist.conf  </span><br><span class="line">在最后一行添加：blacklist nouveau</span><br><span class="line">改好后执行：sudo update-initramfs -u</span><br><span class="line">重启后,在终端输入:lsmod | grep nouveau,没有输出则说明禁用成功</span><br></pre></td></tr></table></figure>

<h3 id="3-命令行模式下安装驱动"><a href="#3-命令行模式下安装驱动" class="headerlink" title="3. 命令行模式下安装驱动"></a>3. 命令行模式下安装驱动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">注销或重启进入登录界面,Ctrl+Alt+F1进入命令行模式  </span><br><span class="line">禁用X服务：sudo &#x2F;etc&#x2F;init.d&#x2F;lightdm stop</span><br><span class="line">cd到下载目录,给下载的驱动.run文件赋予可执行权限：sudo chmod a+x NVIDIA-Linux-x86_64-390.67.run</span><br><span class="line">安装：sudo .&#x2F;NVIDIA-Linux-x86_64-390.67.run -no-opengl-files(只安装驱动文件，不安装OpenGL文件)</span><br><span class="line">如果安装过程中出现 the distribution-provided pre-install script failed 继续安装即可</span><br><span class="line">开启X服务：sudo &#x2F;etc&#x2F;init.d&#x2F;lightdm start</span><br><span class="line">重启，重新进入图形界面,在终端输入：nvidia-smi,显示显卡信息则安装成功</span><br></pre></td></tr></table></figure>


<h2 id="安装CUDA8-0"><a href="#安装CUDA8-0" class="headerlink" title="安装CUDA8.0"></a>安装CUDA8.0</h2><h3 id="下载安装CUDA8-0"><a href="#下载安装CUDA8-0" class="headerlink" title="下载安装CUDA8.0"></a>下载安装CUDA8.0</h3><p>从<a href="https://developer.nvidia.com/cuda-80-ga2-download-archive" target="_blank" rel="noopener">官网</a>下载CUDA8.0,install type选择deb(local)<br>下载完成后进入安装包所在文件夹,依次执行下面三句命令进行安装(安装过程中不用安装NVIDIA Graphics Driversl了,已经安装过了,其余的均安装):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install cuda</span><br></pre></td></tr></table></figure>
<h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>sudo gedit /etc/profile<br>在末尾加上两句  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin:$PATH</span><br><span class="line">export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure>
<h3 id="验证是否安装成功"><a href="#验证是否安装成功" class="headerlink" title="验证是否安装成功"></a>验证是否安装成功</h3><p>安装时默认安装了测试用例,以deviceQuery为例  </p>
<ul>
<li>cd /usr/local/cuda-8.0/samples/1_Utilities/deviceQuery</li>
<li>sudo make</li>
<li>./deviceQuery  </li>
<li>显示GPU信息则安装成功</li>
</ul>
<h2 id="安装cuDNN6-0"><a href="#安装cuDNN6-0" class="headerlink" title="安装cuDNN6.0"></a>安装cuDNN6.0</h2><h3 id="下载cuDNN"><a href="#下载cuDNN" class="headerlink" title="下载cuDNN"></a>下载cuDNN</h3><p>从<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">官网</a>下载下面红框中的三个文件(需要用NVIDIA账号登录,用QQ登录即可):<br><img src="https://upload-images.jianshu.io/upload_images/9608551-9e3eed2541bcc3fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"></p>
<h3 id="安装cuDNN"><a href="#安装cuDNN" class="headerlink" title="安装cuDNN"></a>安装cuDNN</h3><p>进入安装包所在目录依次执行下面三句命令进行安装<br>sudo dpkg -i libcudnn6-doc_6.0.21-1+cuda8.0_amd64.deb<br>sudo dpkg -i libcudnn6-dev_6.0.21-1+cuda8.0_amd64.deb<br>sudo dpkg -i libcudnn6_6.0.21-1+cuda8.0_amd64.deb</p>
<h3 id="验证是否安装成功-1"><a href="#验证是否安装成功-1" class="headerlink" title="验证是否安装成功"></a>验证是否安装成功</h3><p>安装时默认安装了测试用例,以/usr/src/cudnn_samples_v6为例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">将这个例子复制到HOME中,执行: sudo cp -r &#x2F;usr&#x2F;src&#x2F;cudnn_samples_v6&#x2F; $HOME</span><br><span class="line">进入到HOME下: cd $HOME&#x2F;cudnn_samples_v6&#x2F;mnistCUDNN</span><br><span class="line">进行编译: make clean &amp;&amp; make</span><br><span class="line">执行脚本: .&#x2F;mnistCUDNN</span><br><span class="line">看到命令行输出 Test passed! 则安装成功</span><br></pre></td></tr></table></figure>
<h3 id="如果出现libcudnn-6-cannot…的问题"><a href="#如果出现libcudnn-6-cannot…的问题" class="headerlink" title="如果出现libcudnn.6: cannot…的问题"></a>如果出现libcudnn.6: cannot…的问题</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">之前安装过cuDNN7,使用时出现问题:ImportError: libcudnn.6: cannot open shared object file: No such file or directory的问题,我的问题是出在cuda的lib64文件夹中没有这个文件</span><br><span class="line">解决方法:在computer中搜索libcudnn,最后在&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu中发现了libcudnn.so.7,将这个文件复制到&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64&#x2F;中,并更新软连接:ln -s libcudnn.so.7 libcudnn.so.6</span><br></pre></td></tr></table></figure>

<h2 id="安装gpu版的TensorFlow"><a href="#安装gpu版的TensorFlow" class="headerlink" title="安装gpu版的TensorFlow"></a>安装gpu版的TensorFlow</h2><p>我用的是ubuntu16.04自带的python2.7,安装的是TensorFlow-gpu-1.4版本,在终端执行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export TF_BINARY_URL&#x3D;https:&#x2F;&#x2F;storage.googleapis.com&#x2F;tensorflow&#x2F;linux&#x2F;gpu&#x2F;tensorflow_gpu-1.4.0-cp27-none-linux_x86_64.whl</span><br><span class="line">sudo pip install --upgrade $TF_BINARY_URL</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/19/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BLDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/06/19/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BLDA/" class="post-title-link" itemprop="url">主题模型之LDA</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-06-19 17:08:03" itemprop="dateCreated datePublished" datetime="2018-06-19T17:08:03+08:00">2018-06-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a href="http://littlehaes.com/2018/06/19/%E6%96%87%E6%9C%AC%E5%BB%BA%E6%A8%A1%E4%B9%8BUnigram-Model/" target="_blank" rel="noopener">文本建模之Unigram Model</a>考虑了先验分布，但是没有考虑主题<br><a href="http://littlehaes.com/2018/06/19/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BPLSA/" target="_blank" rel="noopener">主题模型之PLSA</a>考虑了主题，但是没有考虑先验分布<br>本篇介绍的LDA(Latent Dirichlet Allocation,潜在的狄利克雷分配)主题模型既考虑了先验分布也考虑了主题。<br>预备知识：<br><a href="http://littlehaes.com/2018/03/20/Dirichlet-Multinomial-共轭/" target="_blank" rel="noopener">Dirichlet Multinomial 共轭</a><br><a href="http://littlehaes.com/2018/03/21/Markov-Chain-Monte-Carlo-和-Gibbs-Sampling算法/" target="_blank" rel="noopener">吉布斯采样</a><br><img src="https://upload-images.jianshu.io/upload_images/9608551-8737b7c1e79a5042.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.jpg">  </p>
<p>参考：<br>靳志辉，《LDA数学八卦》</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/19/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BPLSA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/06/19/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BPLSA/" class="post-title-link" itemprop="url">主题模型之PLSA</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-06-19 17:07:40" itemprop="dateCreated datePublished" datetime="2018-06-19T17:07:40+08:00">2018-06-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>上一篇文章介绍了<a href="http://littlehaes.com/2018/06/19/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BPLSA/" target="_blank" rel="noopener">文本建模之Unigram Model</a>，但这个模型太过于简略，本篇文章介绍PLSA(Probabilistic Latent Semantic Analysis,概率化的潜在语义分析)<br><img src="https://upload-images.jianshu.io/upload_images/9608551-6fd99e8411b22b56.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.jpg"><br>参考：<br>靳志辉，《LDA数学八卦》</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/19/%E6%96%87%E6%9C%AC%E5%BB%BA%E6%A8%A1%E4%B9%8BUnigram-Model/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/06/19/%E6%96%87%E6%9C%AC%E5%BB%BA%E6%A8%A1%E4%B9%8BUnigram-Model/" class="post-title-link" itemprop="url">文本建模之Unigram Model</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-06-19 16:13:53" itemprop="dateCreated datePublished" datetime="2018-06-19T16:13:53+08:00">2018-06-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>通过生成文章及语料的例子介绍unigram model<br><img src="https://upload-images.jianshu.io/upload_images/9608551-94ca3d5016a89eed.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.jpg"><br>参考：<br>靳志辉，《LDA数学八卦》</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/13/word2vec%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/06/13/word2vec%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B/" class="post-title-link" itemprop="url">word2vec数学推导过程</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-06-13 15:41:58" itemprop="dateCreated datePublished" datetime="2018-06-13T15:41:58+08:00">2018-06-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a href="http://littlehaes.com/" target="_blank" rel="noopener">Welcome To My Blog</a><br>word2vec包含两种框架，一种是CBOW(Continuous Bag-of-Words Model)，另一种是Skip-gram(Continuous Skip-gram Model)，如下图所示。这两种模型的任务是：进行词的预测，CBOW是预测P(w|context(w))，Skip-gram是预测P(context(w)|w)。当整个词典中所有词的预测任务整体达到最优时，此时的词向量便是我们想要的结果。<br><img src="https://upload-images.jianshu.io/upload_images/9608551-9b2c3a2881181197.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="1.png"><br>word2vec有两种计算方式专门提升训练速度，分别是：Hierarchical Softmax 和 Negative Sampling。<br>本篇文章只写出有关模型的数学推导过程，其它细节可参考peghoty的<a href="https://blog.csdn.net/itplus/article/details/37969519" target="_blank" rel="noopener">word2vec 中的数学</a>,我也是根据这篇文章学习的</p>
<h2 id="Hierarchical-Softmax-with-Continuous-Bag-of-Words-Model"><a href="#Hierarchical-Softmax-with-Continuous-Bag-of-Words-Model" class="headerlink" title="Hierarchical Softmax with Continuous Bag-of-Words Model"></a>Hierarchical Softmax with Continuous Bag-of-Words Model</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-9422833f31693a44.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.jpg"></p>
<h2 id="Hierarchical-Softmax-with-Continuous-Skip-gram-Model"><a href="#Hierarchical-Softmax-with-Continuous-Skip-gram-Model" class="headerlink" title="Hierarchical Softmax with Continuous Skip-gram Model"></a>Hierarchical Softmax with Continuous Skip-gram Model</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-a849676592ecc953.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.jpg"></p>
<h2 id="Negative-Sampling-with-Continuous-Bag-of-Words-Model"><a href="#Negative-Sampling-with-Continuous-Bag-of-Words-Model" class="headerlink" title="Negative Sampling with Continuous Bag-of-Words Model"></a>Negative Sampling with Continuous Bag-of-Words Model</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-1a0d7fd737b39752.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4.jpg"></p>
<h2 id="Negative-Sampling-with-Continuous-Skip-gram-Model"><a href="#Negative-Sampling-with-Continuous-Skip-gram-Model" class="headerlink" title="Negative Sampling with Continuous Skip-gram Model"></a>Negative Sampling with Continuous Skip-gram Model</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-ee82de712a870565.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.jpg"><br>参考<br>Tomas Mikolov, Efficient Estimation of Word Representations in Vector Space<br>peghoty, <a href="https://blog.csdn.net/itplus/article/details/37969519" target="_blank" rel="noopener">word2vec 中的数学</a>  </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/06/word2vec%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91-%E9%83%A8%E5%88%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/06/06/word2vec%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91-%E9%83%A8%E5%88%86/" class="post-title-link" itemprop="url">word2vec论文翻译(部分)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-06-06 17:45:58" itemprop="dateCreated datePublished" datetime="2018-06-06T17:45:58+08:00">2018-06-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>毕设要求翻译5000外文，于是翻译了Mikolov提出word2vec的那篇：Efficient Estimation of Word Representations in Vector Space，标题序号，公式序号，图名称均与原论文一致<br>向量空间中词表示的有效估计  </p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了两个新颖的模型架构用来计算大规模数据集中的连续词向量表示。计算得出的结果通过词相似任务进行衡量。通过将这些结果和目前为止表现最好的基于不同类型的神经网络的方法进行对比后发现，该方法的精度得到大幅度提升，并且计算成本要小得多。举例来说，这个方法用了不到一天的时间从含有16亿个词的数字集中训练出了高质量的词向量。不仅如此，用这些词向量进行语法和语义上的词相似度任务均获得了最好的表现。</p>
<h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>当前很多的自然语言处理系统或技术都把词当做原子单元—词表示就是词在字典中的索引，这导致了在词和词之间不能描述相似度的问题。这样做是因为大规模数据集上应用简单模型得到的结果优于在小规模数据集上应用复杂模型的结果，具体表现在：简单性，鲁棒性，可观测性这三方面。一个具体的例子是广为流传的N元模型（N-gram），它用于统计语言模型中。今天，实际上它可能训练任何数据集（万亿级别的词）。<br>然而，简单模型在很多任务上都有限制。比如说：用于自动语音识别的相关域内数据量是有限的—结果往往由高质量转录的语音数据的规模决定（通常是百万级别的词）。在机器翻译中，各种各样现存的语料仅仅包含不超过十亿级别的词汇量。因此，有些情况下，简单改进基本技术不会导致任何重大进展，我们必须关注更先进的技术。<br>伴随着近些年机器翻译的进展，在更复杂更大规模的数据集上训练模型成为了可能，而且这些方法比简单的模型效果更好。或许用词的分布式表示是最成功的概念，比如说基于神经网络的语言模型明显地优于N元模型。</p>
<h3 id="1-1-论文的目标"><a href="#1-1-论文的目标" class="headerlink" title="1.1 论文的目标"></a>1.1 论文的目标</h3><p>该论文的主要目的就是提出了一个学习高质量词向量的技术，这个技术能够应用在十亿级别的数据集，百万级别的词汇量上。据我们所知，之前没有方法能从几百万的词当中成功地学习50维到100维之间的词向量。<br>我们使用了最近提出的方法并测试其产生的向量的质量，我们不仅仅希望相似的词之间距离更近，而且词有多种相似度。这在曲折语言的上下文中出现过，举例来说，名词可以有不同的尾缀，如果我们在原始向量空间的子空间中搜索一个名词，那我们应该可以找到具有相似尾缀的名词。<br>令人意外的是，词的表示的相似度比简单的语法规则更复杂。使用词偏移技术，在词向量上执行简单的代数运算，一个典型的例子是：King的词向量减去Man的词向量加上Woman的词向量的结果最接近Queen的词向量。<br>在本文中，我们试图通过开发新的模型体系结构来最大化这些向量操作的准确性，以保留词之间的线性规律性。我们设计了一个新的综合测试集用于测量语法和语义规则，并且结果显示许多这样的规律可以高准确度地学习到。此外，我们讨论训练时间和精度如何依赖于词向量的维数和训练数据的数量。</p>
<h3 id="1-2-之前的工作"><a href="#1-2-之前的工作" class="headerlink" title="1.2 之前的工作"></a>1.2 之前的工作</h3><p>把单词表示为连续向量拥有很长的历史；一个用于估计神经网语言模型的非常流行框架模型由Bengio提出，这个模型用一个带有线性投影层和非线性隐藏层的前馈神经网络训练得到词向量和一个统计语言模型。很多人追随这个工作继续深入。</p>
<h2 id="2-模型框架"><a href="#2-模型框架" class="headerlink" title="2 模型框架"></a>2 模型框架</h2><p>很多不同种类的模型提出后用来估计词的连续表示，包括著名的潜在语义分析 (LSA)和潜在狄利克雷分配(LDA)。在本篇论文中，我们把重点放在通过神经网络学习到的词的分布式表示上，就像之前展示的，它能存储词与词之间的线性规则从而使得性能明显优于LSA，同时，与LDA相比，LDA在大规模数据集上的计算代价很大。<br>为比较不同的模型框架，首先我们定义模型的计算复杂度为为充分训练模型所需要的参数的数量。接着，我们在最小化计算复杂度的同时最大化模型的准确度。对于接下来提出的所有模型，训练复杂度正比于：<br>O=E×T×Q  (1)<br>其中，E是指训练的迭代次数，T是训练集中词的数量，Q将在之后的模型框架中具体定义。E的一般选择范围是3-50，T可达到十亿量级，所有模型均使用随机梯度下降和反向传播进行训练。</p>
<h3 id="2-1-前馈神经网络语言模型-NNLM"><a href="#2-1-前馈神经网络语言模型-NNLM" class="headerlink" title="2.1 前馈神经网络语言模型 (NNLM)"></a>2.1 前馈神经网络语言模型 (NNLM)</h3><p>概率化的前馈神经网络语言模型由Bengio 提出，该模型由输入层，投影层，隐藏层，输出层构成。在输入层，前N个词使用 独热(one-hot)编码,V是指词典中词的数量；接着输入层被投影到到投影层P,该投影层的维度是N×D，共享同一个的投影矩阵，因为在任何给定时间只有N个输入有效，所以投影层的组成是性价比相对较高的操作。<br>神经网络语言模型架构在投影层和隐藏层之间的计算变得复杂，因为投影层上的值是稠密的。一个通常的选择是N取10,那么投影层可能包含500-2000个数，与此同时，隐藏层通常有500-1000个数，而且隐藏层用来对词典中的所有词计算概率分布的，这导致输出层含有V个数（V是词典中词的个数），所以对于每一个训练样本来说模型的计算复杂度为：<br>Q =N×D+N×D×H+H×V (2)<br>其中，对计算复杂度影响最大的为H×V这一项。为避免计算该项，Mnih等人提出了一些实际的解决方案，比如用softmax的分层版本，或者通过使用在训练过程中没有归一化的模型来完全避免归一化模型。通过将词典构造成二叉树形式，需要被估计的输出单元的数量可以下降到log2(V)，这样处理的话，对计算复杂度影响最大的就变成了N×D×H这一项。<br>在我们的模型中，我们使用的分层的softmax形式，词典被构造成哈夫曼树的形式，这延续了我们之前的工作：在神经网络语言模型中词频对于获取类别很有效果。在哈夫曼树上，对于高频词赋予短的二进制编码，这进一步减少了需要被估计的输出单元的数量。虽然平衡二叉树需要评估log2(V)数量的输出单元，但是基于哈夫曼树的分层softmax结构只需要评估大约log2(V的一元困惑度)数量的输出单元。举例来说，当词典含有一百万个词时，使用该结构会使评估速度大约提升两倍，但是对于神经网络语言模型来说这并不是主要的提速，因为计算的瓶颈在于N×D×H这项，之后我们将提出不包括隐藏层的结构，这样模型的计算复杂度主要取决于softmax归一化的效率。</p>
<h3 id="2-2-循环神经网络语言模型（RNNLM）"><a href="#2-2-循环神经网络语言模型（RNNLM）" class="headerlink" title="2.2 循环神经网络语言模型（RNNLM）"></a>2.2 循环神经网络语言模型（RNNLM）</h3><p>基于循环神经网络的语言模型被用于突破前馈神经网络语言模型的一些局限性，例如前馈神经网络语言模型需要指定上下文的长度，因为理论上比起浅层神经网络，循环神经网络可以有效地表征更复杂的模式，循环神经网络模型没有投影层，只有输入层，隐藏层、输出层。使用时间延迟连接将隐藏层和自身连接的循环矩阵是这类模型的特殊之处，这允许循环模型形成某种短期记忆，因为来自过去的信息可以由隐藏层状态表示，隐藏层状态根据当前输入和上一时间中隐藏层的状态得到更新。循环神经网络在训练每个例子时的复杂度为:<br>Q=H×H+H×V    (3)<br>这里，词的表示D和隐含层状态H拥有相同的维度，这样，项H×V能够通过使用层次softmax降低至H×log2(V)，复杂度主要取决于H×H。</p>
<h3 id="2-3-神经网络的并行训练"><a href="#2-3-神经网络的并行训练" class="headerlink" title="2.3 神经网络的并行训练"></a>2.3 神经网络的并行训练</h3><p>为了训练庞大的数据集上的模型，我们在称为DistBelief的大规模分布式框架之上实现了几个模型，包括本文提出的前馈神经网络语言模型和本论文提出的新模型。该框架允许我们并行地运行同一模型的多个副本，并且每个副本都通过保留所有参数的中央服务器同步地进行梯度更新。对于这种并行训练，我们使用称为Adagrad的自适应学习速率程序使用小批量且异步的梯度下降。在此框架下，通常使用100个或更多的模型副本，每个副本在数据中心的不同机器上使用多个CPU内核。</p>
<h2 id="3-新对数线性模型"><a href="#3-新对数线性模型" class="headerlink" title="3 新对数线性模型"></a>3 新对数线性模型</h2><p>在本节中，我们提出了两种新的模型体系结构，用于学习试图最小化计算复杂性的词的分布式表示。由前一节的主要结果可知，大部分复杂性是由模型中的非线性隐藏层引起的。虽然这正是使得神经网络如此有吸引力的地方，但我们决定探索可能无法像神经网络那样精确地表示数据的较简单模型，不过可以对更多数据进行有效地训练。<br>新架构直接遵循我们早期工作中提出的架构，我们发现神经网络语言模型可以分两步成功训练：首先，使用简单模型学习连续词向量，然后使用N元神经网络语言模型在这些分布式词表示上进行训练。虽然后来大量的工作重点学习词向量，但我们认为我们之前提出的方法是最简单的。注意，相关模型也早已由Hinton等人提出过。</p>
<h3 id="3-1-连续词袋模型"><a href="#3-1-连续词袋模型" class="headerlink" title="3.1 连续词袋模型"></a>3.1 连续词袋模型</h3><p>首先提出的架构类似于前馈神经网络语言模型，其中非线性隐藏层被移除并且投影层由所有词共享（不仅仅是投影矩阵），因此所有词都被投影到相同的位置（它们的向量被平均）。 我们称这种架构为词袋模型，因为词的顺序不影响投影。<br>此外，我们还使用前面的文字，通过用某个词前面四个词和后面四个词作为输入建立一个对数线性分类器，我们在接下来将要提到的任务里到达了最优的表现，其中训练标准是正确分类当前的词(中间的词)，那么训练的复杂度为：<br>Q= N×D+D×log2(V)    (4)<br>我们把这个模型称为CBOW，不同于标准的词袋模型，它使用上下文的连续的分布表示。这个模型框架如图1所示，注意输入层与投影层之间的权重矩阵由所有位置的词共享，和神经网络语言模型类似。 </p>
<h3 id="3-2-连续的Skip-gram模型"><a href="#3-2-连续的Skip-gram模型" class="headerlink" title="3.2 连续的Skip-gram模型"></a>3.2 连续的Skip-gram模型</h3><p>第二个框架与CBOW相似，但是不再基于上下文预测当前词了，该模型试图优化一个基于同一个句子中的其他词的词分类器；更精确的说，我们使用当前词作为带有连续的投影层的对数线性分类器的输入，然后预测当前词之前和之后一定范围内的词。我们发现增加范围能够提升词向量的质量，但同时也增加了计算复杂度。因为距离当前词远的词通常不如距离它近的词更相关，所以在我们的训练集中通过采样更少的那些距离当前词远的词从而赋给它们更小的权重。<br>这个框架的训练复杂度为:<br>Q=C×（D+D×log2(V))     (5)<br>这里，C是词与词之间的最大距离，所以如果我们取C=5，对于每个训练中的词，我们将随机地从1到C中选择一个数字R，然后使用当前词的前R个词和后R个词作为当前词的正确标签。这需要我们对R×2个单词做分类，分类器的输入为当前词，当前词的前R个词和后R个词作为输出，在下面的实验中，我们取C = 10。<br><img src="https://upload-images.jianshu.io/upload_images/9608551-ea551658c8169737.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/800" alt="1.png">  </p>
<p>图一：新模型架构，CBOW架构根据上下文预测当前词，Skip-gram架构根据当前词预测上下文</p>
<h2 id="4-学习到的关系示例"><a href="#4-学习到的关系示例" class="headerlink" title="4 学习到的关系示例"></a>4 学习到的关系示例</h2><p>表8显示了遵循各种关系的词。我们遵循上面提到的方法：通过减去两个单词向量来定义关系，并将结果添加到另一个单词中。例如，巴黎 - 法国 + 意大利 = 罗马。可以看出，虽然显然还有很大的进一步改进空间，但准确性还是相当不错的（请注意，使用我们的准确度度量假设完全匹配，表8中的结果只会得到约60％的分数）。我们相信在更大维数的更大数据集上训练的单词向量将表现得更好，并且可以开发新的创新应用程序。另一种提高准确性的方法是提供多个关系的例子。通过使用十个例子而不是一个来形成关系向量（我们将各个向量平均到一起），我们发现，在语义语法测试中，我们的最佳模型的准确性提高了约10％。<br>表8 使用来自表4的最佳单词向量的单词对关系的示例<br><img src="https://upload-images.jianshu.io/upload_images/9608551-ade9a60b787715b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"><br>也可以应用矢量运算来解决不同的任务。例如，通过计算单词列表的平均向量并找到最远的单词向量，我们已经观察到了选择不在列表单词的良好准确性。在某些人类智力测试中，这是一种流行的问题。显然，通过使用这些技术仍然有很多东西等待我们发现。</p>
<h2 id="5-结果"><a href="#5-结果" class="headerlink" title="5 结果"></a>5 结果</h2><p>为了比较不同词向量版本的质量，之前的论文通常会使用表格展示样本词和与它最相似的词，在直观上体会词向量的质量。虽然展示法国这个词向量类似于意大利这个词向量(或者其它国家的词向量)很简单，但是将词向量应用于更复杂的相似度任务中更具挑战性。我们知道有很多种词与词之间的相似度类型，比如说，在一定程度上，“大的”的词向量类似于“更大的”词向量，这种类似关系与“小的”的词向量类似于“更小的”词向量一致，另一种类型的相似关系可以是“大的”与“最大的”和“小的”与“最小的”。进一步，我们对拥有相同关系的两对词提出一个问题，比如我们可以问：按照“最大的”类似于“大的”这种关系，“小的”类似于那个词？<br>令人惊讶的是，这种问题可以通过词向量之间简单的代数运算得到答案。为了找到“小的”类似于哪个词，我们仅仅通过计算向量X=“最大的”词向量-“大的”词向量+“小的”词向量，然后再词向量空间搜索一个与该向量余弦距离最近的词向量作为X，即问题的答案(在搜索过程中我们抛弃了问题中出现的词)。当词向量被很好的训练后，通过这种代数运算找到问题的正确答案(“最小的”)是可能的。<br>最后，我们发现，当我们在大规模数据集上训练高维词向量时，得到的向量可以用来回答单词之间非常微妙的语义关系，例如城市和它所属的国家，像法国对于巴黎来说就像德国对于柏林的关系。具有这种语义关系的词向量可以用来改进许多现有的自然语言处理应用，例如机器翻译，信息检索和问答系统，没准可以应用于将来的某项应用中。</p>
<h2 id="6-结论"><a href="#6-结论" class="headerlink" title="6 结论"></a>6 结论</h2><p>在本论文中，通过进行一系列语法和语义的语言任务，我们研究了由各种模型训练得到的词向量的质量。我们观察到，比起流行的神经网络模型（前馈和循环），使用非常简单的模型架构训练高质量的词向量是可能做到的。由于计算复杂度的大幅度降低，从大规模数据集上训练非常精确的高维词向量是可能的。使用DistBelief分布式框架，可能能够在万亿级别的语料（基本上是无限大的词汇量）上训练CBOW和Skip-gram模型。 这比类似模型的最佳结果大上好几个数量级。<br>SemEval-2012任务2是一个有趣的任务，其中使用单词向量进行任务最近被证明明显地优于先前的方法。将公开可用的基于循环神经网络的词向量与其他技术一起使用在Spearman’s rank correlation任务上获得了超过50%的提升。基于神经网络的词向量在之前已经应用于许多其他自然语言处理任务中，例如情感分析和释义检测。可以预料，这些应用可以从本论文提出的模型架构中受益。<br>我们正在进行的工作表明，词向量可以成功地应用于知识库中事实的自动扩展，并且还可以用于验证已有事实的正确性。词向量用于机器翻译的实验结果看起来也很有前景。将来，将我们的技术与潜在关系分析等技术进行比较也会很有趣的。我们相信，我们的综合测试集将有助于研究社区改进估计词向量的现有技术。我们也期望高质量的单词向量将成为未来自然语言处理应用的重要组成部分。</p>
<h2 id="7-接下来的工作"><a href="#7-接下来的工作" class="headerlink" title="7 接下来的工作"></a>7 接下来的工作</h2><p>在完成该论文的最初版本之后，我们开源了单机多线程C ++代码，用于计算词向量，同时使用CBOW模型和Skip-gram模型。训练速度明显高于论文之前记录的速度，举例来说，对于典型的超参数选择，模型每小时大约能够训练十亿级别的词向量。我们还开源了140多万个表示命名实体的向量，这是在超过1000亿词的语料上训练得到的。我们的一些后续工作将在即将发布的NIPS 2013论文中发表。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/22/%E5%AE%8B%E8%AF%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/05/22/%E5%AE%8B%E8%AF%8D/" class="post-title-link" itemprop="url">宋词</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-05-22 20:49:25" itemprop="dateCreated datePublished" datetime="2018-05-22T20:49:25+08:00">2018-05-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>学习之余，读读宋词</p>
<h2 id="姜夔"><a href="#姜夔" class="headerlink" title="姜夔"></a>姜夔</h2><p>姜夔（1155-1221），字尧章，号白石道人，饶州鄱阳（今属江西）人。为人狷洁清高，终老布衣。一生湖海飘零，寄人篱下。但与杨万里、范成大交游并得其赏识，靠诗人萧德藻、贵胄张鉴资助，迹近清客。其词也有咏叹时事者，多数是写湖山之美和身世之慨，感念旧游，卷怀恋人，寄物托请，均精深华妙。词风潇洒而醇雅，笔力峭拔而隽健，讲究韵律，多自度腔，有十七首词自注工尺旁谱，其音节文采为一时之冠。有《白石道人歌曲》六卷行世。</p>
<h3 id="扬州慢"><a href="#扬州慢" class="headerlink" title="扬州慢"></a>扬州慢</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">扬州慢</span><br><span class="line">淮左名都，竹西佳处，解鞍少驻初程。过春风十里，尽荠麦青青。自胡马窥江去后，废池乔木，犹厌言兵。渐黄昏、清角吹寒，都在空城。</span><br><span class="line">杜郎俊赏，算而今、重到须惊。纵豆蔻词工，青楼梦好，难赋深情。二十四桥仍在，波心荡、冷月无声。念桥边红药，年年知为谁生。</span><br></pre></td></tr></table></figure>
<p>扬州慢乃姜夔自度曲，其中原委，见词。这是一首乱后感怀之作。上片写词人初到扬州的所见所感。有虚写，有实写。“淮左名都”、“竹西佳处”，主要出自词人之前对这座名城的耳闻，属虚写；“废池乔木”、“清角吹寒”，则是词人亲眼所见。正因有之前的耳闻，才有了当前的触目惊心。下片以昔日繁华，反衬今日之萧飒、冷落。明月应该是今夕荣枯的唯一见证者吧！而冷月无声，一个“冷”字，生出无边凄凉。逢时必发的桥边红药，是有情的吗？她年年花发，又是为谁而生呢？至此，一种旷古的幽怨，笼罩全篇。</p>
<h3 id="鹧鸪天"><a href="#鹧鸪天" class="headerlink" title="鹧鸪天"></a>鹧鸪天</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">鹧鸪天</span><br><span class="line">元夕有所梦</span><br><span class="line">肥水东流无尽期。当初不合种相思。梦中未比丹青见，暗里忽惊山鸟啼。  </span><br><span class="line">春未绿，鬓先丝。人间别久不成悲。谁教岁岁红莲夜，两处沉吟各自知。</span><br></pre></td></tr></table></figure>
<p>唐圭璋《唐宋词简释》：“此首元夕感梦之作。起局沉痛，谓水无尽期，犹狠无尽期。‘当初’一句，因恨而悔，悔当初错种相思，致今日有此恨也。‘梦中’二句，写缠绵颠倒之情，既经相思遂能不忘，以致入梦，而梦中隐约模糊，又不如丹青所见之真。‘暗里’一句，谓即此隐约模糊之梦，亦不能就做，偏被山鸟惊醒。换头，伤羁旅之久。‘别久不成悲’一语，尤道出人在天涯况味。”</p>
<h3 id="点绛唇"><a href="#点绛唇" class="headerlink" title="点绛唇"></a>点绛唇</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">点绛唇</span><br><span class="line">丁未冬，过吴松作  </span><br><span class="line">燕雁无心，太湖西畔随云去。数峰清苦。商略黄昏雨。  </span><br><span class="line">第四桥边，拟共天随住。今何许。凭阑怀古。残柳参差舞。</span><br></pre></td></tr></table></figure>
<p>上片写景，燕雁无心，随白云而来去；数峰有情，向黄昏而落雨，情景交融，不分彼此。下片吊古伤情，“凭阑怀古”点出题旨，继而以“残柳参差舞”收绾，无穷哀感，都在虚处；令读者吊古伤今，不能自止</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/21/DynamicProgramming%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/04/21/DynamicProgramming%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" class="post-title-link" itemprop="url">DynamicProgramming动态规划</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-21 21:49:55" itemprop="dateCreated datePublished" datetime="2018-04-21T21:49:55+08:00">2018-04-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>刚接触动态规划,突然发现这是个很大的领域,所以就先通过别人的分享以及做题逐步总结了.</p>
<h2 id="DynamicProgramming"><a href="#DynamicProgramming" class="headerlink" title="DynamicProgramming"></a>DynamicProgramming</h2><p>dynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions<br>维基百科说动态规划是解决复杂问题的方法,通过三点实现:</p>
<ol>
<li>将原问题分解为简单的子问题(subproblem)</li>
<li>每个子问题只解决一次</li>
<li>存储每个子问题的解<h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2></li>
<li>将原问题分解为子问题</li>
<li>确定状态<br>状态就是每个子问题所能取到的值,所有状态构成了状态空间</li>
<li>确定边界值(初始状态)</li>
<li>确定递推方程(状态转移方程)<h2 id="经验"><a href="#经验" class="headerlink" title="经验"></a>经验</h2>解题过程中的体会 </li>
<li><strong>对于当前下标对应的元素,选还是不选</strong></li>
<li><strong>从后向前分析,从边界(从前)开始向后实现</strong><ul>
<li>实现时,使用递归时先写边界条件;使用数组时先写边界条件(或者说从边界开始推)</li>
<li>使用时,输入最后一个索引</li>
</ul>
</li>
<li>递归版本:各种情况下要return;循环版本:更新某个值而不是return了!</li>
</ol>
<h2 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h2><h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><p>给定一串数字1,2,4,1,7,8,3,现从中取出某几个数字,使这些数字的和最大(<strong>不能取相邻的数字</strong>)<br><img src="https://upload-images.jianshu.io/upload_images/9608551-7a3e34ac818f355c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png">  </p>
<h4 id="python"><a href="#python" class="headerlink" title="python"></a>python</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#递归版,包含大量重叠子问题,换成循环版</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rec_opt</span><span class="params">(arr,i)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>: <span class="keyword">return</span> arr[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">elif</span> i == <span class="number">1</span>:<span class="keyword">return</span> max(arr[<span class="number">0</span>],arr[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        A = rec_opt(arr,i<span class="number">-2</span>)+arr[i]</span><br><span class="line">        B = rec_opt(arr,i<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> max(A,B)</span><br><span class="line">arr = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">3</span>]</span><br><span class="line">print(rec_opt(arr,len(arr)<span class="number">-1</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#循环版</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">opt = np.zeros(len(arr))</span><br><span class="line">opt[<span class="number">0</span>] = arr[<span class="number">0</span>]</span><br><span class="line">opt[<span class="number">1</span>] = max(opt[<span class="number">0</span>],opt[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,len(arr)):</span><br><span class="line">    A = opt[i<span class="number">-2</span>] + arr[i]</span><br><span class="line">    B = opt[i<span class="number">-1</span>]</span><br><span class="line">    opt[i] = max(A,B)</span><br><span class="line">print(opt[len(arr)<span class="number">-1</span>])</span><br></pre></td></tr></table></figure>

<h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><p>正好学习java中,附带java版本</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//递归版</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">rec_opt</span><span class="params">(<span class="keyword">int</span>[] arr,<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(index==<span class="number">0</span>) <span class="keyword">return</span> arr[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(index==<span class="number">1</span>) <span class="keyword">return</span> Math.max(arr[<span class="number">0</span>],arr[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">int</span> A = rec_opt(arr,index-<span class="number">2</span>)+arr[index];</span><br><span class="line">            <span class="keyword">int</span> B = rec_opt(arr,index-<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">return</span> Math.max(A,B);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span> <span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">   <span class="keyword">int</span>[] arr = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">3</span>&#125;;</span><br><span class="line">   <span class="keyword">int</span> res = rec_opt(arr,arr.length-<span class="number">1</span>);</span><br><span class="line">   System.out.print(res);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//循环版</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span> <span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span>[] arr = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">3</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span>[] opt  = <span class="keyword">new</span> <span class="keyword">int</span>[arr.length];</span><br><span class="line">    <span class="comment">//从边界开始向后实现</span></span><br><span class="line">    opt[<span class="number">0</span>] = arr[<span class="number">0</span>];</span><br><span class="line">    opt[<span class="number">1</span>] = Math.max(arr[<span class="number">0</span>],arr[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i&lt;arr.length;i++)&#123;</span><br><span class="line">        <span class="keyword">int</span> A = opt[i-<span class="number">2</span>] + arr[i];</span><br><span class="line">        <span class="keyword">int</span> B = opt[i-<span class="number">1</span>];</span><br><span class="line">        opt[i] = Math.max(A,B);</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.print(opt[arr.length-<span class="number">1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><p>给定一串数字3,34,4,12,5,2,现从中取出某几个数字,使这些数字的和为S(S为某个整数,选出的数字不能是相邻的)<br><img src="https://upload-images.jianshu.io/upload_images/9608551-873b41bb7a85d212.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"></p>
<h4 id="python-1"><a href="#python-1" class="headerlink" title="python"></a>python</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#递归版</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rec_okay</span><span class="params">(arr,i,S)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> i==<span class="number">0</span>:<span class="keyword">return</span> arr[<span class="number">0</span>]==S</span><br><span class="line">    <span class="keyword">elif</span> i==<span class="number">1</span>:</span><br><span class="line">        A = arr[<span class="number">1</span>] == S</span><br><span class="line">        B = arr[<span class="number">0</span>] == S</span><br><span class="line">        <span class="keyword">return</span> A <span class="keyword">or</span> B</span><br><span class="line">    <span class="keyword">elif</span> S==<span class="number">0</span>: <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">elif</span> arr[i]&gt;S: <span class="keyword">return</span> rec_okay(arr,i<span class="number">-1</span>,S)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        A = rec_okay(arr,i<span class="number">-2</span>,S-arr[i])</span><br><span class="line">        B = rec_okay(arr,i<span class="number">-1</span>,S)</span><br><span class="line">        <span class="keyword">return</span> A <span class="keyword">or</span> B</span><br><span class="line">arr = [<span class="number">3</span>,<span class="number">34</span>,<span class="number">4</span>,<span class="number">12</span>,<span class="number">5</span>,<span class="number">2</span>]</span><br><span class="line">print(rec_okay(arr,len(arr)<span class="number">-1</span>,<span class="number">9</span>))</span><br><span class="line">print(rec_okay(arr,len(arr)<span class="number">-1</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#循环版</span></span><br><span class="line"><span class="comment">#i是索引,j是当前结果s</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">okay</span><span class="params">(arr,S)</span>:</span></span><br><span class="line">    okay = np.zeros((len(arr),S+<span class="number">1</span>),dtype=bool)</span><br><span class="line">    <span class="comment">#从边界向后推,所以先给出边界情况</span></span><br><span class="line">    okay[<span class="number">0</span>,:] = <span class="literal">False</span></span><br><span class="line">    okay[<span class="number">0</span>,arr[<span class="number">0</span>]] = <span class="literal">True</span></span><br><span class="line">    okay[:,<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,len(arr)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,S+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">1</span>:</span><br><span class="line">                okay[<span class="number">1</span>,j] = (arr[<span class="number">1</span>] == j) <span class="keyword">or</span> (arr[<span class="number">0</span>] == j)</span><br><span class="line">            <span class="keyword">elif</span> arr[i]&gt;j:</span><br><span class="line">                okay[i,j] = okay[i<span class="number">-1</span>,j]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                A = okay[i<span class="number">-2</span>,j-arr[i]]</span><br><span class="line">                B = okay[i<span class="number">-1</span>,j]</span><br><span class="line">                okay[i,j] = A <span class="keyword">or</span> B</span><br><span class="line">    <span class="comment">#从边界向后推,直至推导要求的位置    </span></span><br><span class="line">    <span class="keyword">return</span> okay[len(arr)<span class="number">-1</span>,S]</span><br><span class="line">arr = [<span class="number">3</span>,<span class="number">34</span>,<span class="number">4</span>,<span class="number">12</span>,<span class="number">5</span>,<span class="number">2</span>]</span><br><span class="line">print(okay(arr,<span class="number">9</span>))</span><br><span class="line">print(okay(arr,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//递归版</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">rec_okay</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> index, <span class="keyword">int</span> sum)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (index == <span class="number">0</span>) <span class="keyword">return</span> arr[<span class="number">0</span>] == sum;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (index == <span class="number">1</span>) <span class="keyword">return</span> (arr[<span class="number">1</span>] == sum) || (arr[<span class="number">0</span>] == sum);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (sum == <span class="number">0</span>) <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (arr[index] &gt; sum) <span class="keyword">return</span> rec_okay(arr, index - <span class="number">1</span>, sum);</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">boolean</span> A = rec_okay(arr, index - <span class="number">2</span>, sum - arr[index]);</span><br><span class="line">            <span class="keyword">boolean</span> B = rec_okay(arr, index - <span class="number">1</span>, sum);</span><br><span class="line">            <span class="keyword">return</span> A || B;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123;<span class="number">3</span>, <span class="number">34</span>, <span class="number">4</span>, <span class="number">12</span>, <span class="number">5</span>, <span class="number">2</span>&#125;;</span><br><span class="line">        <span class="keyword">boolean</span> res = rec_okay(arr, arr.length - <span class="number">1</span>, <span class="number">9</span>);</span><br><span class="line">        System.out.println(res);</span><br><span class="line">        <span class="keyword">boolean</span> res2 = rec_okay(arr, arr.length - <span class="number">1</span>, <span class="number">10</span>);</span><br><span class="line">        System.out.println(res2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//循环版</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">okay</span><span class="params">(<span class="keyword">int</span>[] arr,<span class="keyword">int</span> sum)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span>[][] okay = <span class="keyword">new</span> <span class="keyword">boolean</span>[arr.length][sum+<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j &lt;= sum;j++) okay[<span class="number">0</span>][j] = j == arr[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j &lt;= sum;j++) okay[<span class="number">1</span>][j] = (arr[<span class="number">1</span>] == j)|| (arr[<span class="number">0</span>] == j);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; arr.length; i++) okay[i][<span class="number">0</span>] = <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i&lt;arr.length; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>; j&lt;=sum; j++)&#123;</span><br><span class="line">                <span class="comment">//如果当前的数字大于当前的结果,则只有一个选项:不能选当前数字</span></span><br><span class="line">                <span class="keyword">if</span>(arr[i]&gt;j)  okay[i][j] = okay[i-<span class="number">1</span>][j];</span><br><span class="line">                <span class="keyword">else</span>  okay[i][j] = okay[i-<span class="number">2</span>][j-arr[i]] || okay[i-<span class="number">1</span>][j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> okay[arr.length-<span class="number">1</span>][sum];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123;<span class="number">3</span>, <span class="number">34</span>, <span class="number">4</span>, <span class="number">12</span>, <span class="number">5</span>, <span class="number">2</span>&#125;;</span><br><span class="line">        <span class="keyword">boolean</span> res = okay(arr,  <span class="number">9</span>);</span><br><span class="line">        System.out.println(res);</span><br><span class="line">        <span class="keyword">boolean</span> res2 = okay(arr,<span class="number">10</span>);</span><br><span class="line">        System.out.println(res2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3"><a href="#3" class="headerlink" title="3"></a>3</h3><p><img src="https://upload-images.jianshu.io/upload_images/9608551-d3e26002ee9c357c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/300" alt="3.png"><br>在上面的数字三角形中寻找一条从顶部到底边的路径，使得路径上所经过的数字之和最大。路径上的每一步都只能往左下或 右下走。只需要求出这个最大和即可，不必给出具体路径。<br><img src="https://upload-images.jianshu.io/upload_images/9608551-2f611cb5b3222756.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/800" alt="4.png"></p>
<h4 id="python-2"><a href="#python-2" class="headerlink" title="python"></a>python</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#递归版</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rec_opt</span><span class="params">(arr,i,j)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> i==<span class="number">0</span>:<span class="keyword">return</span> arr[<span class="number">0</span>,j]</span><br><span class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> max(rec_opt(arr,i<span class="number">-1</span>,j),rec_opt(arr,i<span class="number">-1</span>,j+<span class="number">1</span>))+arr[i,j]</span><br><span class="line">arr = np.array([[<span class="number">4</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">6</span>,<span class="number">5</span>],[<span class="number">2</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">0</span>],[<span class="number">8</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">3</span>,<span class="number">8</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">7</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line">print(rec_opt(arr,<span class="number">4</span>,<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#循环版</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">arr = np.array([[<span class="number">4</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">6</span>,<span class="number">5</span>],[<span class="number">2</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">0</span>],[<span class="number">8</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">3</span>,<span class="number">8</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">7</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#第0行的值不变,从第一行开始调整</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">5</span>-i):</span><br><span class="line">        arr[i,j] = max(arr[i<span class="number">-1</span>,j],arr[i<span class="number">-1</span>,j+<span class="number">1</span>]) + arr[i,j]</span><br><span class="line"><span class="keyword">print</span> (arr[<span class="number">4</span>,<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<h4 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//递归版</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">rec_opt</span><span class="params">(<span class="keyword">int</span>[][] arr,<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (i == <span class="number">0</span>) <span class="keyword">return</span> arr[<span class="number">0</span>][j];</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> Math.max(rec_opt(arr,i-<span class="number">1</span>,j),rec_opt(arr,i-<span class="number">1</span>,j+<span class="number">1</span>)) + arr[i][j];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[][] arr = &#123;&#123;<span class="number">4</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">6</span>,<span class="number">5</span>&#125;,&#123;<span class="number">2</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">0</span>&#125;,&#123;<span class="number">8</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,&#123;<span class="number">3</span>,<span class="number">8</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,&#123;<span class="number">7</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;&#125;;</span><br><span class="line">        <span class="keyword">int</span> res = rec_opt(arr,<span class="number">4</span>,<span class="number">0</span>);</span><br><span class="line">        System.out.println(res);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//循环版</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[][] arr = &#123;&#123;<span class="number">4</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">6</span>,<span class="number">5</span>&#125;,&#123;<span class="number">2</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">0</span>&#125;,&#123;<span class="number">8</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,&#123;<span class="number">3</span>,<span class="number">8</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,&#123;<span class="number">7</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;&#125;;</span><br><span class="line">        <span class="comment">//第0行不变,从第1行开始调整</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">5</span>; i++)</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">5</span> - i; j++)&#123;</span><br><span class="line">                arr[i][j] = Math.max(arr[i-<span class="number">1</span>][j],arr[i-<span class="number">1</span>][j+<span class="number">1</span>]) + arr[i][j];</span><br><span class="line">            &#125;</span><br><span class="line">        System.out.println(arr[<span class="number">4</span>][<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>




<p>参考:<br><a href="https://www.bilibili.com/video/av18512769?t=32" target="_blank" rel="noopener">正月点灯笼</a><br><a href="https://blog.csdn.net/baidu_28312631/article/details/47418773" target="_blank" rel="noopener">教你彻底学会动态规划——入门篇</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/16/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E6%97%A0%E6%B3%95%E5%AD%A6%E4%B9%A0XOR-%E5%BC%82%E6%88%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/04/16/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E6%97%A0%E6%B3%95%E5%AD%A6%E4%B9%A0XOR-%E5%BC%82%E6%88%96/" class="post-title-link" itemprop="url">线性模型无法学习XOR(异或)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-16 23:02:16" itemprop="dateCreated datePublished" datetime="2018-04-16T23:02:16+08:00">2018-04-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>如果试图通过线性模型去学习异或,是做不到的,计算过程及原因如下图<br>随便一记主要是想提醒自己,计算时要细心,尤其是关于向量的计算,别把常数和向量弄混<br><img src="https://upload-images.jianshu.io/upload_images/9608551-7f1f862b88e73a01.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="计算过程.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-e5651b6406c6e37a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="why.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/09/LeetCode/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/04/09/LeetCode/" class="post-title-link" itemprop="url">LeetCode</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-09 23:00:35" itemprop="dateCreated datePublished" datetime="2018-04-09T23:00:35+08:00">2018-04-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>开始刷LeetCode,希望能保持每天一道的速度,使用Java,顺便起到学习Java的作用<br>为了避免篇幅较多的问题,我把解题思路及代码放到了<a href="https://github.com/smallhaes/LeetCode" target="_blank" rel="noopener">github</a>上.  </p>
<h2 id="Done"><a href="#Done" class="headerlink" title="Done"></a>Done</h2><ol>
<li>Two Sum (easy) 2018.4.12</li>
<li>Add Two Numbers (medium) 2018.4.13</li>
<li>Longest Substring Without Repeating Characters (medium) 2018.4.13</li>
<li><strong>Median of Two Sorted Arrays (hard)</strong> 2018.4.14</li>
<li>Longest Palindromic Substring (medium) 2018.4.15</li>
<li>ZigZag Conversion (medium) 2018.4.16</li>
<li>Reverse Integer (easy) 2018.4.17</li>
<li>String to Integer (atoi) (medium) 2018.4.17</li>
<li>Palindrome Number (easy) 2018.4.18<br>明天毕设中期答辩,正好碰上hard,明天补上 2018.4.19<br>突然接到的任务,还在弄统计显著性检验,明天补 2018.4.20</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/06/Maximum-Entropy-Model%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/04/06/Maximum-Entropy-Model%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">Maximum Entropy Model最大熵模型</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-06 17:08:53" itemprop="dateCreated datePublished" datetime="2018-04-06T17:08:53+08:00">2018-04-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>最大熵模型(Maximum Entropy Model)属于对数线性模型,由最大熵原理推导实现.</p>
<h2 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h2><p>最大熵原理是概率模型学习的一个准则.<br>最大熵原理认为,学习概率模型时,在所有可能的概率模型(分布)中,熵最大的模型是最好的模型.<br>通常用约束条件来确定概率模型的集合,所以,最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型<br><img src="https://upload-images.jianshu.io/upload_images/9608551-6f99a7d689d257b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"><br>直观地,</p>
<ul>
<li>最大熵原理认为要选择的概率模型首先必须满足已有的事实,即约束条件,<strong>在没有更多信息的情况下,那些不确定的部分都是”等可能的”</strong>  </li>
<li><strong>等概率表示了对事实的无知.因为没有更多信息,所以取等概率是合理的</strong></li>
<li><strong>最大熵原理通过熵的最大化来表示等可能性</strong>  </li>
<li><strong>“等可能性”不容易操作,而熵则是一个可优化的数值指标</strong>  </li>
</ul>
<h2 id="最大熵模型的定义"><a href="#最大熵模型的定义" class="headerlink" title="最大熵模型的定义"></a>最大熵模型的定义</h2><p>将最大熵原理应用到分类得到最大熵模型<br><strong>假设分类模型是一个条件概率分布P(Y|X)</strong>,<img src="https://upload-images.jianshu.io/upload_images/9608551-5d7f22e8cb24aa76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"><br><strong>这个模型表示的是,对于给定的输入X,以条件概率P(Y|X)输出Y</strong><br>给定一个训练集T={(x1,y1),…,(xn,yn)},学习的目标是用最大熵原理选择最好的分类模型<br>首先考虑模型应该满足的条件.给定训练数据集,可以确定联合分布P(X,Y)的经验分布和边缘分布P(X)的经验分布,表示为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-4b4e598aaf46ba6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.png"><br>引入约束<br><img src="https://upload-images.jianshu.io/upload_images/9608551-6f85518da66f7e81.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4.png"><br>联合分布的期望:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-5d2c829f50f33a3c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.png"><br>期望作为约束:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-a4ed64caff7984b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="6.png"><br>最大熵模型的定义:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-667e62478f5affbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.png">  </p>
<h2 id="最大熵模型的学习"><a href="#最大熵模型的学习" class="headerlink" title="最大熵模型的学习"></a>最大熵模型的学习</h2><h3 id="拉格朗日对偶性"><a href="#拉格朗日对偶性" class="headerlink" title="拉格朗日对偶性"></a>拉格朗日对偶性</h3><p>最大熵模型的学习可形式化为约束最优化问题.<br><img src="https://upload-images.jianshu.io/upload_images/9608551-bf679ca7f9512848.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/800" alt="8.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-b2117bb8e7b93cac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/800" alt="9.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-ab76ad3a872796aa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="10.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-cedd4864a95aede0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="11.png"><br>由于<strong>拉格朗日函数L(P,w)是P的凸函数,等式约束是仿射的</strong>,所以原始问题的解与对偶问题的解是等价的.这样就可以通过求解对偶问题来求解原始问题<br><img src="https://upload-images.jianshu.io/upload_images/9608551-16eba520dd3550d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/800" alt="12.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-124db14966754ff0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/800" alt="13.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-c9d17644327c357f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/800" alt="14.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-955f038fa630ccb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/800" alt="15.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-a152ac65f5d4b32e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/800" alt="16.png">  </p>
<h3 id="最大化Ψ-x-等价于MLE"><a href="#最大化Ψ-x-等价于MLE" class="headerlink" title="最大化Ψ(x)等价于MLE"></a>最大化Ψ(x)等价于MLE</h3><p>下面证明对偶函数的极大化等价于最大熵模型的极大似然估计<br><img src="https://upload-images.jianshu.io/upload_images/9608551-4717d507d942683e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/800" alt="17.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-3942386f25f598f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/800" alt="18.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-6056c449baa407e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="19.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-5b38b59ced4171c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/800" alt="20.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-a26f59930c721e85.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/800" alt="21.png"><br>这样,最大熵模型的学习问题就转换为具体求解对数似然函数极大化或对偶函数极大化的问题<br>可以将最大熵模型写成更一般的形式<br><img src="https://upload-images.jianshu.io/upload_images/9608551-d72e543dabd160d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/800" alt="22.png"></p>
<p>最大熵模型与logistic回归模型有类似的形式,它们又称为对数线性模型(log linear model).模型学习就是在给定的训练集下对模型进行极大似然估计或正则化的极大似然估计  </p>
<h2 id="模型学习的最优化算法"><a href="#模型学习的最优化算法" class="headerlink" title="模型学习的最优化算法"></a>模型学习的最优化算法</h2><h3 id="改进的迭代尺度法"><a href="#改进的迭代尺度法" class="headerlink" title="改进的迭代尺度法"></a>改进的迭代尺度法</h3><p>改进的迭代尺度法(improved iterative scaling,IIS)的想法是:假设最大熵模型当前的参数向量是w=(w1,w2,…,wn)^T,我们<strong>希望找到一个新的参数向量w+δ=(w1+δ1,w2+δ2,…,wn+δn)^T,使得模型的对数似然函数值增大.如果能有这样一种参数向量更新的方法τ:w→w+δ,那么就可以重复使用这一方法,直至找到对数似然函数的最大值.</strong>  </p>
<h4 id="Jensen-不等式"><a href="#Jensen-不等式" class="headerlink" title="Jensen 不等式"></a>Jensen 不等式</h4><p>先引出Jensen不等式,它是凸函数必满足的不等式,下面的推导过程会用到<br><img src="https://upload-images.jianshu.io/upload_images/9608551-b7d34d6f9a79f1ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="24.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-80213cdcbea66f7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="25.png"></p>
<h4 id="IIS推导过程"><a href="#IIS推导过程" class="headerlink" title="IIS推导过程"></a>IIS推导过程</h4><p>对数似然为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-cb7089369d313ee5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="23.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-d43c088857fb0eca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="26.png"><br><strong>注意:Z_(w+δ)与Z_(w)之间有这样的关系:</strong><br><img src="https://upload-images.jianshu.io/upload_images/9608551-83b8cb4b9fc18d35.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="27.png"><br>推导似然函数改变量的下界:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-01c5b1cef0311aa9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="28.png"><br><strong>如果能找到适当的δ使下界A(δ|w)提高,那么对数似然函数也会提高</strong>.然而,函数A(δ|w)中的δ是一个向量,含有多个变量,<strong>不易同时优化</strong>.<br><strong>IIS试图一次只优化其中一个变量δi,而固定其它变量δj,j≠i.</strong><br>为达到这一目的,IIS进一步降低下界A(δ|w),<strong>下降后方便提升</strong>.具体地,IIS引进一个量f^#(x,y)<br><img src="https://upload-images.jianshu.io/upload_images/9608551-bf28ea03386e3179.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="29.1.png"><br><strong>因为fi(x,y)是二值函数,故f^#(x,y)表示所有特征函数中fi(x,y)值为1的个数</strong><br>将A(δ|w)改写为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-fe55b98e095168d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="29.png"><br>降低下界后:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-33c85c7ed7c404d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="30.png"><br>这里,B(δ|w)是对数似然函数改变量的一个新的(相对不紧的)下界.<br><img src="https://upload-images.jianshu.io/upload_images/9608551-5455a0a1fe0da379.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="31.png"> </p>
<h4 id="IIS算法流程"><a href="#IIS算法流程" class="headerlink" title="IIS算法流程"></a>IIS算法流程</h4><p><img src="https://upload-images.jianshu.io/upload_images/9608551-6696d3e016b39847.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="32.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-6a3c4b5e3f647c8b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="33.png"><br>关于牛顿法,可参考之前的文章,<a href="http://littlehaes.com/2018/03/26/Newton-method-and-quasi-Newton-method牛顿法与拟牛顿法/" target="_blank" rel="noopener">牛顿法</a>  </p>
<h3 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h3><p>最大熵模型学习还可以应用拟牛顿法.关于拟牛顿法,可参考之前的文章,<a href="http://littlehaes.com/2018/03/26/Newton-method-and-quasi-Newton-method牛顿法与拟牛顿法/" target="_blank" rel="noopener">拟牛顿法</a><br><img src="https://upload-images.jianshu.io/upload_images/9608551-c1514f551233b5f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="34.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-34f8603800be11e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="35.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-747be278214efc5e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="36.png"></p>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><p><img src="https://upload-images.jianshu.io/upload_images/9608551-d18ca5d620e826e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="37.png">  </p>
<p>参考:<br>李航,统计学习方法  </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/02/Matrix-Derivative%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/04/02/Matrix-Derivative%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/" class="post-title-link" itemprop="url">Matrix Derivative矩阵求导</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-02 10:30:18" itemprop="dateCreated datePublished" datetime="2018-04-02T10:30:18+08:00">2018-04-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>学习机器学习算法时总碰见矩阵求导,现学习一波,主要总结下<br>注意:这里只涉及实数的求导,研究通信的人可能接触的往往是负数求导<br>矩阵可以写成列向量(column vectors)或行向量(row vectors)的形式,这两种不同的形式把矩阵求导分成了两种不同的情况  </p>
<h2 id="求导类型"><a href="#求导类型" class="headerlink" title="求导类型"></a>求导类型</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-5383ae601da3e1ad.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="1.jpg"><br>表格列举了六种不同的矩阵求导类型,粗体代表向量或者矩阵(其实标量和向量也可以看作矩阵).<br>表格中还有三个空格没写出,实际上也是存在,但暂时先不讨论,因为这三种情况的求导结果大部分都是高于二阶的张量(tensor)形式,与常见的二维矩阵形式不同.</p>
<h2 id="布局约定Layout-conventions"><a href="#布局约定Layout-conventions" class="headerlink" title="布局约定Layout conventions"></a>布局约定Layout conventions</h2><p>机器学习中,以线性回归为例,每个输入都有多个属性,在表示属性时可以采用列向量或者行向量的形式,这两种形式会造成求导结果形式的不同.<br>注意是形式上的不同,因为本质上形式的不同不会影响求导结果,只不过将结果按照不同的方式组织起来,方便进一步运算<br>布局决定(Layout conventions)就是为了将不同形式的求导分类.分为两种布局:分子布局(numerator layout)和分母布局(denominator layout)<br>通俗解释,现规定<strong>向量或者矩阵</strong>分为原始形式和转置形式两种,比如在线性回归中我们把列向量作为属性值的原始形式,其转置形式就是行向量  </p>
<ul>
<li>对于分子布局(numerator layout),求导结果中分子保持原始形式,分母为转置形式</li>
<li>对于分母布局(denominator layout),求导结果中分子为转置形式,分母保持原始形式<br>下图展示各种类型求导与两种布局之间的关系<br><img src="https://upload-images.jianshu.io/upload_images/9608551-ad2bcd0dd32a86b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png">  <h3 id="numerator-layout"><a href="#numerator-layout" class="headerlink" title="numerator layout"></a>numerator layout</h3>将上述表格中的分子布局单独拿出来,求导结果如下<br><img src="https://upload-images.jianshu.io/upload_images/9608551-2d737adcce52523a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="3.png"><br>下面的两种定义只在分子布局中有意义<br><img src="https://upload-images.jianshu.io/upload_images/9608551-a52c7ad2bd465123.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="4.png"><h3 id="denominator-layout"><a href="#denominator-layout" class="headerlink" title="denominator layout"></a>denominator layout</h3>将上述表格中的分母布局单独拿出来,求导结果如下<br><img src="https://upload-images.jianshu.io/upload_images/9608551-f7bde8dad0702aa0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="5.png"></li>
</ul>
<h2 id="常见求导结果"><a href="#常见求导结果" class="headerlink" title="常见求导结果"></a>常见求导结果</h2><p>现给出常见的求导结果,推导相关公式时可以查表<br>求导有链式法则(Chain Rule),但是矩阵乘积不满足交换律,所以链式法则对于matrix-by-scalar derivatives和scalar-by-matrix derivatives这两种情况不适用<br>下面贴出三种求导结果</p>
<h3 id="Vector-by-vector"><a href="#Vector-by-vector" class="headerlink" title="Vector-by-vector"></a>Vector-by-vector</h3><p>之所以先展示vector-by-vector的表格,是因为所有适用于vector-by-vector求导的操作也直接适用于vector-by-scalar or scalar-by-vector这两种情况<br><img src="https://upload-images.jianshu.io/upload_images/9608551-adac6ef571a02d01.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="6.png"></p>
<h3 id="Scalar-by-vector"><a href="#Scalar-by-vector" class="headerlink" title="Scalar-by-vector"></a>Scalar-by-vector</h3><p><img src="https://upload-images.jianshu.io/upload_images/9608551-6ecb61e770a06fa6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.png"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/9608551-aa592b312f54684c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="8.png"></p>
<h3 id="Vector-by-scalar"><a href="#Vector-by-scalar" class="headerlink" title="Vector-by-scalar"></a>Vector-by-scalar</h3><p><img src="https://upload-images.jianshu.io/upload_images/9608551-2042615480b176cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="9.png">  </p>
<p>参考:<br><a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">Matrix calculus</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/01/Logistic-Regression%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/04/01/Logistic-Regression%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92/" class="post-title-link" itemprop="url">Logistic Regression逻辑斯蒂回归</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-01 15:30:04" itemprop="dateCreated datePublished" datetime="2018-04-01T15:30:04+08:00">2018-04-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><p>logistic回归是统计学习中的经典分类方法,他属于对数线性模型,logistic回归来源于logistic分布,先从logistic分布说起</p>
<h2 id="Logistic-distribution"><a href="#Logistic-distribution" class="headerlink" title="Logistic distribution"></a>Logistic distribution</h2><p>设X是连续随机变量,X服从logistic分布,其分布函数和概率密度函数如下:</p>
<h3 id="分布函数"><a href="#分布函数" class="headerlink" title="分布函数"></a>分布函数</h3><p>其中,μ为位置参数,s为形状参数<br><img src="https://upload-images.jianshu.io/upload_images/9608551-95edc9f8477dd868.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-a3ff0b4a2c46e68b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"><br>分布函数即为通常所说的logistic函数,其图像关于(μ,0.5)对称,满足:<br>F(-x+μ)+F(x-μ)=1<br>曲线在中心附近增长速度较快,在两端增长速度较慢.(<strong>这个特性将使得使用梯度下降优化模型时,可以对误分样本快速调整</strong>)<br>形状参数s的值越小,曲线在中心附近增长得越快  </p>
<h3 id="概率密度函数"><a href="#概率密度函数" class="headerlink" title="概率密度函数"></a>概率密度函数</h3><p>其中,μ为位置参数,s为形状参数<br><img src="https://upload-images.jianshu.io/upload_images/9608551-cb5e4d5dc985ae80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-a1f50bb8a1e33a11.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4.png">   </p>
<h2 id="二项logistic回归"><a href="#二项logistic回归" class="headerlink" title="二项logistic回归"></a>二项logistic回归</h2><p>二项logistic回归(binomial logistic regression)是一种分类模型,<strong>由条件概率分布P(Y|X)表示</strong>,形式为参数化的logistic分布.这里,<strong>随机变量X取值为实数,随机变量Y取值为1或0.通过监督学习的方法来估计模型参数</strong></p>
<h3 id="概率的公理化定义"><a href="#概率的公理化定义" class="headerlink" title="概率的公理化定义"></a>概率的公理化定义</h3><p>柯尔莫哥洛夫给出了概率的公理化定义:</p>
<ul>
<li>0≤P(A)≤1</li>
<li>P(Ω)=1,P(∅)=0,必然事件概率为1,不可能事件概率为0</li>
<li>加法定理:<strong>若干个互斥事件之和的概率等于各事件的概率之和</strong>,即P(A1+A2+…)=P(A)+P(B)+…事件的个数可以是有限的或无限的<h3 id="条件概率分布"><a href="#条件概率分布" class="headerlink" title="条件概率分布"></a>条件概率分布</h3>二项logistic回归模型是如下的条件概率分布(<strong>该条件概率分布满足上述公理化定义,所以可以作为概率,通过训练使得这个概率值更准确</strong>):  </li>
</ul>
<p><strong>P(Y=1|x)=F(X≤x),P(Y=0|x)=1-F(X≤x)=F(X&gt;x)</strong><br><img src="https://upload-images.jianshu.io/upload_images/9608551-29bc7f40bf18fb41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.png"><br>或表示成如下形式<br><img src="https://upload-images.jianshu.io/upload_images/9608551-32ca35e02ba1973d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="6.png"><br>logistic回归采用的是logistic分布函数,对于任意x,F(x)最小值为0,最大值为1,相当于把实数域R映射到(0,1)上.<br>对于给定的输入实例x,按照上式求出P(Y=1|x)和P(Y=0|x),比较两个条件概率的大小,将实例x分到概率值较大的那一类.  </p>
<h3 id="logistic回归模型的特点"><a href="#logistic回归模型的特点" class="headerlink" title="logistic回归模型的特点"></a>logistic回归模型的特点</h3><p>事件发生的概率与不发生的概率之比称作事件的几率(odds),考察logistic回归的对数几率(log odds)或logit函数:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-8a0a95e665434b3e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.png"><br>这就是说,在logistic回归模型中,输出Y=1的对数几率是输入x的线性函数.或者说,输出Y=1的对数几率是由输入x的线性函数表示的模型.(同样地,输出Y=0的对数几率是-wx,x越大则Y=0的对数几率越小,x越小则Y=0的对数几率越大.这样说不满足几率的定义,但是这样解释没有问题,嗯)<br><strong>换一个角度看,考虑对输入x进行分类的线性函数wx</strong>,其值域为实数域.注意这里x∈R_(n+1),w∈R_(n+1).通过logistic回归模型可以将wx转换为概率.这时,线性函数wx的值越接近正无穷,概率值越接近1;线性函数wx的值越接近负无穷,概率值越接近0<br>这样的模型就是logistic回归模型</p>
<h3 id="模型参数估计"><a href="#模型参数估计" class="headerlink" title="模型参数估计"></a>模型参数估计</h3><p><img src="https://upload-images.jianshu.io/upload_images/9608551-1aec11875f7ca8a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="8.png"><br>关于极大似然法(Maximum Likelihood Estimation),可以参考之前的文章<a href="http://littlehaes.com/2018/03/30/Maximum-Likelihood-Estimation极大似然估计/" target="_blank" rel="noopener">极大似然估计</a><br><img src="https://upload-images.jianshu.io/upload_images/9608551-f1876db755479d31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="9.png"><br><strong>求得使L(w)取得最大值的w也就得到logistic回归模型了</strong><br>logistic回归学习中通常采用的方法是梯度下降法(Gradient descent)和拟牛顿法(Quasi Newton method),见之前的文章<a href="http://littlehaes.com/2018/03/25/Gradient-descent梯度下降-Steepest-descent/" target="_blank" rel="noopener">梯度下降</a>和<a href="http://littlehaes.com/2018/03/26/Newton-method-and-quasi-Newton-method牛顿法与拟牛顿法/" target="_blank" rel="noopener">拟牛顿法</a>  </p>
<h3 id="多项logistic回归"><a href="#多项logistic回归" class="headerlink" title="多项logistic回归"></a>多项logistic回归</h3><p>之前介绍的是二项logistic回归,<strong>用于二分类</strong>.可以将其推广为multi-nomial logistic regression model.用于多类分类.<br>假设离散型随机变量Y的取值集合是{1,2,…,K},那么多项logistic回归模型是:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-fa3fb6c60d131815.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="10.png"><br>二项logistic回归的参数估计方法也可以推广到多项logistic回归中  </p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>曲线在中心附近增长速度较快,在两端增长速度较慢.(<strong>这个特性将使得使用梯度下降优化模型时,可以对误分样本快速调整</strong>)  </p>
<ol>
<li>形式为<strong>参数化</strong>的logistic分布,即μ和s变成了w和b  </li>
<li>P(Y=1|x)=F(X≤x),P(Y=0|x)=1-F(X≤x)=F(X&gt;x)</li>
<li>logistic回归的对数几率是线性的:wx+b,所以logistic回归属于对数线性模型</li>
<li>对数似然中对于<strong>单个样本</strong>的表达形式要有清晰的认识:P(yi|xi)=π(xi)^yi · [1-π(xi)]^(1-yi),决定这个式子形式的是yi</li>
</ol>
<p>参考:<br>李航,统计学习方法  </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/31/Linear%20Regression%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/31/Linear%20Regression%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" class="post-title-link" itemprop="url">Linear Regression线性回归</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-31 11:28:53" itemprop="dateCreated datePublished" datetime="2018-03-31T11:28:53+08:00">2018-03-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><p>线性回归(Linear Regression)是一种线性模型(linear model),<strong>它将各个特征进行线性组合,实现对新输入的预测</strong><br><strong>线性回归可解释性很强,因为特征对应的权值大小直接衡量了这个特征的重要性</strong>  </p>
<h2 id="表示形式"><a href="#表示形式" class="headerlink" title="表示形式"></a>表示形式</h2><p><strong>设每个输入x_i都有m个特征,每个特征x_ij对应一个权值w_j</strong><br>对于一个输入<br><img src="https://upload-images.jianshu.io/upload_images/9608551-d2f7b7832202bb5e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"><br>现有训练集T={(x1,y1),(x2,y2),…,(xN,yN)},(<strong>xi和yi的取值范围视具体情况决定</strong>),则线性回归的形式为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-f9b0acfa7cdee7fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png">  </p>
<h2 id="属性值离散"><a href="#属性值离散" class="headerlink" title="属性值离散"></a>属性值离散</h2><p>像高度,质量,速度这种属性值连续的变量,直接把其数值赋给对应的x即可<br>对于属性值是离散的情况</p>
<ul>
<li>如果属性的各个取值有某种顺序,也就是存在序(order)关系,那么可以通过连续化将其转化为连续值,例如高度不取连续值而是用低,中,高这三个离散值表示时,可以按低=1,中=2,高=3处理</li>
<li>如果属性的各个取值见不存在序关系,则将其转化为向量形式,比如one-hot形式,以花的颜色为例,取值为红,黄,蓝,可以将取值编码为红=(1,0,0),黄=(0,1,0),蓝=(0,0,1)<h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2>均方误差有很好的几何含义,它表示的是欧式距离(Euclidean distance),基于均方误差最小化来进行模型求解的方差称为”最小二乘法”(least square method).<strong>在线性回归中,最小二乘法就是试图找到一条直线,使所有样本到直线上的欧氏距离之和最小</strong><br><img src="https://upload-images.jianshu.io/upload_images/9608551-e2cc6d6119f0c7bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="3.png"><br>L(w,b)分别对w,b求偏导,并令偏导为0可得到w,b的解析解,关于求导细节,可以看<a href="http://littlehaes.com/2018/04/02/Matrix-Derivative矩阵求导/" target="_blank" rel="noopener">矩阵求导</a><br><img src="https://upload-images.jianshu.io/upload_images/9608551-254e18db46d0e39b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4.png"><br>参考:<br>周志华,机器学习</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/30/Maximum-Likelihood-Estimation%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/30/Maximum-Likelihood-Estimation%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/" class="post-title-link" itemprop="url">Maximum Likelihood Estimation极大似然估计</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-30 08:42:20" itemprop="dateCreated datePublished" datetime="2018-03-30T08:42:20+08:00">2018-03-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h3><p>现通过<strong>分类问题</strong>解释贝叶斯公式:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-fde2d89caa672242.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"><br>P(c)是类别c的先验(prior)概率<br>P(x|c)是似然概率(likelihood probability),或者说是样本x相对于类别c的类条件概率(class-conditional probability)<br>P(x)是用于归一化的证据(evidence)因子,通过全概率公式将P(x)展开,即上式第二个等号右边分母,<br>固定x观察c,可以看出,对于任意一个P(c|x),分母都是一样的,<strong>所以通过似然概率和先验概率便能估计P(c|x)</strong></p>
<h3 id="伯努利大数定理"><a href="#伯努利大数定理" class="headerlink" title="伯努利大数定理"></a>伯努利大数定理</h3><p>下面的说明会用到这个定理<br><img src="https://upload-images.jianshu.io/upload_images/9608551-5407453bf6b1063b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"></p>
<h3 id="先验与似然的进一步说明"><a href="#先验与似然的进一步说明" class="headerlink" title="先验与似然的进一步说明"></a>先验与似然的进一步说明</h3><ol>
<li>类别c的先验概率P(c)表达了<strong>样本空间中各类样本所占的比例</strong>,根据伯努利大数定理,当训练集包含充足的独立同分布样本时,P(c)可通过各类样本出现的频率来近似  </li>
<li>对于类条件概率P(x|c),x可以是个n维列向量,也就是说样本x中有很多特征(或说属性),所以P(x|c)=P(x1,x2,…,xn|c)涉及关于x的所有特征(属性)的联合概率,假设第i个特征xi有wi种取值,那么对于任意一个类别c而言,共有∏wi种可能,在现实应用中∏wi往往远远大于样本总数,这就是说,很多可能的情况并没有被采样到,其出现频率为0,会被估计成0,显然是不合理的,因为”被采样(观测)”到和”出现概率为0”通常是不同的.  </li>
<li>在朴素贝叶斯法(NaiveBayes)那篇文章中也提过类似的问题,联合概率导致参数数量太过庞大,这就是为什么朴素贝叶斯法要假设条件独立,假设条件独立后条件联合概率就可写成各个条件概率的乘积.<br>针对某个样本条件,对其属于各个类别的概率值进行估计,这样就大大简化了估计过程.当然也难免会有某个值频率为零的情况,这种情况下可使用拉普拉斯平滑(Laplace smoothing)解决.具体见之前文章<a href="http://littlehaes.com/2018/03/28/Naive-Bayes朴素贝叶斯法/" target="_blank" rel="noopener">Naive Bayes朴素贝叶斯法</a>中的”为什么需要假设条件独立”及”贝叶斯估计”两小节  </li>
<li><strong>朴素贝叶斯法</strong>通过<strong>假设条件独立</strong>减少条件联合概率参数(取值个数)过多的问题,<strong>极大似然估计</strong>采取其他办法,常用的一种策略是<strong>先假定样本x服从某种确定的概率分布形式,再基于训练样本对概率分布的参数进行估计</strong>,这种方法就不再是用频率估计概率了,而是直接优化之前假设的概率分布的参数<h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3>极大似然估计(Maximum Likelihood Estimation)是频率派提出的.<br><img src="https://upload-images.jianshu.io/upload_images/9608551-cafb4f6904bfef7d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-7d569fdb4801c6b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4.png"><h4 id="下溢-underflow"><a href="#下溢-underflow" class="headerlink" title="下溢(underflow)"></a>下溢(underflow)</h4>似然概率容易出现下溢的情况,什么是下溢?<br>连续数学在数字计算机上的根本困难是，我们需要通过有限数量的位模式来表示无限多的实数.这意味着我们在计算机中表示实数时，几乎总会引入一些近似误差.在许多情况下，这仅仅是舍入误差.舍入误差会导致一些问题，特别是当许多<br>操作复合时，即使是理论上可行的算法，如果在设计时没有考虑最小化舍入误差的累积，在实践时也可能会导致算法失效.  </li>
</ol>
<p><strong>一种极具毁灭性的舍入误差是下溢（underflow）.当接近零的数被四舍五入为零时发生下溢.许多函数在其参数为零而不是一个很小的正数时才会表现出质的不同.例如，我们通常要避免被零除（一些软件环境将在这种情况下抛出异常，有些会返回一个非数字(not-a-number, NaN)的占位符）或避免取零的对数（这通常被视为-∞，进一步的算术运算会使其变成非数字)</strong><br>另一个极具破坏力的数值错误形式是上溢（overflow）.当大量级的数被近似为+∞或-∞时发生上溢.进一步的运算通常会导致这些无限值变为非数字.<br>必须对上溢和下溢进行数值稳定的一个例子是softmax函数  </p>
<h4 id="对数似然解决下溢"><a href="#对数似然解决下溢" class="headerlink" title="对数似然解决下溢"></a>对数似然解决下溢</h4><p>因为似然概率中有连乘的形式,每个因式都小于1,过多的连乘会导致数值大小,进而被当成0发生下溢.<br>解决办法是利用对数似然,由连乘变为相加<br><img src="https://upload-images.jianshu.io/upload_images/9608551-b24784895b4514de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-81e7fa2cfbd0e1a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="6.png"></p>
<h3 id="以高斯分布为例"><a href="#以高斯分布为例" class="headerlink" title="以高斯分布为例"></a>以高斯分布为例</h3><p><img src="https://upload-images.jianshu.io/upload_images/9608551-4c569a02b1f621ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.png"><br>可以发现,<strong>通过极大似然法得到的正态分布的均值就是无偏估计的样本均值,方差是有偏估计的样本方差</strong><br>关于无偏估计,可参考之前的文章:<a href="http://littlehaes.com/2018/02/17/Unbiased-Estimation-无偏估计/" target="_blank" rel="noopener">Unbiased Estimation 无偏估计</a></p>
<h3 id="假设分布带来的问题"><a href="#假设分布带来的问题" class="headerlink" title="假设分布带来的问题"></a>假设分布带来的问题</h3><p>需要注意的是,估计类条件概率时,虽然假设某种确定的概率分布形式能使类条件概率的估计变得简单,<strong>但是估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布.</strong><br>在现实应用中,欲做出能较好地接近潜在真实分布的假设,往往需在一定程度上利用关于应用任务本身的经验知识,否则仅凭猜测来假设概率分布形式,很可能产生误导性的结果  </p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol>
<li>联合概率导致参数指数级增长,假设独立后参数空间减小很多</li>
<li>朴素贝叶斯假设条件独立;极大似然法假设x的分布,模型的好坏很大程度上取决于假设的分布的好坏<br>参考:<br>周志华,机器学习<br>Ian Goodfellow,深度学习</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/28/Naive-Bayes%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/28/Naive-Bayes%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/" class="post-title-link" itemprop="url">Naive Bayes朴素贝叶斯法</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-28 20:33:43" itemprop="dateCreated datePublished" datetime="2018-03-28T20:33:43+08:00">2018-03-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>朴素贝叶斯方法基于贝叶斯公式,之所以朴素(Naive)是因为它有一个较强的假设,让自己包含的条件概率数量大大减少,有助于模型的训练与预测,这个假设是:<strong>条件独立</strong><br><img src="https://upload-images.jianshu.io/upload_images/9608551-68e63e4921263bef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"><br>注意:朴素贝叶斯估计和贝叶斯估计是不同的概念,下文会提到贝叶斯估计</p>
<h2 id="Naive-Bayes法的训练与分类"><a href="#Naive-Bayes法的训练与分类" class="headerlink" title="Naive Bayes法的训练与分类"></a>Naive Bayes法的训练与分类</h2><h3 id="朴素贝叶斯法的基本思路"><a href="#朴素贝叶斯法的基本思路" class="headerlink" title="朴素贝叶斯法的基本思路:"></a>朴素贝叶斯法的基本思路:</h3><ul>
<li>对于给定的训练集,基于特征之间条件独立的假设去学习条件概率分布P(X=x|Y=c_k),先验概率分布P(Y=c_k)  </li>
<li>对于测试集(或新数据)中的输入x,基于训练得到的模型(上述概率分布),利用贝叶斯定理求出使后验概率最大的输出y<br>具体点就是:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-9ab103ea27f837d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"><br>后验概率计算公式:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-007962a2f7922f4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4.png"><br>朴素贝叶斯分类器:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-b053cadce80a56e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.png"><h3 id="为什么需要假设条件独立"><a href="#为什么需要假设条件独立" class="headerlink" title="为什么需要假设条件独立"></a>为什么需要假设条件独立</h3><img src="https://upload-images.jianshu.io/upload_images/9608551-7878ef8a8a0fcef4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.png">  <h3 id="后验概率最大化的含义"><a href="#后验概率最大化的含义" class="headerlink" title="后验概率最大化的含义"></a>后验概率最大化的含义</h3>朴素贝叶斯分类中,后验概率最大化等价于期望风险最小化<br><img src="https://upload-images.jianshu.io/upload_images/9608551-19508026d19ae65f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="6.png"><br>进一步有:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-b7498c76a2da45c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.png"><h2 id="朴素贝叶斯法的参数估计"><a href="#朴素贝叶斯法的参数估计" class="headerlink" title="朴素贝叶斯法的参数估计"></a>朴素贝叶斯法的参数估计</h2><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3></li>
</ul>
<p><strong>极大似然估计属于频率派的想法</strong>  </p>
<h4 id="先验概率估计"><a href="#先验概率估计" class="headerlink" title="先验概率估计"></a>先验概率估计</h4><p><img src="https://upload-images.jianshu.io/upload_images/9608551-fc5cd61bddf82489.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="8.png"></p>
<h4 id="条件概率估计"><a href="#条件概率估计" class="headerlink" title="条件概率估计"></a>条件概率估计</h4><p><img src="https://upload-images.jianshu.io/upload_images/9608551-a82a89c7e69f79d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="9.png"></p>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>Naive Bayes algorithm<br><img src="https://upload-images.jianshu.io/upload_images/9608551-989aa55c072543f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="10.png"></p>
<h3 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h3><p>用极大似然估计可能会出现所要估计的概率值为0的情况,这时会影响到后验概率的计算结果,使分类产生偏差.<br><strong>可以使用贝叶斯估计解决这一问题</strong><br><img src="https://upload-images.jianshu.io/upload_images/9608551-26355dd4bbdc519a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="11.png"><br>参数及可行性说明<br><img src="https://upload-images.jianshu.io/upload_images/9608551-e5b3b2b42b827526.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="12.png">  </p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>假设条件独立就是:假设数据中的各个特征在类确定的条件下都是独立的</li>
</ol>
<p>参考:<br>李航,统计学习方法</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/26/Newton-method-and-quasi-Newton-method%E7%89%9B%E9%A1%BF%E6%B3%95%E4%B8%8E%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/26/Newton-method-and-quasi-Newton-method%E7%89%9B%E9%A1%BF%E6%B3%95%E4%B8%8E%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/" class="post-title-link" itemprop="url">Newton's method and Quasi Newton method牛顿法与拟牛顿法</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-26 18:47:52" itemprop="dateCreated datePublished" datetime="2018-03-26T18:47:52+08:00">2018-03-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>牛顿法和拟牛顿法是求解无约束最优化问题的常用方法,优点是收敛速度快.<br>牛顿法是迭代算法,每一步需要求解目标函数的Hessian矩阵的逆矩阵,矩阵的逆运算很耗时.<br>拟牛顿法通过正定矩阵近似Hessian矩阵的逆矩阵或Hessian矩阵,简化Hessian矩阵的求逆计算过程  </p>
<h1 id="采用线搜索框架"><a href="#采用线搜索框架" class="headerlink" title="采用线搜索框架"></a>采用线搜索框架</h1><p><img src="https://upload-images.jianshu.io/upload_images/9608551-a781bb3cbc7338a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"><br><strong>搜索方向由牛顿法或拟牛顿法给出,步长可以通过精确线搜索或非精确线搜索获得</strong><br>关于步长,之前的文章有提过:<a href="http://littlehaes.com/2018/03/23/Line-search-and-Step-length线搜索与步长/" target="_blank" rel="noopener">Line search and Step length线搜索与步长</a>   </p>
<h1 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h1><ol>
<li>假设f(x)具有二阶连续偏导数.要求解的无约束最优化问题是min f(x),x*标识目标函数f(x)的极小点.  </li>
<li>若第k次迭代值为x^(k),则可将f(x)在x^(k)附近进行二阶泰勒展开(Taylor expansion):<br><img src="https://upload-images.jianshu.io/upload_images/9608551-432203ebaca87062.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png">  </li>
<li><ul>
<li>函数f(x)有极值的必要条件是在极值点处一阶导数为0,即梯度向量为0.特别地,当H(x^(k))是正定矩阵时,函数f(x)的极值为极小值.  </li>
<li>牛顿法利用极小点的必要条件▽f(x)=0  </li>
<li>每次迭代中从点x^(k)开始,求目标函数的极小点,作为第k+1次迭代值x^(k+1).具体地,假设x^(k+1)满足▽f(x^(k+1))=0  </li>
<li>由二阶泰勒展开,对f(x)关于(x-x^(k))求梯度得:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-be3e5d36a25794e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.png"></li>
<li>将x^(k+1)带入上面的梯度公式:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-07ab256d46998598.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4.png">  <h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><img src="https://upload-images.jianshu.io/upload_images/9608551-9ba5dc26749ed66a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.png">  </li>
</ul>
</li>
</ol>
<h1 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h1><p>牛顿法算法流程的步骤(4)涉及Hessian矩阵的求逆,计算复杂,所以有其它改进的方法,比如<br><img src="https://upload-images.jianshu.io/upload_images/9608551-1855c02c9d6eefe0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="6.png"><br>先看牛顿法迭代中Hessian矩阵H_k满足的条件,进而引出<strong>拟牛顿条件</strong>.<br><img src="https://upload-images.jianshu.io/upload_images/9608551-b4350d3d190a73e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.png"><br>下面说明如果H_k正定,则牛顿法的搜索方向是下降方向<br><img src="https://upload-images.jianshu.io/upload_images/9608551-097d808f13a94209.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="8.png"><br><strong>拟牛顿法是用一个n阶矩阵G_k去近似Hessian矩阵的逆,所以矩阵G_k需要满足Hessian矩阵满足的条件</strong><br><img src="https://upload-images.jianshu.io/upload_images/9608551-b7ea73fd9be75883.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="9.png"><br>每次迭代时需要更新G_k矩阵,更新方法有多种类选择,主要介绍下Broyden类拟牛顿法</p>
<h2 id="DFP算法"><a href="#DFP算法" class="headerlink" title="DFP算法"></a>DFP算法</h2><p>DFP(Davidon-Flectcher-Powell)算法选择G_(k+1)的方法是:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-3029a331080edab8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="10.png"><br>进一步,关于P_k,Q_k<br><img src="https://upload-images.jianshu.io/upload_images/9608551-942e9c7ab3e32cea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="11.png"><br><strong>P_k,Q_k如上取值的可行性证明:</strong><br> <img src="https://upload-images.jianshu.io/upload_images/9608551-326aa8e92b3868e4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="12.jpg">  </p>
<p> <img src="https://upload-images.jianshu.io/upload_images/9608551-a3b1fe8a41c08ca7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="14.png">  </p>
<h3 id="DFP算法流程"><a href="#DFP算法流程" class="headerlink" title="DFP算法流程"></a>DFP算法流程</h3><p><img src="https://upload-images.jianshu.io/upload_images/9608551-0ca486c999562c7e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="13.png"></p>
<h2 id="BFGS算法"><a href="#BFGS算法" class="headerlink" title="BFGS算法"></a>BFGS算法</h2><p>BFGS(Broyden-Flectcher-Goldfarb-Shanno)算法是最流行的拟牛顿算法.<br><img src="https://upload-images.jianshu.io/upload_images/9608551-f4421f870c12d0b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="15.png"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/9608551-a9304c77cea1e40d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="16.png"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/9608551-d5bf1ddead15c875.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="17.png"></p>
<h3 id="BFGS算法流程"><a href="#BFGS算法流程" class="headerlink" title="BFGS算法流程"></a>BFGS算法流程</h3><p><img src="https://upload-images.jianshu.io/upload_images/9608551-2182ed3eb8720b52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="18.png">   </p>
<h2 id="DFP和BFGS的迭代公式很像"><a href="#DFP和BFGS的迭代公式很像" class="headerlink" title="DFP和BFGS的迭代公式很像"></a>DFP和BFGS的迭代公式很像</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-96f2fcb2790e1606.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="19.png">  </p>
<h2 id="Broyden类算法"><a href="#Broyden类算法" class="headerlink" title="Broyden类算法"></a>Broyden类算法</h2><h3 id="Sherman-Morrison公式"><a href="#Sherman-Morrison公式" class="headerlink" title="Sherman-Morrison公式"></a>Sherman-Morrison公式</h3><p><img src="https://upload-images.jianshu.io/upload_images/9608551-7d4dc4c7382fc640.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="20.png">  </p>
<h3 id="Broyden类"><a href="#Broyden类" class="headerlink" title="Broyden类"></a>Broyden类</h3><p><img src="https://upload-images.jianshu.io/upload_images/9608551-ee1aabdbf727d0ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="21.png"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/9608551-4e770879233e8518.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="22.png">  </p>
<p>参考:<br>李航,统计学习方法</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/25/Gradient-descent%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-Steepest-descent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/25/Gradient-descent%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-Steepest-descent/" class="post-title-link" itemprop="url">Gradient descent梯度下降(Steepest descent)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-25 17:23:33" itemprop="dateCreated datePublished" datetime="2018-03-25T17:23:33+08:00">2018-03-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>梯度下降(gradient descent)也叫最速下降(steepest descent),用来求解无约束最优化问题的一种常用方法,结果是局部最优解,对于目标函数为凸的情况,可以得到全局最优解.梯度下降是迭代算法,每一步需要求解目标函数的梯度向量.  </p>
<h2 id="采用线搜索的框架"><a href="#采用线搜索的框架" class="headerlink" title="采用线搜索的框架"></a>采用线搜索的框架</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-a781bb3cbc7338a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1.png"><br><strong>搜索方向取负梯度方向,步长可以通过精确线搜索或非精确线搜索获得</strong><br>关于步长,之前的文章有提过:<a href="http://littlehaes.com/2018/03/23/Line-search-and-Step-length线搜索与步长/" target="_blank" rel="noopener">Line search and Step length线搜索与步长</a>  </p>
<h2 id="泰勒展开简化形式"><a href="#泰勒展开简化形式" class="headerlink" title="泰勒展开简化形式"></a>泰勒展开简化形式</h2><ol>
<li>假设f(x)是R^n上具有一阶连续偏导数的函数.要求解的无约束最优化问题是min f(x),x*标识目标函数f(x)的极小点.  </li>
<li>选取适当的初值x^(0),不断迭代,更新x的值,进行目标函数的极小化,直到收敛.由于负梯度方向是使函数值下降最快的方向,在迭代的每一步,以负梯度方向更新x的值,从而达到减小函数值的目的.  </li>
<li>因为f(x)具有一阶连续偏导数, 若第k次迭代值为x^(k),则可将f(x)在x^(k)附近进行一阶泰勒展开(Taylor expansion):<br><img src="https://upload-images.jianshu.io/upload_images/9608551-6a8d6ce238132a35.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"></li>
</ol>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-eae7b5e4cba6c71f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.png"><br>简化版:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-935bdaced03e4364.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4.png"></p>
<h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><h3 id="收敛慢"><a href="#收敛慢" class="headerlink" title="收敛慢"></a>收敛慢</h3><h4 id="碗形函数-bowl-shape"><a href="#碗形函数-bowl-shape" class="headerlink" title="碗形函数(bowl shape)"></a>碗形函数(bowl shape)</h4><p>蓝色的线是函数的等高线(线上的函数值相等)<br>从x_0点开始,沿x_0的负梯度方向(与该点切线垂直)的前进适当的步长,函数值会减小<br>对于该图来说,一次一次迭代可以收敛全局最优点<br><img src="https://upload-images.jianshu.io/upload_images/9608551-cce1a345241054d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.png">  </p>
<h4 id="之字形Zig-Zagging"><a href="#之字形Zig-Zagging" class="headerlink" title="之字形Zig-Zagging"></a>之字形Zig-Zagging</h4><p>实际中的等高线可能并没有这么好<br>下图这样的等高线会导致每次迭代走的是之字形(Zig-Zagging),这样会使得收敛速度很慢<br><img src="https://upload-images.jianshu.io/upload_images/9608551-670bc6a5de3e00a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="6.png">  </p>
<h4 id="Rosenbrock-函数"><a href="#Rosenbrock-函数" class="headerlink" title="Rosenbrock 函数"></a>Rosenbrock 函数</h4><p>对于像Rosenbrock这样的病态函数(pathological functions)来说,等高线如下图<br>不仅有走之字形(Zig-Zagging)的情况,而且函数图像的底部很平坦,这样每次前进的步长很小,导致收敛速度太慢<br>The bottom of the valley is very flat. Because of the curved flat valley the optimization is zig-zagging slowly with small stepsizes towards the minimum.<br><img src="https://upload-images.jianshu.io/upload_images/9608551-49563f08eb67baee.gif?imageMogr2/auto-orient/strip" alt="7.gif"><br>梯度下降的收敛速度比起很多其他方法都慢,如果函数不凸,梯度下降过程中会走更多的之字形,因为总有当前点的梯度方向与当前点到最小点的方向是垂直的情况,也就是说要走很多冤枉路</p>
<h3 id="不可微的函数"><a href="#不可微的函数" class="headerlink" title="不可微的函数"></a>不可微的函数</h3><p>对于不可微的函数,就不能直接用梯度下降了,需要进行额外的平滑处理  </p>
<p>参考:<br>李航,统计学习方法</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E6%98%AF%E5%87%BD%E6%95%B0%E5%80%BC%E5%A2%9E%E5%A4%A7%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E5%90%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E6%98%AF%E5%87%BD%E6%95%B0%E5%80%BC%E5%A2%9E%E5%A4%A7%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E5%90%91/" class="post-title-link" itemprop="url">为什么梯度方向是函数值增大最快的方向</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-24 20:12:09" itemprop="dateCreated datePublished" datetime="2018-03-24T20:12:09+08:00">2018-03-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>梯度下降中,梯度反方向是函数值下降最快的方向,说明梯度方向是函数值上升最快的方向.<br>下面给出说明,基础好的可以直接看最后一部分:沿梯度方向函数值增大最快<br>##无穷小量<br><img src="https://upload-images.jianshu.io/upload_images/9608551-549cb101113847ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.png"></p>
<h2 id="定理"><a href="#定理" class="headerlink" title="定理"></a>定理</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-cb06d0610ff4a760.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="6.png"></p>
<h2 id="高阶无穷小"><a href="#高阶无穷小" class="headerlink" title="高阶无穷小"></a>高阶无穷小</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-6b52793e808c7fab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.png"></p>
<h2 id="引出微分"><a href="#引出微分" class="headerlink" title="引出微分"></a>引出微分</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-c6fd3b2133d55914.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="8.png"></p>
<h2 id="微分"><a href="#微分" class="headerlink" title="微分"></a>微分</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-b47cb419f1527bdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="9.png"></p>
<h2 id="全微分"><a href="#全微分" class="headerlink" title="全微分"></a>全微分</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-e1a3720dc2aeb8a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="10.png"></p>
<h2 id="方向导数"><a href="#方向导数" class="headerlink" title="方向导数"></a>方向导数</h2><p>定义:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-0a977e0c11c4c467.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"><br><strong>方向导数实际上是函数f在x_0处沿l方向关于距离t的变化率</strong><br>方向导数的几何意义,f(x,y)在x_0处有唯一的切线,该点关于l方向的斜率就是方向导数.<br>在方向导数中,一种特别重要的情形是<strong>沿着坐标轴正向的方向导数,这就是偏导数</strong>  </p>
<h2 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h2><p>定义:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-3a18d9f39adc7c59.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.png">  </p>
<h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><p>定义:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-e31043010ddcba47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4.png"><br><strong>梯度就是个向量</strong></p>
<h2 id="可微的必要条件"><a href="#可微的必要条件" class="headerlink" title="可微的必要条件"></a>可微的必要条件</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-1404e9c0c7df547b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="11.png"></p>
<h2 id="沿梯度方向函数值增大最快"><a href="#沿梯度方向函数值增大最快" class="headerlink" title="沿梯度方向函数值增大最快"></a>沿梯度方向函数值增大最快</h2><p><strong>在x0处沿某一方向的方向导数反映了:在x0的邻域内沿着这个方向,函数值能增大多少或者能减小多少</strong><br>将上述f在(x0,y0)处沿任意方向l的方向导数结果写成向量的内积形式<br><img src="https://upload-images.jianshu.io/upload_images/9608551-93891c451708819c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="12.png"><br><strong>所以在x0处沿着梯度方向能使f在该点处的方向导数最大,也就是在x0的邻域内沿着这个方向,函数值增长最快;同理沿着负梯度方向,函数值减小最快</strong></p>
<p>参考:<br>王绵森,工科数学分析基础上下册</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/23/Line-search-and-Step-length%E7%BA%BF%E6%90%9C%E7%B4%A2%E4%B8%8E%E6%AD%A5%E9%95%BF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/23/Line-search-and-Step-length%E7%BA%BF%E6%90%9C%E7%B4%A2%E4%B8%8E%E6%AD%A5%E9%95%BF/" class="post-title-link" itemprop="url">Line search and Step length线搜索与步长</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-23 20:58:05" itemprop="dateCreated datePublished" datetime="2018-03-23T20:58:05+08:00">2018-03-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在最优化(optimization)问题中,<strong>线搜索(line search)</strong>和置信域(trust region)方法是寻找局部最小值(local minimum)基本迭代方法(iterative approach),主要说说线搜索方法(置信域方法过于专业)</p>
<h2 id="线搜索-Line-search"><a href="#线搜索-Line-search" class="headerlink" title="线搜索(Line search)"></a>线搜索(Line search)</h2><p>以f(x)为例,线搜索会先找一个使f(x)<strong>下降的方向</strong>,接着计算一个<strong>步长</strong>,步长决定了x改变的大小.<br><strong>下降方向</strong>:可以通过梯度下降,牛顿法,拟牛顿法等计算<br><strong>步长</strong>:有精确(exact)和非精确(inexact)两种,精确方法就是找出导数为零的极值点,例如共轭梯度法conjugate gradient method;非精确方法没有找出导数为零的点,而是使f(x)有一个充分的下降(sufficient descent),例如backtracking,wolfe conditions,goldstein conditions<br>线搜索流程:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-cea67e752863350d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"></p>
<h2 id="非精确线搜索步长"><a href="#非精确线搜索步长" class="headerlink" title="非精确线搜索步长"></a>非精确线搜索步长</h2><p>精确线搜索的步长计算往往非常耗时,所以一般采用非精确线搜索 </p>
<h3 id="Wolfe-conditions"><a href="#Wolfe-conditions" class="headerlink" title="Wolfe conditions"></a>Wolfe conditions</h3><p>Wolfe conditions 由 Armijo conditions和Curvature conditions构成,先分别介绍Armijo conditions和Curvature conditions  </p>
<h4 id="Armijo-conditions"><a href="#Armijo-conditions" class="headerlink" title="Armijo conditions"></a>Armijo conditions</h4><p>Armijo条件:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-a863991ce0edfb1d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.png"><br>不等式两侧都可看成α的线性函数,所以不等式要求相当于,左侧的直线在右侧直线下方.实际应用中将c1取得很小,以使得不等式右侧的直线不是太倾斜(太倾斜会使得下降太多,可能取不到最优点)  </p>
<h4 id="Curvature-condition"><a href="#Curvature-condition" class="headerlink" title="Curvature condition"></a>Curvature condition</h4><p>在满足Armijo条件的情况下我们希望步长尽量大一些,这样收敛快,所以引入curvature条件:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-72c2aa7bf89cf920.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4.png"><br>将等式两侧都写成α的函数形式,则有:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-cc1d57a1bd1076af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.png"><br>这要求:θ(α)’大于等于θ(0)’的c2倍<br>经验取值:<br>在拟牛顿法中,c2=0.9<br>在非线性共轭梯度方法中,c2=0.1  </p>
<h3 id="Wolfe-conditions-1"><a href="#Wolfe-conditions-1" class="headerlink" title="Wolfe conditions"></a>Wolfe conditions</h3><p>Wolfe conditions:将Armijo conditions和Curvature conditions结合在一起<br><img src="https://upload-images.jianshu.io/upload_images/9608551-3723571406fbb5e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.png"></p>
<p>直观点,如下图:(忽略丑陋的字…)<br><img src="https://upload-images.jianshu.io/upload_images/9608551-b07930a140c03363.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="6.png"><br>由Armijo:函数值位于橘黄色的直线下面<br>由Curvature:斜率要大于等于θ(0)’的c2倍<br>所以最后满足条件的区域为(α1,α2)和(α3,α4)<br>可以看到,最小值在(α3,α4)中  </p>
<h3 id="Goldstein-conditions"><a href="#Goldstein-conditions" class="headerlink" title="Goldstein conditions"></a>Goldstein conditions</h3><p>一般用于牛顿法,但不适合拟牛顿法,因为拟合的Hessian矩阵不能保持正定<br>第二个不等号和sufficient descent的形式完全一样<br><img src="https://upload-images.jianshu.io/upload_images/9608551-93991e76f4f70ce6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="8.png"><br>类似Wolfe conditions,写成θ(α)的形式<br><img src="https://upload-images.jianshu.io/upload_images/9608551-db9073baddf8b231.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="9.png"><br>Goldstein conditions就是要求θ(α)要在两条直线之间,但可能避开最优解<br>像下面这样:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-1b6dfabd24ab405f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="10.png"><br>满足条件的区域为(α1,α2)和(α3,α4)<br>最小值并不在这里面  </p>
<h3 id="Backtracking-method"><a href="#Backtracking-method" class="headerlink" title="Backtracking method"></a>Backtracking method</h3><p>在实际应用中,为提高效率,放弃Wolfe conditions中的Curvature conditions,只使用sufficient descent条件,这就是Backtracking method  </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Little Haes"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Little Haes</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">76</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/smallhaes" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;smallhaes" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:littlehaes@bupt.edu.cn" title="E-Mail → mailto:littlehaes@bupt.edu.cn" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/smallhaes" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;smallhaes" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://music.163.com/#/user/home?id=75165464" title="网易云 → https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;75165464" rel="noopener" target="_blank"><i class="fa fa-fw fa-cloud"></i>网易云</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/littlehaes" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;littlehaes" rel="noopener" target="_blank">CSDN</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Little Haes</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.1.1
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.6.0
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
