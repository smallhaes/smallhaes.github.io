<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Haes&#39; Blog">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;page&#x2F;11&#x2F;index.html">
<meta property="og:site_name" content="Haes&#39; Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Little Haes">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/page/11/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>Haes' Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Haes' Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/smallhaes" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E6%98%AF%E5%87%BD%E6%95%B0%E5%80%BC%E5%A2%9E%E5%A4%A7%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E5%90%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E6%98%AF%E5%87%BD%E6%95%B0%E5%80%BC%E5%A2%9E%E5%A4%A7%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E5%90%91/" class="post-title-link" itemprop="url">为什么梯度方向是函数值增大最快的方向</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-24 20:12:09" itemprop="dateCreated datePublished" datetime="2018-03-24T20:12:09+08:00">2018-03-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2018/03/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E6%98%AF%E5%87%BD%E6%95%B0%E5%80%BC%E5%A2%9E%E5%A4%A7%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E5%90%91/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/24/为什么梯度方向是函数值增大最快的方向/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>梯度下降中,梯度反方向是函数值下降最快的方向,说明梯度方向是函数值上升最快的方向.<br>下面给出说明,基础好的可以直接看最后一部分:沿梯度方向函数值增大最快<br>##无穷小量<br><img src="https://upload-images.jianshu.io/upload_images/9608551-549cb101113847ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.png"></p>
<h2 id="定理"><a href="#定理" class="headerlink" title="定理"></a>定理</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-cb06d0610ff4a760.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="6.png"></p>
<h2 id="高阶无穷小"><a href="#高阶无穷小" class="headerlink" title="高阶无穷小"></a>高阶无穷小</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-6b52793e808c7fab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.png"></p>
<h2 id="引出微分"><a href="#引出微分" class="headerlink" title="引出微分"></a>引出微分</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-c6fd3b2133d55914.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="8.png"></p>
<h2 id="微分"><a href="#微分" class="headerlink" title="微分"></a>微分</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-b47cb419f1527bdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="9.png"></p>
<h2 id="全微分"><a href="#全微分" class="headerlink" title="全微分"></a>全微分</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-e1a3720dc2aeb8a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="10.png"></p>
<h2 id="方向导数"><a href="#方向导数" class="headerlink" title="方向导数"></a>方向导数</h2><p>定义:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-0a977e0c11c4c467.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"><br><strong>方向导数实际上是函数f在x_0处沿l方向关于距离t的变化率</strong><br>方向导数的几何意义,f(x,y)在x_0处有唯一的切线,该点关于l方向的斜率就是方向导数.<br>在方向导数中,一种特别重要的情形是<strong>沿着坐标轴正向的方向导数,这就是偏导数</strong>  </p>
<h2 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h2><p>定义:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-3a18d9f39adc7c59.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.png">  </p>
<h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><p>定义:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-e31043010ddcba47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4.png"><br><strong>梯度就是个向量</strong></p>
<h2 id="可微的必要条件"><a href="#可微的必要条件" class="headerlink" title="可微的必要条件"></a>可微的必要条件</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-1404e9c0c7df547b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="11.png"></p>
<h2 id="沿梯度方向函数值增大最快"><a href="#沿梯度方向函数值增大最快" class="headerlink" title="沿梯度方向函数值增大最快"></a>沿梯度方向函数值增大最快</h2><p><strong>在x0处沿某一方向的方向导数反映了:在x0的邻域内沿着这个方向,函数值能增大多少或者能减小多少</strong><br>将上述f在(x0,y0)处沿任意方向l的方向导数结果写成向量的内积形式<br><img src="https://upload-images.jianshu.io/upload_images/9608551-93891c451708819c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="12.png"><br><strong>所以在x0处沿着梯度方向能使f在该点处的方向导数最大,也就是在x0的邻域内沿着这个方向,函数值增长最快;同理沿着负梯度方向,函数值减小最快</strong></p>
<p>参考:<br>王绵森,工科数学分析基础上下册</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/23/Line-search-and-Step-length%E7%BA%BF%E6%90%9C%E7%B4%A2%E4%B8%8E%E6%AD%A5%E9%95%BF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/23/Line-search-and-Step-length%E7%BA%BF%E6%90%9C%E7%B4%A2%E4%B8%8E%E6%AD%A5%E9%95%BF/" class="post-title-link" itemprop="url">Line search and Step length线搜索与步长</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-23 20:58:05" itemprop="dateCreated datePublished" datetime="2018-03-23T20:58:05+08:00">2018-03-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2018/03/23/Line-search-and-Step-length%E7%BA%BF%E6%90%9C%E7%B4%A2%E4%B8%8E%E6%AD%A5%E9%95%BF/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/23/Line-search-and-Step-length线搜索与步长/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在最优化(optimization)问题中,<strong>线搜索(line search)</strong>和置信域(trust region)方法是寻找局部最小值(local minimum)基本迭代方法(iterative approach),主要说说线搜索方法(置信域方法过于专业)</p>
<h2 id="线搜索-Line-search"><a href="#线搜索-Line-search" class="headerlink" title="线搜索(Line search)"></a>线搜索(Line search)</h2><p>以f(x)为例,线搜索会先找一个使f(x)<strong>下降的方向</strong>,接着计算一个<strong>步长</strong>,步长决定了x改变的大小.<br><strong>下降方向</strong>:可以通过梯度下降,牛顿法,拟牛顿法等计算<br><strong>步长</strong>:有精确(exact)和非精确(inexact)两种,精确方法就是找出导数为零的极值点,例如共轭梯度法conjugate gradient method;非精确方法没有找出导数为零的点,而是使f(x)有一个充分的下降(sufficient descent),例如backtracking,wolfe conditions,goldstein conditions<br>线搜索流程:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-cea67e752863350d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2.png"></p>
<h2 id="非精确线搜索步长"><a href="#非精确线搜索步长" class="headerlink" title="非精确线搜索步长"></a>非精确线搜索步长</h2><p>精确线搜索的步长计算往往非常耗时,所以一般采用非精确线搜索 </p>
<h3 id="Wolfe-conditions"><a href="#Wolfe-conditions" class="headerlink" title="Wolfe conditions"></a>Wolfe conditions</h3><p>Wolfe conditions 由 Armijo conditions和Curvature conditions构成,先分别介绍Armijo conditions和Curvature conditions  </p>
<h4 id="Armijo-conditions"><a href="#Armijo-conditions" class="headerlink" title="Armijo conditions"></a>Armijo conditions</h4><p>Armijo条件:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-a863991ce0edfb1d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3.png"><br>不等式两侧都可看成α的线性函数,所以不等式要求相当于,左侧的直线在右侧直线下方.实际应用中将c1取得很小,以使得不等式右侧的直线不是太倾斜(太倾斜会使得下降太多,可能取不到最优点)  </p>
<h4 id="Curvature-condition"><a href="#Curvature-condition" class="headerlink" title="Curvature condition"></a>Curvature condition</h4><p>在满足Armijo条件的情况下我们希望步长尽量大一些,这样收敛快,所以引入curvature条件:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-72c2aa7bf89cf920.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4.png"><br>将等式两侧都写成α的函数形式,则有:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-cc1d57a1bd1076af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.png"><br>这要求:θ(α)’大于等于θ(0)’的c2倍<br>经验取值:<br>在拟牛顿法中,c2=0.9<br>在非线性共轭梯度方法中,c2=0.1  </p>
<h3 id="Wolfe-conditions-1"><a href="#Wolfe-conditions-1" class="headerlink" title="Wolfe conditions"></a>Wolfe conditions</h3><p>Wolfe conditions:将Armijo conditions和Curvature conditions结合在一起<br><img src="https://upload-images.jianshu.io/upload_images/9608551-3723571406fbb5e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.png"></p>
<p>直观点,如下图:(忽略丑陋的字…)<br><img src="https://upload-images.jianshu.io/upload_images/9608551-b07930a140c03363.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="6.png"><br>由Armijo:函数值位于橘黄色的直线下面<br>由Curvature:斜率要大于等于θ(0)’的c2倍<br>所以最后满足条件的区域为(α1,α2)和(α3,α4)<br>可以看到,最小值在(α3,α4)中  </p>
<h3 id="Goldstein-conditions"><a href="#Goldstein-conditions" class="headerlink" title="Goldstein conditions"></a>Goldstein conditions</h3><p>一般用于牛顿法,但不适合拟牛顿法,因为拟合的Hessian矩阵不能保持正定<br>第二个不等号和sufficient descent的形式完全一样<br><img src="https://upload-images.jianshu.io/upload_images/9608551-93991e76f4f70ce6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="8.png"><br>类似Wolfe conditions,写成θ(α)的形式<br><img src="https://upload-images.jianshu.io/upload_images/9608551-db9073baddf8b231.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="9.png"><br>Goldstein conditions就是要求θ(α)要在两条直线之间,但可能避开最优解<br>像下面这样:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-1b6dfabd24ab405f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="10.png"><br>满足条件的区域为(α1,α2)和(α3,α4)<br>最小值并不在这里面  </p>
<h3 id="Backtracking-method"><a href="#Backtracking-method" class="headerlink" title="Backtracking method"></a>Backtracking method</h3><p>在实际应用中,为提高效率,放弃Wolfe conditions中的Curvature conditions,只使用sufficient descent条件,这就是Backtracking method  </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/21/Markov-Chain-Monte-Carlo-%E5%92%8C-Gibbs-Sampling%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/21/Markov-Chain-Monte-Carlo-%E5%92%8C-Gibbs-Sampling%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">Markov Chain Monte Carlo 和 Gibbs Sampling算法</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-21 22:19:08" itemprop="dateCreated datePublished" datetime="2018-03-21T22:19:08+08:00">2018-03-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2018/03/21/Markov-Chain-Monte-Carlo-%E5%92%8C-Gibbs-Sampling%E7%AE%97%E6%B3%95/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/21/Markov-Chain-Monte-Carlo-和-Gibbs-Sampling算法/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="蒙特卡洛模拟"><a href="#蒙特卡洛模拟" class="headerlink" title="蒙特卡洛模拟"></a>蒙特卡洛模拟</h2><p>蒙特卡洛模拟(Monte Carlo Simulation)是随机模拟的别名,关于随机模拟的一个重要的问题就是:给定一个概率分布p(x),如何生成它的样本?<br>一般而言,均匀分布Uniform(0,1)的样本容易生成,而常见的概率分布(连续或离散)都可以基于均匀分布的样本生成,例如正态分布可以通过Box-Muller变换得到.<br>但是像p(x,y,z)这样甚至更高维度分布的样本很难生成,而MCMC(Markov Chain Monte Carlo)和Gibbs Sampling算法就是解决这个问题的.让我们从马尔科夫链(Markov Chain)说起</p>
<h2 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h2><p>马尔科夫链(Markov Chain),简称马氏链,</p>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义:"></a>定义:</h3><p><img src="https://upload-images.jianshu.io/upload_images/9608551-d800449f0a70e49c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="1.png"><br><strong>含义:当前所处状态只和前一个状态有直接联系</strong></p>
<h3 id="非周期马氏链的引出"><a href="#非周期马氏链的引出" class="headerlink" title="非周期马氏链的引出"></a>非周期马氏链的引出</h3><ul>
<li>对于一个有限状态马氏链,如果状态i是经过有限步转移后迟早要返回的状态,则称状态i是常返态.(即若i→j,则j→i;换句话说,从i能到j,一定可以从j到i,i是常返态,j也是常返态;常返态’链’上的状态是常返态),不是常返态的状态称为过渡态(如果k是过渡态,则从k出发后无法再回到k)  </li>
<li>如果马氏链中任何两个状态互通(互通的两个状态是常返态),则称马氏链为不可约的</li>
<li>一个有限马氏链按照互通关系所分成的子集中的状态要么是常返的,称为常返类;要么是过渡的,称为过度类.</li>
<li>一个有限马氏链至少有一个常返类和若干个过渡类(对于每一个状态都有入边和出边,所以至少有一个常返类)<br><img src="https://upload-images.jianshu.io/upload_images/9608551-ae96971de2d048ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="3.png"><br>对于非周期马氏链来说,对于它的任意状态i,从状态i出发,可以有非负条路径返回i,在每条路径走过的步数为 (path1,path2,…), 这些path之间的最大公约数是1. eg:(path1=5,path2=9,path3=10)<h3 id="马氏链的收敛定理"><a href="#马氏链的收敛定理" class="headerlink" title="马氏链的收敛定理"></a>马氏链的收敛定理</h3><img src="https://upload-images.jianshu.io/upload_images/9608551-a9c6e71ca89fa250.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="2.png"><br>对于2.和3.两条:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-f34c6331e96fa56d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="2.1.png"><br>这个定理非常重要,<strong>所有的MCMC方法都是建立在这个定理基础上的</strong>(定理的证明非常麻烦,书上也没有给出)  </li>
</ul>
<p><strong>马氏链的任何两个状态相通的含义是:存在一个n,使得P^n的任意元素大于零</strong><br><strong>由马氏链的收敛定理,我们可以这样做,从马氏链的某个状态出发,按照转移概率矩阵转移,假设第n步后马氏链收敛,则在第n+1步,n+2步…n+j步…所处的状态就是马氏链平稳分布π的样本</strong></p>
<h2 id="Metropolis-Hastings算法"><a href="#Metropolis-Hastings算法" class="headerlink" title="Metropolis-Hastings算法"></a>Metropolis-Hastings算法</h2><p>虽然马氏链能收敛,但是收敛后的分布不一定是我们想要的,如果让收敛后的分布就是我们希望的分布呢?<br>马尔科夫链蒙特卡洛方法(MCMC:Markov Chain Monte Carlo)方法就是解决这一问题的<br>马尔科夫链蒙特卡洛方法被评为二十世纪的十大算法之一  </p>
<p>下面介绍原版算法的改进算法:Metropolis-Hastings算法:<br>Metropolis-Hastings算法是一种马尔科夫蒙特卡洛（MCMC）方法，用于在难以直接采样时从某一概率分布中抽取随机样本序列。得到的序列可用于估计该概率分布或计算积分（如期望值）等。<strong>Metropolis-Hastings算法一般用于从多变量（尤其是高维）分布中采样</strong><br>主要使用如下定理:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-d145bdc6023a18db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="4.png"><br>细致平稳条件的直观理解:开始位于状态i,从状态i跳转到状态j的概率等于开始位于状态j,从状态j跳转到状态i的概率<br><strong>细致平稳条件只是马尔科夫链收敛的充分条件，不是必要条件</strong><br>一般而言,马氏链对应的概率转移矩阵P不满足细致平稳条件,可以构造出满足细致平稳条件的等式:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-bae47b5bd5c992f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="5.png"><br><strong>α(i,j)称为接受率,表示接受从状态i转移到状态j的概率</strong><br><img src="https://upload-images.jianshu.io/upload_images/9608551-1aca61c85e7fd219.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="6.png"><br>但是,α(i,j)或者α(j,i)往往很小,接受转移的概率小会使采样的次数过多,遍历马氏链所有的状态需要非常长的时间,进而导致样本收敛到平稳分布需要非常长的时间<br>可以把构造出的细致平稳等式两边同时放大一下,将α(i,j),α(j,i)中较大的那个扩成1,也即:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-2f4be23ebd94fcb9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="7.png"><br>这个式子是说,当α(i,j)是较大的那一个则取1,当α(i,j)是较小的那一个则取另一个</p>
<h2 id="Metropolis-Hastings算法采样流程"><a href="#Metropolis-Hastings算法采样流程" class="headerlink" title="Metropolis-Hastings算法采样流程"></a>Metropolis-Hastings算法采样流程</h2><p>(from Wikipedia)<br><strong>其中:</strong><br>A(x’|x)即上面说的接受率α(i,j)=α(x_t+1=j|x_t=i)<br>g(x’|x_t)即上面说的P_ij=P(x_t+1=j|x_t=i)  </p>
<p><img src="https://upload-images.jianshu.io/upload_images/9608551-6412b7a12dce1d8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="8.png"><br><strong>每次从分布g(x’|x_t)中随机产生一个状态x’,再从Uniform(0,1)中随机产生一个数u,如果u大于接受率A(x’|x),认为t+1时刻的状态为x’,即x_(t+1)=x’</strong><br><strong>采样流程中并没有给出具体的g(x’|x_t)分布也没有给出迭代次数,这些都是自己调整的,比如可以使用高斯分布或者其他方便产生样本的分布</strong>  </p>
<h2 id="Gibbs-Sampling"><a href="#Gibbs-Sampling" class="headerlink" title="Gibbs Sampling"></a>Gibbs Sampling</h2><p>吉布斯采样(Gibbs Sampling)<br>吉布斯采样是MCMC的一种,用于获得样本序列,这个样本序列构成了马尔科夫链,每个样本之间是相关的,不是独立的!当样本构成的马氏链收敛后,马氏链的平稳分布便能拟合含有多个多变量的分布,比如近似一个很难直接采样的联合概率分布,前提是<strong>得知道每个变量的条件概率</strong><br>吉布斯采样是一种随机算法(使用随机数),常用于贝叶斯推理(因为贝叶斯网络含有条件概率的集合),作为随机算法,它是用于统计推理的确定性算法(如EM算法:expectation-maximization algorithm)的一种替代方法<br>再回到之前Metropolis-Hastings算法,由于有接受率的存在,并不能保证每次的采样结果都被接收,所以会导致收敛前采样次数的增加,而吉布斯采样就能保证每次采样的结果可以使用.</p>
<h2 id="吉布斯采样流程"><a href="#吉布斯采样流程" class="headerlink" title="吉布斯采样流程"></a>吉布斯采样流程</h2><p><img src="https://upload-images.jianshu.io/upload_images/9608551-4321f2e89f9226d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="9.png">  </p>
<p>注意事项: </p>
<ol>
<li>初始化的值可以随机给定,或者根据EM算法给定</li>
<li>实际上没有必要着重处理初始值,因为一般都是直接忽略前面的样本的(叫作:burn-in period),毕竟需要跳转足够多的次数后马氏链才收敛, </li>
<li>联合分布的参数通过将k个样本的参数平均后近似</li>
<li>举个例子:直接忽略前1000个样本,之后每100个样本拿出一个,比如取出第1100,1200,1300,….2000个样本做平均,因为连续采出的样本不独立</li>
<li>在采样的早期,常用模拟退火来减少随机游走行为 (i.e. the tendency to move slowly around the sample space, with a high amount of autocorrelation between samples, rather than moving around quickly, as is desired). </li>
</ol>
<p>参考:<br>靳志辉,LDA数学八卦<br>田宝玉,信息论基础</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/20/Dirichlet-Multinomial-%E5%85%B1%E8%BD%AD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/20/Dirichlet-Multinomial-%E5%85%B1%E8%BD%AD/" class="post-title-link" itemprop="url">Dirichlet Multinomial 共轭</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-20 18:23:34" itemprop="dateCreated datePublished" datetime="2018-03-20T18:23:34+08:00">2018-03-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2018/03/20/Dirichlet-Multinomial-%E5%85%B1%E8%BD%AD/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/20/Dirichlet-Multinomial-共轭/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>作为LDA的预备知识,Dirichlet Multinomial共轭结构很重要,在介绍这个共轭结构之前,先介绍一下将会用到的相关概念</p>
<h2 id="Gamma-函数"><a href="#Gamma-函数" class="headerlink" title="Gamma 函数"></a>Gamma 函数</h2><p>Gamma函数定义:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-19dd3611014b4449.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="1.png"><br>分部积分后可得:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-022c964076a451d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="2.png"><br>不断展开等式右边,进而有:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-bfd9604547712884.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="3.png"><br>Bohr-Mullerup定理:<br>如果f:(0,∞)→(0,∞),且满足:f(1)=1;f(x+1)=xf(x);log(f(x))是凸函数,那么唯一满足以上条件的就是Γ(x)<br>Gamma函数图像(from Wikipedia):<br><img src="https://upload-images.jianshu.io/upload_images/9608551-d671cdb6cc29bbf9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="4.png"><br>复平面上的Gamma函数(from Wikipedia):<br><img src="https://upload-images.jianshu.io/upload_images/9608551-a4a7785ecc0badbc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="5.png"><br>如下函数被称为Digamma函数:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-714da680527edbb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="6.png"><br>这是个很重要的函数,在求Dirichlet分布相关的参数的极大似然估计时往往用到该函数<br>Digamma函数具有如下性质<br><img src="https://upload-images.jianshu.io/upload_images/9608551-a69cc5479512a4dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="7.png"></p>
<h2 id="Beta-Binomial-共轭"><a href="#Beta-Binomial-共轭" class="headerlink" title="Beta Binomial 共轭"></a>Beta Binomial 共轭</h2><p><strong>在贝叶斯统计中，如果后验分布与先验分布属于同分布，则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验.</strong><br>Beta分布是Binomial分布的共轭先验   </p>
<h3 id="Beta分布"><a href="#Beta分布" class="headerlink" title="Beta分布"></a>Beta分布</h3><p><img src="https://upload-images.jianshu.io/upload_images/9608551-af749f174ca0a672.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="9.png"><br>可以通过推导n个独立产生于同一个均匀分布的数字中第k大数字的过程推导出Beta分布,具体可参考靳志辉老师的LDA数学八卦<br>Beta分布的期望:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-18a8c2834cef9508.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="13.png">  </p>
<p><img src="https://upload-images.jianshu.io/upload_images/9608551-473bc02b0326d09f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="14.png"><br>Beta分布(from Wikipedia)<br>PDF:probability density function(概率密度函数)<br><img src="https://upload-images.jianshu.io/upload_images/9608551-09494c29a5fc9eb1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="8.png"><br>因为Beta分布可以拟合多种曲线,所以被广泛使用   </p>
<h3 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a>二项分布</h3><p>n是总的试验次数,p是实验成功的概率,k是实验成功的次数<br>Probability mass function:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-df0cfb91ca0a4775.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="12.png">  </p>
<h3 id="Beta-Binomial-共轭-1"><a href="#Beta-Binomial-共轭-1" class="headerlink" title="Beta Binomial 共轭"></a>Beta Binomial 共轭</h3><p>按照贝叶斯推理的过程引出Beta-Binomial共轭:  </p>
<ol>
<li>p是要猜的参数,假设p的先验分布为Beta分布,即<br><img src="https://upload-images.jianshu.io/upload_images/9608551-e6386209e22f77b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="10.png"></li>
<li>现有m个数字,知道这m个数字与p的大小关系,其中有m1个数字比p小,m2个数字比p大(m1+m2=m).可知这m个数字与p的大小关系是二项分布(Binomial Distribution)的一个观察值  </li>
<li>那么根据m1和m2这个经验,我们便可以得到p的后验分布(证明过程可参考靳志辉老师的LDA数学八卦,并不复杂)<br><img src="https://upload-images.jianshu.io/upload_images/9608551-b364ad17cc4ac7e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="11.png"><br>后验分布和先验分布都是Beta分布,只不过是参数变了,所以Beta分布式二项分布的共轭先验.  </li>
</ol>
<p><strong>实际上,第一步也可以假设p服从其它分布,只不过因为观察值服从二项分布,所以假设p为Beta分布后,p的后验概率也服从Beta分布,方面计算</strong></p>
<h2 id="Dirichlet-Multinomial-共轭"><a href="#Dirichlet-Multinomial-共轭" class="headerlink" title="Dirichlet Multinomial 共轭"></a>Dirichlet Multinomial 共轭</h2><h3 id="Dirichlet分布"><a href="#Dirichlet分布" class="headerlink" title="Dirichlet分布"></a>Dirichlet分布</h3><p><img src="https://upload-images.jianshu.io/upload_images/9608551-1888464f88355ee6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="15.png"><br><strong>Beta分布就是Dirichlet分布的参数n=2时的情况</strong><br>Dirichlet分布的期望:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-0f92d3be43897145.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/500" alt="16.png"><br>或者<br><img src="https://upload-images.jianshu.io/upload_images/9608551-08d06e7d4e3aea6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/500" alt="17.png"><br>Dirichlet分布(from Wikipedia)<br>dirichlet-distribution:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-44eac7aedc9e0d54.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="dirichlet-distribution.png"></p>
<p><strong>LogDirichletDensity-alpha_0.3_to_alpha_2.0</strong><br><strong>在LDA中用的主要是α＜1的对称Dirichlet分布</strong><br><img src="https://upload-images.jianshu.io/upload_images/9608551-99f2acf0a081f039.gif?imageMogr2/auto-orient/strip" alt="LogDirichletDensity-alpha_0.3_to_alpha_2.0.gif">  </p>
<h3 id="Multinomial-分布"><a href="#Multinomial-分布" class="headerlink" title="Multinomial 分布"></a>Multinomial 分布</h3><p>多项分布是二项分布的推广,举例来说,多项分布建模的是这一问题:有一个k个面的骰子,投掷一次结果是第i的面概率是pi,现独立地投掷n次,结果是第i个面的有xi次,多项分布就是给出了投掷n次后各种结果的概率公式<br>Probability mass function:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-7d8501965f170945.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="18.png"><br><strong>Binomial分布就是Multinomial分布的n=2时的情况</strong></p>
<h3 id="Dirichlet-Multinomial-共轭-1"><a href="#Dirichlet-Multinomial-共轭-1" class="headerlink" title="Dirichlet Multinomial 共轭"></a>Dirichlet Multinomial 共轭</h3><p>类似Beta Binomial共轭的贝叶斯推理:</p>
<ol>
<li>(p1,p2,…pn)是要猜的参数,假设(p1,p2,…pn)的先验分布为Dirichlet分布</li>
<li>现有n个数字(x1,x2,…,xn),知道这n个数字与(p1,p2,…pn)的大小关系,其中有c1个数字比p1小,c2个数字比p1大同时比p2小,cn个数字比p_(n-1)大同时比pn小.这n个数字与(p1,p2,…pn)的大小关系是多项分布的一个观察值     </li>
<li>根据(c1,c2,…,cn)这个经验,可以得到(p1,p2,…pn)的后验分布<br><img src="https://upload-images.jianshu.io/upload_images/9608551-2448311b96de77d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="19.png"></li>
</ol>
<p>参考:靳志辉,LDA数学八卦</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/19/SupportVectorMachine%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Little Haes">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haes' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/19/SupportVectorMachine%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" class="post-title-link" itemprop="url">SupportVectorMachine支持向量机</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-19 21:52:02" itemprop="dateCreated datePublished" datetime="2018-03-19T21:52:02+08:00">2018-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-13 23:16:44" itemprop="dateModified" datetime="2019-12-13T23:16:44+08:00">2019-12-13</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2018/03/19/SupportVectorMachine%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/19/SupportVectorMachine支持向量机/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>支持向量机(support vector machine,SVM)是一种<strong>二分类</strong>模型,它的基本模型是定义在<strong>特征空间上的</strong>间隔最大的<strong>线性分类器</strong>,间隔最大使它有别于感知机.<br>有3类支持向量机模型:<br>    1. 线性可分支持向量机<br>    2. 线性支持向量机<br>    3. 非线性支持向量机<br>(这三种模型建立思路很像,求解过程不同)</p>
<h2 id="线性可分支持向量机"><a href="#线性可分支持向量机" class="headerlink" title="线性可分支持向量机"></a>线性可分支持向量机</h2><h3 id="几何间隔与函数间隔的引出"><a href="#几何间隔与函数间隔的引出" class="headerlink" title="几何间隔与函数间隔的引出"></a>几何间隔与函数间隔的引出</h3><p>超平面wx+b=0外一点x0到超平面的距离公式(几何间隔):<br><img src="https://upload-images.jianshu.io/upload_images/9608551-cb47940a8aefedf2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w400" alt="1.png"><br>分母是w的二范式,不随x0的改变而改变,所以可以分子|wx0+b|能够相对地表示点x0距离超平面wx+b=0的远近.<br>一个点距离分离超平面的远近可以表示分类预测的确信程度.<br>wx0+b的符号与类标记y0的符号是否一致能够表示分类是否正确.<br>所以,<strong>可用y0(wx0+b)来表示分类的正确性及确信度,这就是函数间隔(functional margin)</strong>  </p>
<h3 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h3><p>超平面(w,b)关于样本点xi的函数间隔为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-e8b5378ba5f3102a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="2.png"><br>超平面(w,b)关于训练集T的函数间隔为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-a71ba18772effb31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="3.png"></p>
<h3 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h3><p>超平面(w,b)关于样本点xi的几何间隔为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-12f6cc40439d27cd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="4.png"><br>超平面(w,b)关于训练集T的几何间隔为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-d7377c98b9552ecc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="5.png"></p>
<h3 id="硬间隔最大化"><a href="#硬间隔最大化" class="headerlink" title="硬间隔最大化"></a>硬间隔最大化</h3><p>找到了超平面(w,b)关于训练集T的几何间隔后,自然地希望最大化这个几何间隔以保证之后分类预测的确信程度<br>目标函数和约束如下:<br>约束表示超平面(w,b)关于任意样本点的几何间隔大于等于超平面(w,b)关于训练集T的几何间隔<br><img src="https://upload-images.jianshu.io/upload_images/9608551-ff6aaae6d39a51a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="6.png"><br>在约束中约去||w||并展开γ^<br><img src="https://upload-images.jianshu.io/upload_images/9608551-79a73fb2c8d759f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="6.1.png"><br>求出w,b的话问题就解决了,先别急,让我们化简一下上面的式子<br>注意到,如果对w,b同比例的放缩,即变成kw,kb,函数间隔yi(wxi+b)也会成比例k变化,而超平面(w,b)没有变化,<br>此时原问题和约束变为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-a1067828cc35c333.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="7.png"><br>约掉k后,目标函数和约束没有改变,说明对w,b进行同比例放缩丝毫不影响目标函数和约束,<strong>那么可以选取合适的k让函数间隔γ^=1</strong>,也就是<br><img src="https://upload-images.jianshu.io/upload_images/9608551-f03595219d20d995.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="8.1.png"><br>这样一来,目标函数和约束就变成了:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-fb455cb8c8f44856.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="8.png"><br>把||w||挪到分子上平方一下再乘个常数就有了:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-0ba320ed7a3a25c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt="9.png"><br>比最初的形式简化了不少,这是个带约束问题,通过Lagrange Multiplier拉格朗日乘子法化成无约束问题:<br>(原问题:极大极小)<br><img src="https://upload-images.jianshu.io/upload_images/9608551-9ad79d65f9fbcfa3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="10.png"></p>
<p>具体原理可参考之前的文章<a href="http://littlehaes.com/2018/03/15/Lagrange-duality拉格朗日对偶性/" target="_blank" rel="noopener">Lagrange duality拉格朗日对偶性</a><br>对偶形式:  </p>
<p><img src="https://upload-images.jianshu.io/upload_images/9608551-37cbecb2b63b0e90.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="11.png"><br>其中:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-36db3de657af14ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" alt="12.png"><br>所以有:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-8bd130a5b32d40c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="13.png"><br>进而有(最终形式):<br><strong>下面的约束是求 min L(w,b,α)时得到的</strong><br><strong>这是个凸二次规划问题(目标函数是二次函数,不等式约束是仿射函数)</strong><br><img src="https://upload-images.jianshu.io/upload_images/9608551-ba6dc472a61377cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="14.png"><br>求解得到α<em>=(α1,α2,…,αN)^t,这是对偶问题的解,<br>可由α*得到w\</em>,b*<br><img src="https://upload-images.jianshu.io/upload_images/9608551-9f239a4deb2998e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="15.png"><br><strong>之所以能从对偶问题获得原问题的解,是因为原问题为凸二次规划问题,并且解α*,w*,b* 满足KKT条件</strong><br>有了w*,b*就能得到最大间隔分离超平面和分类决策函数:  </p>
<p><img src="https://upload-images.jianshu.io/upload_images/9608551-a745a7f9db18b42b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="16.png"></p>
<h3 id="支持向量"><a href="#支持向量" class="headerlink" title="支持向量"></a>支持向量</h3><p>通过由α*得到w*,b*的公式可以知道,对应α*=0的实例xi对超平面(w*,b*)的两个参数都没有贡献,只有对应α*＞0的实例xi对超平面(w*,b*)的两个参数有贡献,也就是说<strong>超平面完全由对应α*＞0的实例决定,这些实例称为支持向量,由KKT的互补条件知,若α*＞0,则有y(wx+b)-1=0,即wx+b=±1,说明支持向量都在间隔边界上</strong></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>对于线性可分SVM学习来说:</p>
<ol>
<li>模型为分离超平面和决策函数</li>
<li>学习策略:硬间隔最大化(间隔的描述及约束)</li>
<li>学习算法:凸二次规划</li>
</ol>
<h2 id="线性支持向量机"><a href="#线性支持向量机" class="headerlink" title="线性支持向量机"></a>线性支持向量机</h2><p>通常情况下,训练数据中有一些特异点(outlier),将这些特异点去掉后,剩下的大部分样本点组成的集合是线性可分的.<strong>线性不可分意味着不是所有点都满足函数间隔大于等于1的约束,为解决这个问题,引入一个松弛变量ζi≥0,使函数间隔加上松弛变量后大于等于1</strong>.即,y(wx+b)+ζ≥1  </p>
<h3 id="软间隔最大化"><a href="#软间隔最大化" class="headerlink" title="软间隔最大化"></a>软间隔最大化</h3><p>对每个松弛变量ζi,支付一个代价ζi,目标函数变为<br><img src="https://upload-images.jianshu.io/upload_images/9608551-cb73960c94d02fd2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="17.png"><br><strong>这里C&gt;0称为惩罚参数</strong>,C越大则对<strong>错误分类</strong>的惩罚越大<br><strong>最小化上述目标函数实现的是软间隔最大化,最小化上式包含两层含义:使1/2||w||^2尽量小,即间隔尽量大,同时使误分类点的个数尽量小</strong>.<br>硬间隔就是真正的间隔,软间隔是包含了代价项ζ的硬间隔<br>由上分析便得到了线性支持向量机的学习目标:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-da01a6d13df631f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="18.png"><br>化为拉格朗日形式(不带约束)的原问题:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-5baedeed43709629.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="19.png"><br>对偶问题:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-2ffb45c01091ba8a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="20.png"><br>其中:  </p>
<p><img src="https://upload-images.jianshu.io/upload_images/9608551-619deff39232027d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="21.png"><br>进而:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-a9bcbe2fe16a3198.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="22.png"><br>所以对偶问题的最终形式:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-9ebde8500c681fd5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="23.png"><br><strong>与线性可分SVM的对偶最终形式相比只是α的约束不同,约束增强了</strong><br>求解得到α<em>=(α1,α2,…,αN)^t,这是对偶问题的解,<br>可由α*得到w\</em>,b*<br><img src="https://upload-images.jianshu.io/upload_images/9608551-75f93ccc1366c519.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="24.png"><br>有了w*,b*就能得到最大间隔分离超平面和分类决策函数:  </p>
<p><img src="https://upload-images.jianshu.io/upload_images/9608551-a745a7f9db18b42b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="16.png"></p>
<h3 id="支持向量-1"><a href="#支持向量-1" class="headerlink" title="支持向量"></a>支持向量</h3><p>和线性可分SVM类似,<strong>超平面完全由对应α*＞0的实例决定,这些实例称为支持向量,但是线性SVM的支持向量不一定都在间隔边界上</strong><br><img src="https://upload-images.jianshu.io/upload_images/9608551-0d88d26ab51bd146.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="25.png"><br><strong>KKT互补条件之一:α*(y(w*x+b)+ζ-1)=0</strong><br>当0&lt;αi&lt;C时,ζi=0,则y(w*x+b)-1=0,此时支持向量在间隔边界上<br>当αi=C时,ζi&gt;0,支持向量可能在:<br>间隔边界与超平面之间:0&lt;ζi&lt;1<br>超平面上:ζi=1<br>误分类一侧:ζi&gt;1</p>
<h3 id="Hinge-Loss-Function合页损失函数"><a href="#Hinge-Loss-Function合页损失函数" class="headerlink" title="Hinge Loss Function合页损失函数"></a>Hinge Loss Function合页损失函数</h3><p>线性SVM的学习还有另一种等价模型,即最小化目标函数:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-dd7d0f6f1c592edb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="26.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-39a14e13eb96d885.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="27.png"><br><img src="https://upload-images.jianshu.io/upload_images/9608551-6d4b1abf51657921.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="28.png"><br>证明新目标函数与原问题等价时,主要抓住三点:<br>    1. hinge loss≥0<br>    2. 令[1-y(wx+b)]_+=ζ<br>    3. 讨论1-y(wx+b)的取值范围<br>L(y(wx+b))说明了:<br>    1. 点到超平面的函数距离≥1时,损失为0<br>    2. 点到超平面的函数距离＜1时,损失为1-y(wx+b)<br>下图蓝线代表hinge loss的图像<br><img src="https://upload-images.jianshu.io/upload_images/9608551-00605cf8a8f496b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="29.png"></p>
<h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>对于线性SVM学习来说:</p>
<ol>
<li>模型为分离超平面和决策函数(线性SVM的对偶形式)</li>
<li>学习策略:软间隔最大化(间隔的描述及约束)</li>
<li>学习算法:凸二次规划</li>
</ol>
<h2 id="非线性支持向量机"><a href="#非线性支持向量机" class="headerlink" title="非线性支持向量机"></a>非线性支持向量机</h2><p>用线性分类方法求解非线性分类任务分为两步:<br>    1. 将训练数据从输入空间(欧式空间或离散集合)映射到新的特征空间(希尔伯特空间)使数据在新特征空间中线性可分<br>    2. 在新特征空间中用线性分类学习方法从训练数据中学习分类模型,<strong>使得输入空间中的超曲面模型对应特征空间中的超平面模型</strong><br>Kernel trick(核技巧)便属于这样的方法</p>
<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p> <img src="https://upload-images.jianshu.io/upload_images/9608551-9bb863f114d19818.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="30.png"><br> K(x1,x2)=φ(x1)φ(x2)是个对称函数,叫作正定核(positive definite kernel)<br> 一般只定义K(x1,x2),而不显式地定义φ(x),那么如何保证K(x1,x2)是正定核呢?<br> 正定核的充要条件:<br> <img src="https://upload-images.jianshu.io/upload_images/9608551-e996f799fb0dba36.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="33.png"><br>证明过程需要预备知识:构造希尔伯特空间  </p>
<ul>
<li>定义映射φ并构成向量空间S(对加法和数乘运算封闭的集合)</li>
<li>在S上定义内积构成内积空间(用到了欧式空间Gram矩阵的半正定性)</li>
<li>将内积空间S完备化为希尔伯特空间</li>
</ul>
<p>由α*得到b*<br><img src="https://upload-images.jianshu.io/upload_images/9608551-10a67cf49667196b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="31.png"><br>分类决策函数<br><img src="https://upload-images.jianshu.io/upload_images/9608551-5d6ac4ec7544f2b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="32.png"></p>
<h3 id="常用的核函数"><a href="#常用的核函数" class="headerlink" title="常用的核函数"></a>常用的核函数</h3><p>正定核的充要条件在构造核函数时很有用.但对于一个具体函数K(x,z)来说,检验它是否为正定核并不容易,因为需要对任意有限输入集{x1,x2,…,xm}验证K对应的Gram矩阵(在希尔伯特空间)是否为半正定的.实际问题中往往应用已有的核函数</p>
<ol>
<li><p>多项式核函数(polynomial kernel function)<br><img src="https://upload-images.jianshu.io/upload_images/9608551-19fdfd50cf1ea07e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="34.png"><br>对应的支持向量机是一个p次多项式分类器,分类决策函数为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-dea5107f0a5c63fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="35.png">  </p>
</li>
<li><p>高斯核函数(Gaussian kernel function)<br>通过泰勒展开可知高斯核是无穷维的<br><img src="https://upload-images.jianshu.io/upload_images/9608551-9bf1ddec2773600f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="36.png"><br>对应的支持向量机是高斯径向基函数(radial basis function)分类器,分类决策函数为:<br><img src="https://upload-images.jianshu.io/upload_images/9608551-408894baa045886f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="37.png">  </p>
</li>
<li><p>字符串核函数(string kernel function)<br>定义在字符串集合上的核函数,字符串核函数应用在文本分类,信息检索,生物信息学等方面.<br>k_n(s,t)给出了字符串s和t中长度等于n的所有子串组成的特征向量的余弦相似度(cosine similarity).直观上,两个字符串相同的子串越多,它们就越相似,字符串核函数的值就越大.字符串核函数可以由动态规划快速计算</p>
<h3 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h3><p>SMO:sequential minimal optimization<br>利用SMO算法实现SVM的学习<br><img src="https://upload-images.jianshu.io/upload_images/9608551-9bb863f114d19818.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/900" alt="30.png"><br>SMO算法包括两个部分:</p>
<ol>
<li>求解两个变量二次规划的解析方法(线性规划;把握好对g(x)和Ei的理解)</li>
<li>选择变量的启发式方法(KKT;|E1-E2|最大)</li>
</ol>
<p>参考:李航,统计学习方法</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/10/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/12/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Little Haes"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Little Haes</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">78</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/smallhaes" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;smallhaes" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:littlehaes@bupt.edu.cn" title="E-Mail → mailto:littlehaes@bupt.edu.cn" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/smallhaes" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;smallhaes" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://music.163.com/#/user/home?id=75165464" title="网易云 → https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;75165464" rel="noopener" target="_blank"><i class="fa fa-fw fa-cloud"></i>网易云</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Little Haes</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.1.1
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.6.0
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://https-littlehaes-com.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>

</body>
</html>
